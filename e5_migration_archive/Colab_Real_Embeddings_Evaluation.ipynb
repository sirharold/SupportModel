{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "view-in-github"
   },
   "source": "Upload these files to Google Drive:\n- `docs_ada_with_embeddings_*.parquet` (2.2GB) - Ada embeddings (1536 dims)\n- `docs_e5large_with_embeddings_*.parquet` (1.7GB) - E5-Large embeddings (1024 dims)  \n- `docs_mpnet_with_embeddings_*.parquet` (1.4GB) - MPNet embeddings (768 dims)\n- `docs_minilm_with_embeddings_*.parquet` (1.0GB) - MiniLM embeddings (384 dims)\n- Questions dataset (JSON with ground truth links)",
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "setup"
   },
   "source": [
    "## üîß Setup and Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "install"
   },
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install sentence-transformers pandas numpy scikit-learn tqdm\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import json\n",
    "from datetime import datetime\n",
    "import gc\n",
    "from typing import List, Dict, Tuple\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "drive-mount"
   },
   "source": [
    "## üìÅ Mount Google Drive"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "mount-drive"
   },
   "outputs": [],
   "source": "from google.colab import drive\ndrive.mount('/content/drive')\n\n# Set paths to your uploaded files\nBASE_PATH = '/content/drive/MyDrive/RAG_Evaluation/'\n\n# Available embedding collections\nEMBEDDING_FILES = {\n    'ada': BASE_PATH + 'docs_ada_with_embeddings_20250721_123712.parquet',\n    'e5-large': BASE_PATH + 'docs_e5large_with_embeddings_20250721_124918.parquet', \n    'mpnet': BASE_PATH + 'docs_mpnet_with_embeddings_20250721_125254.parquet',\n    'minilm': BASE_PATH + 'docs_minilm_with_embeddings_20250721_125846.parquet'\n}\n\n# Questions dataset\nQUESTIONS_FILE = BASE_PATH + 'questions_with_links.json'\n\nprint(\"üìÅ File paths configured:\")\nfor model, path in EMBEDDING_FILES.items():\n    print(f\"  {model}: {path}\")\nprint(f\"  questions: {QUESTIONS_FILE}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "real-retriever"
   },
   "source": [
    "## üîç Real Embedding Retriever Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "retriever-class"
   },
   "outputs": [],
   "source": [
    "class RealEmbeddingRetriever:\n",
    "    \"\"\"Retriever que usa embeddings reales para c√°lculo de coseno\"\"\"\n",
    "    \n",
    "    def __init__(self, parquet_file: str):\n",
    "        \"\"\"\n",
    "        Inicializar con archivo Parquet que contiene embeddings reales\n",
    "        \"\"\"\n",
    "        print(f\"üîÑ Loading real embeddings from {parquet_file}...\")\n",
    "        self.df = pd.read_parquet(parquet_file)\n",
    "        \n",
    "        # Convertir embeddings a matriz numpy\n",
    "        print(\"üîÑ Converting embeddings to numpy matrix...\")\n",
    "        embeddings_list = self.df['embedding'].tolist()\n",
    "        self.embeddings_matrix = np.array(embeddings_list)\n",
    "        \n",
    "        # Informaci√≥n del dataset\n",
    "        self.num_docs = len(self.df)\n",
    "        self.embedding_dim = self.embeddings_matrix.shape[1]\n",
    "        \n",
    "        print(f\"‚úÖ Loaded {self.num_docs:,} documents\")\n",
    "        print(f\"üìê Embedding dimensions: {self.embedding_dim}\")\n",
    "        print(f\"üíæ Memory usage: {self.embeddings_matrix.nbytes / (1024**3):.2f} GB\")\n",
    "        \n",
    "        # Preparar metadatos\n",
    "        self.documents = self.df[['document', 'link', 'title', 'summary', 'content']].to_dict('records')\n",
    "        \n",
    "    def search_documents(self, query_embedding: np.ndarray, top_k: int = 10) -> List[Dict]:\n",
    "        \"\"\"Buscar documentos m√°s similares usando coseno real\"\"\"\n",
    "        # Calcular similaridad coseno real\n",
    "        query_embedding = query_embedding.reshape(1, -1)\n",
    "        similarities = cosine_similarity(query_embedding, self.embeddings_matrix)[0]\n",
    "        \n",
    "        # Obtener √≠ndices de documentos m√°s similares\n",
    "        top_indices = np.argsort(similarities)[::-1][:top_k]\n",
    "        \n",
    "        # Construir resultados con metadatos reales\n",
    "        results = []\n",
    "        for idx in top_indices:\n",
    "            doc = self.documents[idx].copy()\n",
    "            doc['cosine_similarity'] = float(similarities[idx])\n",
    "            doc['rank'] = len(results) + 1\n",
    "            results.append(doc)\n",
    "        \n",
    "        return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "metrics-calc"
   },
   "source": [
    "## üìä Real Metrics Calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "metrics-function"
   },
   "outputs": [],
   "source": [
    "def calculate_real_retrieval_metrics(\n",
    "    question: str,\n",
    "    query_embedding: np.ndarray,\n",
    "    retriever: RealEmbeddingRetriever,\n",
    "    ground_truth_links: List[str],\n",
    "    top_k_values: List[int] = [1, 3, 5, 10]\n",
    ") -> Dict:\n",
    "    \"\"\"Calcular m√©tricas de retrieval reales usando coseno con embeddings aut√©nticos\"\"\"\n",
    "    \n",
    "    # Buscar documentos con coseno real\n",
    "    max_k = max(top_k_values) if top_k_values else 10\n",
    "    retrieved_docs = retriever.search_documents(query_embedding, top_k=max_k)\n",
    "    \n",
    "    # Normalizar enlaces para comparaci√≥n\n",
    "    def normalize_link(link: str) -> str:\n",
    "        if not link:\n",
    "            return \"\"\n",
    "        link = link.split('#')[0].split('?')[0]\n",
    "        return link.rstrip('/')\n",
    "    \n",
    "    # Normalizar ground truth\n",
    "    gt_normalized = set(normalize_link(link) for link in ground_truth_links)\n",
    "    \n",
    "    # Calcular m√©tricas para cada k\n",
    "    metrics = {}\n",
    "    for k in top_k_values:\n",
    "        top_k_docs = retrieved_docs[:k]\n",
    "        \n",
    "        # Enlaces recuperados (normalizados)\n",
    "        retrieved_links = set()\n",
    "        for doc in top_k_docs:\n",
    "            link = normalize_link(doc.get('link', ''))\n",
    "            if link:\n",
    "                retrieved_links.add(link)\n",
    "        \n",
    "        # M√©tricas\n",
    "        relevant_retrieved = retrieved_links.intersection(gt_normalized)\n",
    "        \n",
    "        # Precision@k, Recall@k, F1@k\n",
    "        precision_k = len(relevant_retrieved) / k if k > 0 else 0.0\n",
    "        recall_k = len(relevant_retrieved) / len(gt_normalized) if gt_normalized else 0.0\n",
    "        f1_k = (2 * precision_k * recall_k) / (precision_k + recall_k) if (precision_k + recall_k) > 0 else 0.0\n",
    "        \n",
    "        metrics[f'precision@{k}'] = precision_k\n",
    "        metrics[f'recall@{k}'] = recall_k\n",
    "        metrics[f'f1@{k}'] = f1_k\n",
    "    \n",
    "    # MRR (Mean Reciprocal Rank)\n",
    "    mrr = 0.0\n",
    "    for rank, doc in enumerate(retrieved_docs, 1):\n",
    "        link = normalize_link(doc.get('link', ''))\n",
    "        if link in gt_normalized:\n",
    "            mrr = 1.0 / rank\n",
    "            break\n",
    "    \n",
    "    metrics['mrr'] = mrr\n",
    "    metrics['ground_truth_count'] = len(gt_normalized)\n",
    "    metrics['retrieved_count'] = len(retrieved_docs)\n",
    "    \n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "load-data"
   },
   "source": [
    "## üìÇ Load Questions Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "load-questions"
   },
   "outputs": [],
   "source": [
    "# Load questions with ground truth\n",
    "print(\"üì• Loading questions dataset...\")\n",
    "with open(QUESTIONS_FILE, 'r', encoding='utf-8') as f:\n",
    "    questions_data = json.load(f)\n",
    "\n",
    "print(f\"‚úÖ Loaded {len(questions_data)} questions\")\n",
    "\n",
    "# Show sample question\n",
    "if questions_data:\n",
    "    sample_q = questions_data[0]\n",
    "    print(f\"\\nüîç Sample question:\")\n",
    "    print(f\"  Question: {sample_q.get('question', '')[:100]}...\")\n",
    "    print(f\"  Ground truth links: {len(sample_q.get('ms_links', []))} links\")\n",
    "    print(f\"  Sample link: {sample_q.get('ms_links', ['N/A'])[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "evaluation"
   },
   "source": [
    "## üéØ Run Real Embedding Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "run-evaluation"
   },
   "outputs": [],
   "source": "# Configuration\nEMBEDDING_MODEL_TO_EVALUATE = 'ada'  # Change to 'e5-large', 'mpnet', or 'minilm'\nNUM_QUESTIONS_TO_EVALUATE = 100  # Set to None for all questions\n\n# Model mappings for query embedding generation\nQUERY_MODELS = {\n    'ada': 'sentence-transformers/all-MiniLM-L6-v2',  # Proxy for Ada\n    'e5-large': 'intfloat/e5-large-v2',\n    'mpnet': 'sentence-transformers/multi-qa-mpnet-base-dot-v1',\n    'minilm': 'sentence-transformers/all-MiniLM-L6-v2'\n}\n\nprint(f\"üöÄ Starting Real Embedding Evaluation\")\nprint(f\"üìä Embedding model: {EMBEDDING_MODEL_TO_EVALUATE}\")\nprint(f\"üìÑ Document corpus: {EMBEDDING_FILES[EMBEDDING_MODEL_TO_EVALUATE]}\")\nprint(f\"‚ùì Questions to evaluate: {NUM_QUESTIONS_TO_EVALUATE or 'ALL'}\")\nprint(\"=\"*80)\n\n# Load retriever with real embeddings\nretriever = RealEmbeddingRetriever(EMBEDDING_FILES[EMBEDDING_MODEL_TO_EVALUATE])\n\n# Load query embedding model\nquery_model_name = QUERY_MODELS[EMBEDDING_MODEL_TO_EVALUATE]\nprint(f\"\\nüî§ Loading query model: {query_model_name}\")\nquery_model = SentenceTransformer(query_model_name)\nprint(f\"‚úÖ Query model loaded\")\n\n# Select questions to evaluate\nquestions_to_eval = questions_data[:NUM_QUESTIONS_TO_EVALUATE] if NUM_QUESTIONS_TO_EVALUATE else questions_data\nprint(f\"\\nüìä Evaluating {len(questions_to_eval)} questions...\")\n\n# Run evaluation\nall_metrics = []\n\nfor i, qa_item in enumerate(tqdm(questions_to_eval, desc=\"Evaluating questions\")):\n    question = qa_item.get('question', '')\n    ms_links = qa_item.get('ms_links', [])\n    \n    if not question or not ms_links:\n        continue\n        \n    # Generate query embedding\n    query_embedding = query_model.encode(question)\n    \n    # Calculate real metrics\n    metrics = calculate_real_retrieval_metrics(\n        question=question,\n        query_embedding=query_embedding,\n        retriever=retriever,\n        ground_truth_links=ms_links,\n        top_k_values=[1, 3, 5, 10]\n    )\n    \n    metrics['question_index'] = i\n    metrics['question'] = question\n    all_metrics.append(metrics)\n\nprint(f\"\\n‚úÖ Evaluation completed: {len(all_metrics)} questions processed\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "results"
   },
   "source": [
    "## üìà Results Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "analyze-results"
   },
   "outputs": [],
   "source": [
    "# Calculate average metrics\n",
    "if all_metrics:\n",
    "    avg_metrics = {}\n",
    "    for key in ['precision@1', 'precision@3', 'precision@5', 'precision@10',\n",
    "               'recall@1', 'recall@3', 'recall@5', 'recall@10',\n",
    "               'f1@1', 'f1@3', 'f1@5', 'f1@10', 'mrr']:\n",
    "        values = [m[key] for m in all_metrics if key in m]\n",
    "        avg_metrics[f'avg_{key}'] = np.mean(values) if values else 0.0\n",
    "    \n",
    "    # Display results\n",
    "    print(f\"üìä REAL EMBEDDING EVALUATION RESULTS\")\n",
    "    print(f\"üìÑ Model: {EMBEDDING_MODEL_TO_EVALUATE}\")\n",
    "    print(f\"üìö Documents: {retriever.num_docs:,}\")\n",
    "    print(f\"üìê Dimensions: {retriever.embedding_dim}\")\n",
    "    print(f\"‚ùì Questions: {len(all_metrics)}\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Precision metrics\n",
    "    print(\"üéØ PRECISION METRICS:\")\n",
    "    for k in [1, 3, 5, 10]:\n",
    "        precision = avg_metrics[f'avg_precision@{k}'] * 100\n",
    "        print(f\"  Precision@{k:2d}: {precision:6.2f}%\")\n",
    "    \n",
    "    # Recall metrics\n",
    "    print(\"\\nüîç RECALL METRICS:\")\n",
    "    for k in [1, 3, 5, 10]:\n",
    "        recall = avg_metrics[f'avg_recall@{k}'] * 100\n",
    "        print(f\"  Recall@{k:2d}:    {recall:6.2f}%\")\n",
    "    \n",
    "    # F1 metrics\n",
    "    print(\"\\n‚öñÔ∏è  F1-SCORE METRICS:\")\n",
    "    for k in [1, 3, 5, 10]:\n",
    "        f1 = avg_metrics[f'avg_f1@{k}'] * 100\n",
    "        print(f\"  F1@{k:2d}:        {f1:6.2f}%\")\n",
    "    \n",
    "    # MRR\n",
    "    mrr = avg_metrics['avg_mrr']\n",
    "    print(f\"\\nü•á MEAN RECIPROCAL RANK: {mrr:.4f}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"‚úÖ THESE ARE REAL METRICS - NO SIMULATION!\")\n",
    "    print(\"üî¨ Based on actual cosine similarity with real embeddings\")\n",
    "    print(\"üìä Ground truth: Microsoft Learn documentation links\")\n",
    "else:\n",
    "    print(\"‚ùå No metrics calculated\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "detailed-analysis"
   },
   "source": [
    "## üî¨ Detailed Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "detailed-metrics"
   },
   "outputs": [],
   "source": [
    "# Show best and worst performing questions\n",
    "if all_metrics:\n",
    "    # Sort by MRR\n",
    "    sorted_by_mrr = sorted(all_metrics, key=lambda x: x['mrr'], reverse=True)\n",
    "    \n",
    "    print(\"üèÜ TOP 5 BEST PERFORMING QUESTIONS (by MRR):\")\n",
    "    for i, metric in enumerate(sorted_by_mrr[:5]):\n",
    "        print(f\"  {i+1}. MRR: {metric['mrr']:.3f} | P@5: {metric['precision@5']:.3f} | Q: {metric['question'][:60]}...\")\n",
    "    \n",
    "    print(\"\\nüìâ TOP 5 WORST PERFORMING QUESTIONS (by MRR):\")\n",
    "    for i, metric in enumerate(sorted_by_mrr[-5:]):\n",
    "        print(f\"  {i+1}. MRR: {metric['mrr']:.3f} | P@5: {metric['precision@5']:.3f} | Q: {metric['question'][:60]}...\")\n",
    "    \n",
    "    # Statistics\n",
    "    mrr_values = [m['mrr'] for m in all_metrics]\n",
    "    precision5_values = [m['precision@5'] for m in all_metrics]\n",
    "    \n",
    "    print(f\"\\nüìä STATISTICS:\")\n",
    "    print(f\"  MRR - Min: {min(mrr_values):.3f}, Max: {max(mrr_values):.3f}, Std: {np.std(mrr_values):.3f}\")\n",
    "    print(f\"  P@5 - Min: {min(precision5_values):.3f}, Max: {max(precision5_values):.3f}, Std: {np.std(precision5_values):.3f}\")\n",
    "    \n",
    "    # Perfect matches\n",
    "    perfect_mrr = len([m for m in all_metrics if m['mrr'] == 1.0])\n",
    "    zero_mrr = len([m for m in all_metrics if m['mrr'] == 0.0])\n",
    "    \n",
    "    print(f\"\\nüéØ PERFORMANCE DISTRIBUTION:\")\n",
    "    print(f\"  Perfect matches (MRR=1.0): {perfect_mrr} ({perfect_mrr/len(all_metrics)*100:.1f}%)\")\n",
    "    print(f\"  No matches (MRR=0.0):      {zero_mrr} ({zero_mrr/len(all_metrics)*100:.1f}%)\")\n",
    "    print(f\"  Partial matches:           {len(all_metrics)-perfect_mrr-zero_mrr} ({(len(all_metrics)-perfect_mrr-zero_mrr)/len(all_metrics)*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "save-results"
   },
   "source": [
    "## üíæ Save Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "save-output"
   },
   "outputs": [],
   "source": [
    "# Save detailed results\n",
    "results = {\n",
    "    'evaluation_config': {\n",
    "        'embedding_model': EMBEDDING_MODEL_TO_EVALUATE,\n",
    "        'query_model': query_model_name,\n",
    "        'document_corpus': EMBEDDING_FILES[EMBEDDING_MODEL_TO_EVALUATE],\n",
    "        'num_questions_evaluated': len(all_metrics),\n",
    "        'total_documents': retriever.num_docs,\n",
    "        'embedding_dimensions': retriever.embedding_dim,\n",
    "        'evaluation_timestamp': datetime.now().isoformat()\n",
    "    },\n",
    "    'average_metrics': avg_metrics,\n",
    "    'individual_metrics': all_metrics\n",
    "}\n",
    "\n",
    "# Save to Google Drive\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "output_file = f\"{BASE_PATH}real_evaluation_{EMBEDDING_MODEL_TO_EVALUATE}_{timestamp}.json\"\n",
    "\n",
    "with open(output_file, 'w', encoding='utf-8') as f:\n",
    "    json.dump(results, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "print(f\"üíæ Results saved to: {output_file}\")\n",
    "print(f\"üìä File size: {len(json.dumps(results)) / (1024*1024):.1f} MB\")\n",
    "\n",
    "# Cleanup memory\n",
    "del retriever\n",
    "del query_model\n",
    "gc.collect()\n",
    "\n",
    "print(\"\\n‚úÖ Evaluation completed and saved!\")\n",
    "print(\"üéâ You now have real retrieval metrics based on actual embeddings!\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  },
  "accelerator": "GPU",
  "gpuClass": "standard"
 },
 "nbformat": 4,
 "nbformat_minor": 0
}