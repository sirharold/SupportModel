"""
PÃ¡gina de MÃ©tricas Acumulativas - EvalÃºa mÃºltiples preguntas y calcula promedios (REFACTORIZADA)
"""

import streamlit as st
import time
from typing import List, Dict, Any
from config import EMBEDDING_MODELS, GENERATIVE_MODELS, WEAVIATE_CLASS_CONFIG

# Importar utilidades refactorizadas
from utils.memory_utils import get_memory_usage, cleanup_memory
# from utils.data_processing import filter_questions_with_links  # No longer needed
from utils.metrics_display import display_cumulative_metrics, display_models_comparison
from utils.cumulative_evaluation import run_cumulative_metrics_evaluation, run_cumulative_metrics_for_models
from utils.file_utils import display_download_section
from utils.metrics import validate_data_integrity


def show_cumulative_metrics_page():
    """FunciÃ³n principal que muestra la pÃ¡gina de mÃ©tricas acumulativas."""
    st.title("ğŸ“Š MÃ©tricas Acumulativas de RecuperaciÃ³n RAG")
    st.markdown("""
    Esta pÃ¡gina permite evaluar mÃºltiples preguntas y calcular mÃ©tricas promedio para diferentes modelos de embedding.
    """)
    
    # ConfiguraciÃ³n inicial
    col1, col2 = st.columns(2)
    
    with col1:
        st.subheader("âš™ï¸ ConfiguraciÃ³n de EvaluaciÃ³n")
        
        # InformaciÃ³n sobre la fuente de datos
        st.info("ğŸ“Š Las preguntas se extraen aleatoriamente desde Weaviate, filtrando solo aquellas que tienen enlaces de Microsoft Learn en la respuesta aceptada.")
        
        # NÃºmero de preguntas
        num_questions = st.number_input(
            "ğŸ”¢ NÃºmero de preguntas a evaluar:",
            min_value=1,
            max_value=2000,
            value=50,
            step=1,
            help="NÃºmero total de preguntas para la evaluaciÃ³n"
        )
        
        # ConfiguraciÃ³n de modelo generativo
        generative_model_name = st.selectbox(
            "ğŸ¤– Modelo Generativo:",
            list(GENERATIVE_MODELS.keys()),
            index=0,
            help="Modelo usado para reranking LLM"
        )
        
        # ConfiguraciÃ³n de recuperaciÃ³n
        top_k = st.number_input(
            "ğŸ” Top-K documentos:",
            min_value=1,
            max_value=50,
            value=10,
            step=1,
            help="NÃºmero mÃ¡ximo de documentos a recuperar"
        )
        
        # Usar reranking LLM
        use_llm_reranker = st.checkbox(
            "ğŸ¤– Usar Reranking LLM",
            value=True,
            help="Activar reordenamiento de documentos con LLM"
        )
        
        # TamaÃ±o de lote
        batch_size = st.number_input(
            "ğŸ“¦ TamaÃ±o de lote:",
            min_value=10,
            max_value=100,
            value=50,
            step=10,
            help="NÃºmero de preguntas a procesar por lote (para gestiÃ³n de memoria)"
        )
    
    with col2:
        st.subheader("ğŸ¯ SelecciÃ³n de Modelos")
        
        # OpciÃ³n para evaluar todos los modelos
        evaluate_all_models = st.checkbox(
            "ğŸ”„ Evaluar todos los modelos",
            value=False,
            help="Evaluar todos los modelos de embedding disponibles"
        )
        
        if evaluate_all_models:
            selected_models = list(EMBEDDING_MODELS.keys())
            st.info(f"ğŸ“‹ Se evaluarÃ¡n {len(selected_models)} modelos")
            for model in selected_models:
                st.markdown(f"â€¢ {model}")
        else:
            # SelecciÃ³n manual de modelos
            selected_models = st.multiselect(
                "ğŸ›ï¸ Modelos de Embedding:",
                list(EMBEDDING_MODELS.keys()),
                default=[list(EMBEDDING_MODELS.keys())[0]],
                help="Selecciona uno o mÃ¡s modelos para evaluar"
            )
        
        if not selected_models:
            st.error("âŒ Debes seleccionar al menos un modelo")
            return
        
        # InformaciÃ³n sobre memoria estimada
        estimated_memory = len(selected_models) * num_questions * 0.1  # MB aproximados
        st.info(f"ğŸ’¾ Memoria estimada: ~{estimated_memory:.1f} MB")
    
    # Botones de control
    st.markdown("---")
    col1, col2, col3 = st.columns(3)
    
    with col1:
        if st.button("ğŸš€ Ejecutar EvaluaciÃ³n", type="primary"):
            run_evaluation(
                num_questions, selected_models, 
                generative_model_name, top_k, use_llm_reranker, 
                batch_size, evaluate_all_models
            )
    
    with col2:
        if st.button("ğŸ§¹ Limpiar CachÃ©"):
            for key in list(st.session_state.keys()):
                if 'cumulative_results' in key:
                    del st.session_state[key]
            st.success("âœ… CachÃ© limpiado")
            st.rerun()
    
    with col3:
        if st.button("ğŸ“Š Mostrar EstadÃ­sticas de Memoria"):
            show_memory_stats()
    
    # Mostrar resultados si existen
    show_cached_results()


def run_evaluation(num_questions: int, selected_models: List[str],
                  generative_model_name: str, top_k: int, use_llm_reranker: bool,
                  batch_size: int, evaluate_all_models: bool):
    """Ejecuta la evaluaciÃ³n de mÃ©tricas."""
    
    # Mostrar informaciÃ³n de memoria inicial
    initial_memory = get_memory_usage()
    st.info(f"ğŸ’¾ Memoria inicial: {initial_memory:.1f} MB")
    
    # Ejecutar evaluaciÃ³n
    start_time = time.time()
    evaluation_time = time.strftime("%Y-%m-%d %H:%M:%S")
    
    try:
        if evaluate_all_models:
            with st.spinner(f"ğŸ”„ Evaluando {len(selected_models)} modelos..."):
                results = run_cumulative_metrics_for_models(
                    num_questions=num_questions,
                    model_names=selected_models,
                    generative_model_name=generative_model_name,
                    top_k=top_k,
                    use_llm_reranker=use_llm_reranker,
                    batch_size=batch_size
                )
        else:
            # EvaluaciÃ³n de modelo Ãºnico
            model_name = selected_models[0]
            with st.spinner(f"âš™ï¸ Evaluando {model_name}..."):
                single_result = run_cumulative_metrics_evaluation(
                    num_questions=num_questions,
                    model_name=model_name,
                    generative_model_name=generative_model_name,
                    top_k=top_k,
                    use_llm_reranker=use_llm_reranker,
                    batch_size=batch_size
                )
                results = {model_name: single_result}
        
        # Validar integridad de datos
        for model_name, result in results.items():
            results[model_name] = validate_data_integrity(result)
        
        end_time = time.time()
        execution_time = end_time - start_time
        
        # Guardar en cachÃ©
        cache_key = f"cumulative_results_{len(selected_models)}_{num_questions}_{int(start_time)}"
        st.session_state[cache_key] = {
            'results': results,
            'evaluation_time': evaluation_time,
            'execution_time': execution_time,
            'evaluate_all_models': evaluate_all_models,
            'params': {
                'num_questions': num_questions,
                'selected_models': selected_models,
                'embedding_model_name': selected_models[0] if len(selected_models) == 1 else 'Multi-Model',
                'generative_model_name': generative_model_name,
                'top_k': top_k,
                'use_llm_reranker': use_llm_reranker,
                'batch_size': batch_size
            }
        }
        
        st.success(f"âœ… EvaluaciÃ³n completada en {execution_time:.2f} segundos")
        
        # Mostrar memoria final
        final_memory = get_memory_usage()
        memory_increase = final_memory - initial_memory
        st.info(f"ğŸ’¾ Memoria final: {final_memory:.1f} MB (+{memory_increase:.1f} MB)")
        
        # Forzar rerun para mostrar resultados
        st.rerun()
        
    except Exception as e:
        st.error(f"âŒ Error durante la evaluaciÃ³n: {str(e)}")
        st.exception(e)
    finally:
        # Limpieza de memoria
        cleanup_memory()


def show_cached_results():
    """Muestra los resultados cacheados si existen."""
    cached_keys = [k for k in st.session_state.keys() if 'cumulative_results' in k]
    
    if not cached_keys:
        st.info("â„¹ï¸ No hay resultados de evaluaciÃ³n. Ejecuta una evaluaciÃ³n para ver los resultados.")
        return
    
    # Seleccionar quÃ© resultados mostrar
    if len(cached_keys) > 1:
        st.subheader("ğŸ“‹ Resultados Disponibles")
        selected_key = st.selectbox(
            "Selecciona resultados:",
            cached_keys,
            format_func=lambda x: f"EvaluaciÃ³n {x.split('_')[-1]} ({st.session_state[x]['params']['embedding_model_name']})"
        )
    else:
        selected_key = cached_keys[0]
    
    if selected_key not in st.session_state:
        return
    
    cached_results = st.session_state[selected_key]
    results = cached_results['results']
    evaluate_all_models = cached_results['evaluate_all_models']
    params = cached_results['params']
    
    st.markdown("---")
    
    # Mostrar resultados
    if evaluate_all_models:
        st.subheader("ğŸ“ˆ ComparaciÃ³n Multi-Modelo")
        display_models_comparison(results, params['use_llm_reranker'])
    else:
        st.subheader(f"ğŸ“Š Resultados para {params['embedding_model_name']}")
        model_name = list(results.keys())[0]
        display_cumulative_metrics(results[model_name], model_name, params['use_llm_reranker'])
    
    # SecciÃ³n de descarga
    st.markdown("---")
    display_download_section(cached_results)


def show_memory_stats():
    """Muestra estadÃ­sticas detalladas de memoria."""
    current_memory = get_memory_usage()
    
    st.subheader("ğŸ“Š EstadÃ­sticas de Memoria")
    st.metric("Memoria Actual", f"{current_memory:.1f} MB")
    
    # Contar elementos en cachÃ©
    cache_count = len([k for k in st.session_state.keys() if 'cumulative_results' in k])
    st.metric("Resultados en CachÃ©", cache_count)
    
    if cache_count > 0:
        st.info("ğŸ’¡ Usa 'Limpiar CachÃ©' para liberar memoria")


if __name__ == "__main__":
    show_cumulative_metrics_page()