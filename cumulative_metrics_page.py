"""
PÃ¡gina de MÃ©tricas Acumulativas - EvalÃºa mÃºltiples preguntas y calcula promedios
"""

import streamlit as st
import pandas as pd
import numpy as np
import time
import json
import re
from typing import List, Dict, Any
from utils.clients import initialize_clients
from utils.qa_pipeline_with_metrics import answer_question_with_retrieval_metrics
from config import EMBEDDING_MODELS, GENERATIVE_MODELS, WEAVIATE_CLASS_CONFIG
import plotly.express as px
import plotly.graph_objects as go
from plotly.subplots import make_subplots

def extract_ms_links(accepted_answer: str) -> List[str]:
    """
    Extrae links de Microsoft Learn de la respuesta aceptada.
    
    Args:
        accepted_answer: Texto de la respuesta aceptada
        
    Returns:
        Lista de links de Microsoft Learn encontrados
    """
    # PatrÃ³n para encontrar links de Microsoft Learn
    pattern = r'https://learn\.microsoft\.com[\w/\-\?=&%\.]+'
    links = re.findall(pattern, accepted_answer)
    return list(set(links))  # Eliminar duplicados

def filter_questions_with_links(questions_and_answers: List[Dict]) -> List[Dict]:
    """
    Filtra preguntas que tienen links en la respuesta aceptada.
    
    Args:
        questions_and_answers: Lista de preguntas y respuestas
        
    Returns:
        Lista filtrada de preguntas con links en respuesta aceptada
    """
    filtered_questions = []
    
    for qa in questions_and_answers:
        accepted_answer = qa.get('accepted_answer', '')
        
        # Extraer links de Microsoft Learn
        ms_links = extract_ms_links(accepted_answer)
        
        # Solo incluir si hay links
        if ms_links and len(ms_links) > 0:
            # AÃ±adir los links extraÃ­dos al diccionario
            qa_copy = qa.copy()
            qa_copy['ms_links'] = ms_links
            qa_copy['question'] = qa.get('question_content', qa.get('title', ''))
            filtered_questions.append(qa_copy)
    
    return filtered_questions

def calculate_average_metrics(all_metrics: List[Dict]) -> Dict[str, float]:
    """
    Calcula mÃ©tricas promedio de una lista de mÃ©tricas.
    
    Args:
        all_metrics: Lista de diccionarios con mÃ©tricas
        
    Returns:
        Diccionario con mÃ©tricas promedio
    """
    if not all_metrics:
        return {}
    
    # Inicializar contadores
    metric_sums = {}
    metric_counts = {}
    
    # Sumar todas las mÃ©tricas
    for metrics in all_metrics:
        for key, value in metrics.items():
            if isinstance(value, (int, float)) and not np.isnan(value):
                if key not in metric_sums:
                    metric_sums[key] = 0
                    metric_counts[key] = 0
                metric_sums[key] += value
                metric_counts[key] += 1
    
    # Calcular promedios
    average_metrics = {}
    for key in metric_sums:
        if metric_counts[key] > 0:
            average_metrics[key] = metric_sums[key] / metric_counts[key]
    
    return average_metrics

def run_cumulative_metrics_evaluation(
    questions_and_answers: List[Dict],
    num_questions: int,
    model_name: str,
    generative_model_name: str,
    top_k: int = 10,
    use_llm_reranker: bool = True
) -> Dict[str, Any]:
    """
    Ejecuta evaluaciÃ³n de mÃ©tricas acumulativas para mÃºltiples preguntas.
    
    Args:
        questions_and_answers: Lista de preguntas y respuestas
        num_questions: NÃºmero de preguntas a evaluar
        model_name: Nombre del modelo de embedding
        generative_model_name: Nombre del modelo generativo
        top_k: NÃºmero de documentos top-k
        use_llm_reranker: Si usar LLM reranking
        
    Returns:
        Diccionario con resultados de evaluaciÃ³n
    """
    # Filtrar preguntas con links
    filtered_questions = filter_questions_with_links(questions_and_answers)
    
    if len(filtered_questions) < num_questions:
        st.warning(f"âš ï¸ Solo hay {len(filtered_questions)} preguntas con links disponibles. Se usarÃ¡n todas.")
        num_questions = len(filtered_questions)
    
    # Seleccionar preguntas aleatoriamente
    if len(filtered_questions) > num_questions:
        selected_indices = np.random.choice(len(filtered_questions), num_questions, replace=False)
        selected_questions = [filtered_questions[i] for i in selected_indices]
    else:
        selected_questions = filtered_questions
    
    # Inicializar clientes
    weaviate_wrapper, embedding_client, openai_client, gemini_client, local_tinyllama_client, local_mistral_client, openrouter_client, _ = initialize_clients(
        model_name, generative_model_name
    )
    
    # Listas para almacenar mÃ©tricas
    before_reranking_metrics = []
    after_reranking_metrics = []
    all_questions_data = []
    
    # Barra de progreso
    progress_bar = st.progress(0)
    status_text = st.empty()
    
    # Evaluar cada pregunta
    for i, qa in enumerate(selected_questions):
        question = qa['question']
        ground_truth_answer = qa.get('accepted_answer', '')
        ms_links = qa.get('ms_links', [])
        
        status_text.text(f"Evaluando pregunta {i+1}/{num_questions}...")
        
        try:
            # Ejecutar pipeline con mÃ©tricas
            result = answer_question_with_retrieval_metrics(
                question=question,
                weaviate_wrapper=weaviate_wrapper,
                embedding_client=embedding_client,
                openai_client=openai_client,
                gemini_client=gemini_client,
                local_tinyllama_client=local_tinyllama_client,
                local_mistral_client=local_mistral_client,
                openrouter_client=openrouter_client,
                top_k=top_k,
                use_llm_reranker=use_llm_reranker,
                generate_answer=False,  # Sin RAG como se solicitÃ³
                calculate_metrics=True,
                ground_truth_answer=ground_truth_answer,
                ms_links=ms_links,
                generative_model_name=generative_model_name
            )
            
            if len(result) >= 3:
                docs, debug_info, retrieval_metrics = result
                
                # Extraer mÃ©tricas antes y despuÃ©s del reranking
                before_metrics = retrieval_metrics.get('before_reranking', {})
                after_metrics = retrieval_metrics.get('after_reranking', {})
                
                before_reranking_metrics.append(before_metrics)
                after_reranking_metrics.append(after_metrics)
                
                # Almacenar datos de la pregunta para referencia
                all_questions_data.append({
                    'question_num': i + 1,
                    'question': question[:100] + '...' if len(question) > 100 else question,
                    'ground_truth_links': len(ms_links),
                    'docs_retrieved': len(docs),
                    'before_precision_5': before_metrics.get('Precision@5', 0),
                    'after_precision_5': after_metrics.get('Precision@5', 0),
                    'before_recall_5': before_metrics.get('Recall@5', 0),
                    'after_recall_5': after_metrics.get('Recall@5', 0),
                    'before_f1_5': before_metrics.get('F1@5', 0),
                    'after_f1_5': after_metrics.get('F1@5', 0)
                })
                
        except Exception as e:
            st.error(f"Error evaluando pregunta {i+1}: {e}")
            continue
        
        progress_bar.progress((i + 1) / num_questions)
    
    # Calcular mÃ©tricas promedio
    avg_before_metrics = calculate_average_metrics(before_reranking_metrics)
    avg_after_metrics = calculate_average_metrics(after_reranking_metrics)
    
    # Limpiar interfaz
    progress_bar.empty()
    status_text.empty()
    
    return {
        'num_questions_evaluated': len(before_reranking_metrics),
        'avg_before_metrics': avg_before_metrics,
        'avg_after_metrics': avg_after_metrics,
        'all_questions_data': all_questions_data,
        'individual_before_metrics': before_reranking_metrics,
        'individual_after_metrics': after_reranking_metrics
    }

def display_cumulative_metrics(results: Dict[str, Any], model_name: str, use_llm_reranker: bool):
    """
    Muestra los resultados de mÃ©tricas acumulativas en la interfaz.
    
    Args:
        results: Resultados de la evaluaciÃ³n
        model_name: Nombre del modelo usado
        use_llm_reranker: Si se usÃ³ LLM reranking
    """
    num_questions = results['num_questions_evaluated']
    avg_before = results['avg_before_metrics']
    avg_after = results['avg_after_metrics']
    
    st.success(f"âœ… EvaluaciÃ³n completada para {num_questions} preguntas")
    
    # MÃ©tricas principales en columnas
    st.subheader("ğŸ“Š MÃ©tricas Promedio")
    
    # MÃ©tricas principales
    main_metrics = ['Precision@5', 'Recall@5', 'F1@5', 'MRR@5', 'nDCG@5']
    
    col1, col2 = st.columns(2)
    
    with col1:
        st.markdown("**ğŸ” Antes del Reranking**")
        for metric in main_metrics:
            if metric in avg_before:
                st.metric(
                    label=metric,
                    value=f"{avg_before[metric]:.3f}",
                    help=f"Promedio de {metric} antes del reranking LLM"
                )
    
    with col2:
        if use_llm_reranker:
            st.markdown("**ğŸ¤– DespuÃ©s del Reranking LLM**")
            for metric in main_metrics:
                if metric in avg_after:
                    # Calcular delta
                    delta = avg_after[metric] - avg_before.get(metric, 0)
                    st.metric(
                        label=metric,
                        value=f"{avg_after[metric]:.3f}",
                        delta=f"{delta:+.3f}",
                        help=f"Promedio de {metric} despuÃ©s del reranking LLM"
                    )
        else:
            st.info("â„¹ï¸ Reranking LLM deshabilitado")
    
    # GrÃ¡fico de comparaciÃ³n
    if use_llm_reranker and avg_before and avg_after:
        st.subheader("ğŸ“ˆ ComparaciÃ³n Visual")
        
        # Preparar datos para el grÃ¡fico
        metrics_to_plot = ['Precision@5', 'Recall@5', 'F1@5', 'MRR@5', 'nDCG@5']
        before_values = [avg_before.get(m, 0) for m in metrics_to_plot]
        after_values = [avg_after.get(m, 0) for m in metrics_to_plot]
        
        # Crear grÃ¡fico de barras comparativo
        fig = go.Figure()
        fig.add_trace(go.Bar(
            name='Antes del Reranking',
            x=metrics_to_plot,
            y=before_values,
            marker_color='lightblue'
        ))
        fig.add_trace(go.Bar(
            name='DespuÃ©s del Reranking',
            x=metrics_to_plot,
            y=after_values,
            marker_color='darkblue'
        ))
        
        fig.update_layout(
            title=f'ComparaciÃ³n de MÃ©tricas Promedio ({num_questions} preguntas)',
            xaxis_title='MÃ©tricas',
            yaxis_title='Valor Promedio',
            barmode='group',
            height=400
        )
        
        st.plotly_chart(fig, use_container_width=True)
    
    # MÃ©tricas adicionales en expander
    with st.expander("ğŸ“‹ MÃ©tricas Detalladas"):
        col1, col2 = st.columns(2)
        
        with col1:
            st.markdown("**Antes del Reranking**")
            before_df = pd.DataFrame([avg_before]).T
            before_df.columns = ['Valor Promedio']
            before_df.index.name = 'MÃ©trica'
            st.dataframe(before_df.style.format({'Valor Promedio': '{:.4f}'}))
        
        with col2:
            if use_llm_reranker and avg_after:
                st.markdown("**DespuÃ©s del Reranking**")
                after_df = pd.DataFrame([avg_after]).T
                after_df.columns = ['Valor Promedio']
                after_df.index.name = 'MÃ©trica'
                st.dataframe(after_df.style.format({'Valor Promedio': '{:.4f}'}))
    
    # Tabla de evoluciÃ³n por pregunta
    with st.expander("ğŸ“Š EvoluciÃ³n por Pregunta"):
        questions_df = pd.DataFrame(results['all_questions_data'])
        if not questions_df.empty:
            # Renombrar columnas para mejor legibilidad
            column_names = {
                'question_num': 'Pregunta #',
                'ground_truth_links': 'Links GT',
                'docs_retrieved': 'Docs Recuperados',
                'before_precision_5': 'Precision@5 (Antes)',
                'after_precision_5': 'Precision@5 (DespuÃ©s)',
                'before_f1_5': 'F1@5 (Antes)',
                'after_f1_5': 'F1@5 (DespuÃ©s)',
                'before_recall_5': 'Recall@5 (Antes)',
                'after_recall_5': 'Recall@5 (DespuÃ©s)'
            }
            
            # Mostrar tabla con mÃ©tricas por pregunta
            if use_llm_reranker:
                display_columns = ['question_num', 'ground_truth_links', 'docs_retrieved', 
                                 'before_precision_5', 'after_precision_5', 
                                 'before_f1_5', 'after_f1_5']
            else:
                display_columns = ['question_num', 'ground_truth_links', 'docs_retrieved', 
                                 'before_precision_5', 'before_f1_5']
            
            questions_df_display = questions_df[display_columns].rename(columns=column_names)
            
            st.dataframe(questions_df_display.style.format({
                'Precision@5 (Antes)': '{:.3f}',
                'Precision@5 (DespuÃ©s)': '{:.3f}',
                'F1@5 (Antes)': '{:.3f}',
                'F1@5 (DespuÃ©s)': '{:.3f}'
            }), use_container_width=True)

def load_questions_from_json(file_path: str) -> List[Dict]:
    """
    Carga preguntas desde archivo JSON.
    
    Args:
        file_path: Ruta al archivo JSON
        
    Returns:
        Lista de preguntas y respuestas
    """
    try:
        with open(file_path, 'r', encoding='utf-8') as f:
            return json.load(f)
    except FileNotFoundError:
        st.error(f"âŒ No se encontrÃ³ el archivo '{file_path}'")
        return []
    except json.JSONDecodeError as e:
        st.error(f"âŒ Error al leer el archivo JSON: {e}")
        return []

def show_cumulative_metrics_page():
    """Muestra la pÃ¡gina de mÃ©tricas acumulativas."""
    st.title("ğŸ“ˆ MÃ©tricas Acumulativas")
    st.markdown("""
    Esta pÃ¡gina evalÃºa mÃºltiples preguntas y calcula **mÃ©tricas promedio** para obtener una visiÃ³n general 
    del rendimiento del sistema. Solo se evalÃºan preguntas que tienen links en la respuesta aceptada.
    """)
    
    # OpciÃ³n para seleccionar dataset
    dataset_option = st.selectbox(
        "Seleccionar dataset:",
        ["ğŸ“Š Dataset Completo (train + val)", "ğŸ“‹ Solo ValidaciÃ³n", "ğŸ¯ Solo Entrenamiento"],
        index=0,
        help="Elige quÃ© conjunto de datos usar para la evaluaciÃ³n"
    )
    
    # Cargar preguntas segÃºn la selecciÃ³n del usuario
    val_questions = load_questions_from_json('data/val_set.json')
    train_questions = load_questions_from_json('data/train_set.json')
    
    # Seleccionar dataset basado en la opciÃ³n del usuario
    questions_and_answers = []
    if dataset_option == "ğŸ“Š Dataset Completo (train + val)":
        if val_questions:
            questions_and_answers.extend(val_questions)
        if train_questions:
            questions_and_answers.extend(train_questions)
    elif dataset_option == "ğŸ“‹ Solo ValidaciÃ³n":
        if val_questions:
            questions_and_answers = val_questions
        else:
            st.error("âŒ No se pudo cargar el dataset de validaciÃ³n")
            st.stop()
    elif dataset_option == "ğŸ¯ Solo Entrenamiento":
        if train_questions:
            questions_and_answers = train_questions
        else:
            st.error("âŒ No se pudo cargar el dataset de entrenamiento")
            st.stop()
    
    if not questions_and_answers:
        st.error("âŒ No hay preguntas disponibles en el dataset seleccionado")
        st.stop()
    
    # Filtrar preguntas con links
    filtered_questions = filter_questions_with_links(questions_and_answers)
    
    # Mostrar estadÃ­sticas del dataset
    if dataset_option == "ğŸ“Š Dataset Completo (train + val)":
        dataset_info = []
        if val_questions:
            dataset_info.append(f"ValidaciÃ³n: {len(val_questions)}")
        if train_questions:
            dataset_info.append(f"Entrenamiento: {len(train_questions)}")
        st.info(f"ğŸ“Š **Dataset seleccionado**: {len(questions_and_answers)} preguntas total ({', '.join(dataset_info)})")
    elif dataset_option == "ğŸ“‹ Solo ValidaciÃ³n":
        st.info(f"ğŸ“Š **Dataset seleccionado**: {len(questions_and_answers)} preguntas del conjunto de validaciÃ³n")
    elif dataset_option == "ğŸ¯ Solo Entrenamiento":
        st.info(f"ğŸ“Š **Dataset seleccionado**: {len(questions_and_answers)} preguntas del conjunto de entrenamiento")
    
    st.info(f"ğŸ”— **Preguntas con enlaces MS Learn**: {len(filtered_questions)} preguntas disponibles para evaluaciÃ³n")
    
    if len(filtered_questions) == 0:
        st.error("âŒ No hay preguntas con enlaces de Microsoft Learn para evaluar")
        st.stop()
    
    # ConfiguraciÃ³n
    st.subheader("âš™ï¸ ConfiguraciÃ³n de EvaluaciÃ³n")
    
    col1, col2 = st.columns(2)
    
    with col1:
        # NÃºmero de preguntas
        num_questions = st.slider(
            "NÃºmero de preguntas a evaluar",
            min_value=1,
            max_value=min(50, len(filtered_questions)),
            value=5,
            help="Cantidad de preguntas para evaluar (tomadas aleatoriamente)"
        )
        
        # Modelo de embedding
        model_name = st.selectbox(
            "Modelo de Embedding",
            options=list(EMBEDDING_MODELS.keys()),
            index=0,
            help="Modelo para generar embeddings de documentos"
        )
    
    with col2:
        # ConfiguraciÃ³n de retrieval
        top_k = st.slider(
            "Top-K documentos",
            min_value=5,
            max_value=20,
            value=10,
            help="NÃºmero de documentos a recuperar"
        )
        
        # LLM Reranking
        use_llm_reranker = st.checkbox(
            "Usar LLM Reranking",
            value=True,
            help="Usar GPT-4 para reordenar documentos (necesario para mÃ©tricas antes/despuÃ©s)"
        )
    
    # Modelo generativo (solo para reranking)
    generative_model_name = st.selectbox(
        "Modelo Generativo (para reranking)",
        options=list(GENERATIVE_MODELS.keys()),
        index=0,
        help="Modelo usado para el reranking LLM"
    )
    
    # BotÃ³n para ejecutar evaluaciÃ³n
    if st.button("ğŸš€ Ejecutar EvaluaciÃ³n", type="primary"):
        if len(filtered_questions) < num_questions:
            st.error(f"âŒ Solo hay {len(filtered_questions)} preguntas con links disponibles")
            st.stop()
        
        with st.spinner(f"ğŸ” Evaluando {num_questions} preguntas..."):
            start_time = time.time()
            
            results = run_cumulative_metrics_evaluation(
                questions_and_answers=filtered_questions,
                num_questions=num_questions,
                model_name=model_name,
                generative_model_name=generative_model_name,
                top_k=top_k,
                use_llm_reranker=use_llm_reranker
            )
            
            evaluation_time = time.time() - start_time
            
            # Mostrar resultados
            display_cumulative_metrics(results, model_name, use_llm_reranker)
            
            # EstadÃ­sticas adicionales
            st.markdown("---")
            st.subheader("ğŸ“Š EstadÃ­sticas de EvaluaciÃ³n")
            
            col1, col2, col3 = st.columns(3)
            
            with col1:
                st.metric(
                    "ğŸ“ Preguntas Evaluadas",
                    f"{results['num_questions_evaluated']}",
                    help="NÃºmero total de preguntas procesadas"
                )
            
            with col2:
                if results['all_questions_data']:
                    avg_gt_links = np.mean([q['ground_truth_links'] for q in results['all_questions_data']])
                    st.metric(
                        "ğŸ”— Links GT Promedio",
                        f"{avg_gt_links:.1f}",
                        help="NÃºmero promedio de links de ground truth por pregunta"
                    )
            
            with col3:
                st.metric(
                    "â±ï¸ Tiempo Total",
                    f"{evaluation_time:.1f}s",
                    help="Tiempo total de evaluaciÃ³n"
                )
            
            # OpciÃ³n para descargar resultados
            st.markdown("---")
            col1, col2 = st.columns(2)
            
            with col1:
                if st.button("ğŸ’¾ Descargar Resultados Detallados"):
                    results_df = pd.DataFrame(results['all_questions_data'])
                    csv = results_df.to_csv(index=False)
                    st.download_button(
                        label="Descargar CSV Detallado",
                        data=csv,
                        file_name=f"cumulative_metrics_detailed_{model_name}_{num_questions}q.csv",
                        mime="text/csv"
                    )
            
            with col2:
                if st.button("ğŸ“Š Descargar MÃ©tricas Promedio"):
                    # Crear CSV con mÃ©tricas promedio
                    avg_metrics_data = {
                        'Metric': [],
                        'Before_Reranking': [],
                        'After_Reranking': []
                    }
                    
                    for metric in ['Precision@5', 'Recall@5', 'F1@5', 'MRR@5', 'nDCG@5']:
                        avg_metrics_data['Metric'].append(metric)
                        avg_metrics_data['Before_Reranking'].append(results['avg_before_metrics'].get(metric, 0))
                        avg_metrics_data['After_Reranking'].append(results['avg_after_metrics'].get(metric, 0))
                    
                    avg_df = pd.DataFrame(avg_metrics_data)
                    csv = avg_df.to_csv(index=False)
                    st.download_button(
                        label="Descargar CSV Promedio",
                        data=csv,
                        file_name=f"cumulative_metrics_avg_{model_name}_{num_questions}q.csv",
                        mime="text/csv"
                    )