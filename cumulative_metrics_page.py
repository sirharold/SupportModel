"""
P√°gina de M√©tricas Acumulativas - Eval√∫a m√∫ltiples preguntas y calcula promedios
"""

import streamlit as st
import pandas as pd
import numpy as np
import time
import json
import re
from typing import List, Dict, Any
from utils.clients import initialize_clients
from utils.qa_pipeline_with_metrics import answer_question_with_retrieval_metrics
from config import EMBEDDING_MODELS, GENERATIVE_MODELS, WEAVIATE_CLASS_CONFIG
import plotly.express as px
import plotly.graph_objects as go
from plotly.subplots import make_subplots

def extract_ms_links(accepted_answer: str) -> List[str]:
    """
    Extrae links de Microsoft Learn de la respuesta aceptada.
    
    Args:
        accepted_answer: Texto de la respuesta aceptada
        
    Returns:
        Lista de links de Microsoft Learn encontrados
    """
    # Patr√≥n para encontrar links de Microsoft Learn
    pattern = r'https://learn\.microsoft\.com[\w/\-\?=&%\.]+'
    links = re.findall(pattern, accepted_answer)
    return list(set(links))  # Eliminar duplicados

def filter_questions_with_links(questions_and_answers: List[Dict]) -> List[Dict]:
    """
    Filtra preguntas que tienen links en la respuesta aceptada.
    
    Args:
        questions_and_answers: Lista de preguntas y respuestas
        
    Returns:
        Lista filtrada de preguntas con links en respuesta aceptada
    """
    filtered_questions = []
    
    for qa in questions_and_answers:
        accepted_answer = qa.get('accepted_answer', '')
        
        # Extraer links de Microsoft Learn
        ms_links = extract_ms_links(accepted_answer)
        
        # Solo incluir si hay links
        if ms_links and len(ms_links) > 0:
            # A√±adir los links extra√≠dos al diccionario
            qa_copy = qa.copy()
            qa_copy['ms_links'] = ms_links
            qa_copy['question'] = qa.get('question_content', qa.get('title', ''))
            filtered_questions.append(qa_copy)
    
    return filtered_questions

def calculate_average_metrics(all_metrics: List[Dict]) -> Dict[str, float]:
    """
    Calcula m√©tricas promedio de una lista de m√©tricas.
    
    Args:
        all_metrics: Lista de diccionarios con m√©tricas
        
    Returns:
        Diccionario con m√©tricas promedio
    """
    if not all_metrics:
        return {}
    
    # Inicializar contadores
    metric_sums = {}
    metric_counts = {}
    
    # Sumar todas las m√©tricas
    for metrics in all_metrics:
        for key, value in metrics.items():
            if isinstance(value, (int, float)) and not np.isnan(value):
                if key not in metric_sums:
                    metric_sums[key] = 0
                    metric_counts[key] = 0
                metric_sums[key] += value
                metric_counts[key] += 1
    
    # Calcular promedios
    average_metrics = {}
    for key in metric_sums:
        if metric_counts[key] > 0:
            average_metrics[key] = metric_sums[key] / metric_counts[key]
    
    return average_metrics

def run_cumulative_metrics_evaluation(
    questions_and_answers: List[Dict],
    num_questions: int,
    model_name: str,
    generative_model_name: str,
    top_k: int = 10,
    use_llm_reranker: bool = True
) -> Dict[str, Any]:
    """
    Ejecuta evaluaci√≥n de m√©tricas acumulativas para m√∫ltiples preguntas.
    
    Args:
        questions_and_answers: Lista de preguntas y respuestas
        num_questions: N√∫mero de preguntas a evaluar
        model_name: Nombre del modelo de embedding
        generative_model_name: Nombre del modelo generativo
        top_k: N√∫mero de documentos top-k
        use_llm_reranker: Si usar LLM reranking
        
    Returns:
        Diccionario con resultados de evaluaci√≥n
    """
    # Filtrar preguntas con links
    filtered_questions = filter_questions_with_links(questions_and_answers)
    
    if len(filtered_questions) < num_questions:
        st.warning(f"‚ö†Ô∏è Solo hay {len(filtered_questions)} preguntas con links disponibles. Se usar√°n todas.")
        num_questions = len(filtered_questions)
    
    # Seleccionar preguntas aleatoriamente
    if len(filtered_questions) > num_questions:
        selected_indices = np.random.choice(len(filtered_questions), num_questions, replace=False)
        selected_questions = [filtered_questions[i] for i in selected_indices]
    else:
        selected_questions = filtered_questions
    
    # Inicializar clientes
    weaviate_wrapper, embedding_client, openai_client, gemini_client, local_tinyllama_client, local_mistral_client, openrouter_client, _ = initialize_clients(
        model_name, generative_model_name
    )
    
    # Listas para almacenar m√©tricas
    before_reranking_metrics = []
    after_reranking_metrics = []
    all_questions_data = []
    
    # Barra de progreso
    progress_bar = st.progress(0)
    status_text = st.empty()
    
    # Evaluar cada pregunta
    for i, qa in enumerate(selected_questions):
        question = qa['question']
        ground_truth_answer = qa.get('accepted_answer', '')
        ms_links = qa.get('ms_links', [])
        
        status_text.text(f"Evaluando pregunta {i+1}/{num_questions}...")
        
        try:
            # Ejecutar pipeline con m√©tricas
            result = answer_question_with_retrieval_metrics(
                question=question,
                weaviate_wrapper=weaviate_wrapper,
                embedding_client=embedding_client,
                openai_client=openai_client,
                gemini_client=gemini_client,
                local_tinyllama_client=local_tinyllama_client,
                local_mistral_client=local_mistral_client,
                openrouter_client=openrouter_client,
                top_k=top_k,
                use_llm_reranker=use_llm_reranker,
                generate_answer=False,  # Sin RAG como se solicit√≥
                calculate_metrics=True,
                ground_truth_answer=ground_truth_answer,
                ms_links=ms_links,
                generative_model_name=generative_model_name
            )
            
            if len(result) >= 3:
                docs, debug_info, retrieval_metrics = result
                
                # Extraer m√©tricas antes y despu√©s del reranking
                before_metrics = retrieval_metrics.get('before_reranking', {})
                after_metrics = retrieval_metrics.get('after_reranking', {})
                
                before_reranking_metrics.append(before_metrics)
                after_reranking_metrics.append(after_metrics)
                
                # Almacenar datos de la pregunta para referencia
                all_questions_data.append({
                    'question_num': i + 1,
                    'question': question[:100] + '...' if len(question) > 100 else question,
                    'ground_truth_links': len(ms_links),
                    'docs_retrieved': len(docs),
                    'before_precision_5': before_metrics.get('Precision@5', 0),
                    'after_precision_5': after_metrics.get('Precision@5', 0),
                    'before_recall_5': before_metrics.get('Recall@5', 0),
                    'after_recall_5': after_metrics.get('Recall@5', 0),
                    'before_f1_5': before_metrics.get('F1@5', 0),
                    'after_f1_5': after_metrics.get('F1@5', 0)
                })
                
        except Exception as e:
            st.error(f"Error evaluando pregunta {i+1}: {e}")
            continue
        
        progress_bar.progress((i + 1) / num_questions)
    
    # Calcular m√©tricas promedio
    avg_before_metrics = calculate_average_metrics(before_reranking_metrics)
    avg_after_metrics = calculate_average_metrics(after_reranking_metrics)
    
    # Limpiar interfaz
    progress_bar.empty()
    status_text.empty()
    
    return {
        'num_questions_evaluated': len(before_reranking_metrics),
        'avg_before_metrics': avg_before_metrics,
        'avg_after_metrics': avg_after_metrics,
        'all_questions_data': all_questions_data,
        'individual_before_metrics': before_reranking_metrics,
        'individual_after_metrics': after_reranking_metrics
    }

def display_cumulative_metrics(results: Dict[str, Any], model_name: str, use_llm_reranker: bool):
    """
    Muestra los resultados de m√©tricas acumulativas en la interfaz.
    
    Args:
        results: Resultados de la evaluaci√≥n
        model_name: Nombre del modelo usado
        use_llm_reranker: Si se us√≥ LLM reranking
    """
    num_questions = results['num_questions_evaluated']
    avg_before = results['avg_before_metrics']
    avg_after = results['avg_after_metrics']
    
    st.success(f"‚úÖ Evaluaci√≥n completada para {num_questions} preguntas")
    
    # M√©tricas principales en columnas
    st.subheader("üìä M√©tricas Promedio")
    
    # M√©tricas principales
    main_metrics = ['Precision@5', 'Recall@5', 'F1@5', 'MRR@5', 'nDCG@5']
    
    col1, col2 = st.columns(2)
    
    with col1:
        st.markdown("**üîç Antes del Reranking**")
        for metric in main_metrics:
            if metric in avg_before:
                st.metric(
                    label=metric,
                    value=f"{avg_before[metric]:.3f}",
                    help=f"Promedio de {metric} antes del reranking LLM"
                )
    
    with col2:
        if use_llm_reranker:
            st.markdown("**ü§ñ Despu√©s del Reranking LLM**")
            for metric in main_metrics:
                if metric in avg_after:
                    # Calcular delta
                    delta = avg_after[metric] - avg_before.get(metric, 0)
                    st.metric(
                        label=metric,
                        value=f"{avg_after[metric]:.3f}",
                        delta=f"{delta:+.3f}",
                        help=f"Promedio de {metric} despu√©s del reranking LLM"
                    )
        else:
            st.info("‚ÑπÔ∏è Reranking LLM deshabilitado")
    
    # Gr√°fico de comparaci√≥n
    if use_llm_reranker and avg_before and avg_after:
        st.subheader("üìà Comparaci√≥n Visual")
        
        # Preparar datos para el gr√°fico
        metrics_to_plot = ['Precision@5', 'Recall@5', 'F1@5', 'MRR@5', 'nDCG@5']
        before_values = [avg_before.get(m, 0) for m in metrics_to_plot]
        after_values = [avg_after.get(m, 0) for m in metrics_to_plot]
        
        # Crear gr√°fico de barras comparativo
        fig = go.Figure()
        fig.add_trace(go.Bar(
            name='Antes del Reranking',
            x=metrics_to_plot,
            y=before_values,
            marker_color='lightblue'
        ))
        fig.add_trace(go.Bar(
            name='Despu√©s del Reranking',
            x=metrics_to_plot,
            y=after_values,
            marker_color='darkblue'
        ))
        
        fig.update_layout(
            title=f'Comparaci√≥n de M√©tricas Promedio ({num_questions} preguntas)',
            xaxis_title='M√©tricas',
            yaxis_title='Valor Promedio',
            barmode='group',
            height=400
        )
        
        st.plotly_chart(fig, use_container_width=True)
    
    # M√©tricas adicionales en expander
    with st.expander("üìã M√©tricas Detalladas"):
        col1, col2 = st.columns(2)
        
        with col1:
            st.markdown("**Antes del Reranking**")
            before_df = pd.DataFrame([avg_before]).T
            before_df.columns = ['Valor Promedio']
            before_df.index.name = 'M√©trica'
            st.dataframe(before_df.style.format({'Valor Promedio': '{:.4f}'}))
        
        with col2:
            if use_llm_reranker and avg_after:
                st.markdown("**Despu√©s del Reranking**")
                after_df = pd.DataFrame([avg_after]).T
                after_df.columns = ['Valor Promedio']
                after_df.index.name = 'M√©trica'
                st.dataframe(after_df.style.format({'Valor Promedio': '{:.4f}'}))
    
    # Tabla de evoluci√≥n por pregunta
    with st.expander("üìä Evoluci√≥n por Pregunta"):
        questions_df = pd.DataFrame(results['all_questions_data'])
        if not questions_df.empty:
            # Renombrar columnas para mejor legibilidad
            column_names = {
                'question_num': 'Pregunta #',
                'ground_truth_links': 'Links GT',
                'docs_retrieved': 'Docs Recuperados',
                'before_precision_5': 'Precision@5 (Antes)',
                'after_precision_5': 'Precision@5 (Despu√©s)',
                'before_f1_5': 'F1@5 (Antes)',
                'after_f1_5': 'F1@5 (Despu√©s)',
                'before_recall_5': 'Recall@5 (Antes)',
                'after_recall_5': 'Recall@5 (Despu√©s)'
            }
            
            # Mostrar tabla con m√©tricas por pregunta
            if use_llm_reranker:
                display_columns = ['question_num', 'ground_truth_links', 'docs_retrieved', 
                                 'before_precision_5', 'after_precision_5', 
                                 'before_f1_5', 'after_f1_5']
            else:
                display_columns = ['question_num', 'ground_truth_links', 'docs_retrieved', 
                                 'before_precision_5', 'before_f1_5']
            
            questions_df_display = questions_df[display_columns].rename(columns=column_names)
            
            st.dataframe(questions_df_display.style.format({
                'Precision@5 (Antes)': '{:.3f}',
                'Precision@5 (Despu√©s)': '{:.3f}',
                'F1@5 (Antes)': '{:.3f}',
                'F1@5 (Despu√©s)': '{:.3f}'
            }), use_container_width=True)

def load_questions_from_json(file_path: str) -> List[Dict]:
    """
    Carga preguntas desde archivo JSON.
    
    Args:
        file_path: Ruta al archivo JSON
        
    Returns:
        Lista de preguntas y respuestas
    """
    try:
        with open(file_path, 'r', encoding='utf-8') as f:
            return json.load(f)
    except FileNotFoundError:
        st.error(f"‚ùå No se encontr√≥ el archivo '{file_path}'")
        return []
    except json.JSONDecodeError as e:
        st.error(f"‚ùå Error al leer el archivo JSON: {e}")
        return []

def show_cumulative_metrics_page():
    """Muestra la p√°gina de m√©tricas acumulativas."""
    st.title("üìà M√©tricas Acumulativas")
    st.markdown("""
    Esta p√°gina eval√∫a m√∫ltiples preguntas y calcula **m√©tricas promedio** para obtener una visi√≥n general 
    del rendimiento del sistema. Solo se eval√∫an preguntas que tienen links en la respuesta aceptada.
    """)
    
    # Cargar preguntas desde JSON
    questions_and_answers = load_questions_from_json('data/val_set.json')
    
    if not questions_and_answers:
        st.error("‚ùå No se pudieron cargar las preguntas")
        st.stop()
    
    # Filtrar preguntas con links
    filtered_questions = filter_questions_with_links(questions_and_answers)
    
    st.info(f"üìä {len(filtered_questions)} preguntas disponibles con links en respuesta aceptada")
    
    # Configuraci√≥n
    st.subheader("‚öôÔ∏è Configuraci√≥n de Evaluaci√≥n")
    
    col1, col2 = st.columns(2)
    
    with col1:
        # N√∫mero de preguntas
        num_questions = st.slider(
            "N√∫mero de preguntas a evaluar",
            min_value=1,
            max_value=min(50, len(filtered_questions)),
            value=5,
            help="Cantidad de preguntas para evaluar (tomadas aleatoriamente)"
        )
        
        # Modelo de embedding
        model_name = st.selectbox(
            "Modelo de Embedding",
            options=list(EMBEDDING_MODELS.keys()),
            index=0,
            help="Modelo para generar embeddings de documentos"
        )
    
    with col2:
        # Configuraci√≥n de retrieval
        top_k = st.slider(
            "Top-K documentos",
            min_value=5,
            max_value=20,
            value=10,
            help="N√∫mero de documentos a recuperar"
        )
        
        # LLM Reranking
        use_llm_reranker = st.checkbox(
            "Usar LLM Reranking",
            value=True,
            help="Usar GPT-4 para reordenar documentos (necesario para m√©tricas antes/despu√©s)"
        )
    
    # Modelo generativo (solo para reranking)
    generative_model_name = st.selectbox(
        "Modelo Generativo (para reranking)",
        options=list(GENERATIVE_MODELS.keys()),
        index=0,
        help="Modelo usado para el reranking LLM"
    )
    
    # Bot√≥n para ejecutar evaluaci√≥n
    if st.button("üöÄ Ejecutar Evaluaci√≥n", type="primary"):
        if len(filtered_questions) < num_questions:
            st.error(f"‚ùå Solo hay {len(filtered_questions)} preguntas con links disponibles")
            st.stop()
        
        with st.spinner(f"üîç Evaluando {num_questions} preguntas..."):
            start_time = time.time()
            
            results = run_cumulative_metrics_evaluation(
                questions_and_answers=filtered_questions,
                num_questions=num_questions,
                model_name=model_name,
                generative_model_name=generative_model_name,
                top_k=top_k,
                use_llm_reranker=use_llm_reranker
            )
            
            evaluation_time = time.time() - start_time
            
            # Mostrar resultados
            display_cumulative_metrics(results, model_name, use_llm_reranker)
            
            # Estad√≠sticas adicionales
            st.markdown("---")
            st.subheader("üìä Estad√≠sticas de Evaluaci√≥n")
            
            col1, col2, col3 = st.columns(3)
            
            with col1:
                st.metric(
                    "üìù Preguntas Evaluadas",
                    f"{results['num_questions_evaluated']}",
                    help="N√∫mero total de preguntas procesadas"
                )
            
            with col2:
                if results['all_questions_data']:
                    avg_gt_links = np.mean([q['ground_truth_links'] for q in results['all_questions_data']])
                    st.metric(
                        "üîó Links GT Promedio",
                        f"{avg_gt_links:.1f}",
                        help="N√∫mero promedio de links de ground truth por pregunta"
                    )
            
            with col3:
                st.metric(
                    "‚è±Ô∏è Tiempo Total",
                    f"{evaluation_time:.1f}s",
                    help="Tiempo total de evaluaci√≥n"
                )
            
            # Opci√≥n para descargar resultados
            st.markdown("---")
            col1, col2 = st.columns(2)
            
            with col1:
                if st.button("üíæ Descargar Resultados Detallados"):
                    results_df = pd.DataFrame(results['all_questions_data'])
                    csv = results_df.to_csv(index=False)
                    st.download_button(
                        label="Descargar CSV Detallado",
                        data=csv,
                        file_name=f"cumulative_metrics_detailed_{model_name}_{num_questions}q.csv",
                        mime="text/csv"
                    )
            
            with col2:
                if st.button("üìä Descargar M√©tricas Promedio"):
                    # Crear CSV con m√©tricas promedio
                    avg_metrics_data = {
                        'Metric': [],
                        'Before_Reranking': [],
                        'After_Reranking': []
                    }
                    
                    for metric in ['Precision@5', 'Recall@5', 'F1@5', 'MRR@5', 'nDCG@5']:
                        avg_metrics_data['Metric'].append(metric)
                        avg_metrics_data['Before_Reranking'].append(results['avg_before_metrics'].get(metric, 0))
                        avg_metrics_data['After_Reranking'].append(results['avg_after_metrics'].get(metric, 0))
                    
                    avg_df = pd.DataFrame(avg_metrics_data)
                    csv = avg_df.to_csv(index=False)
                    st.download_button(
                        label="Descargar CSV Promedio",
                        data=csv,
                        file_name=f"cumulative_metrics_avg_{model_name}_{num_questions}q.csv",
                        mime="text/csv"
                    )