{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# 📊 Análisis Acumulativo de N Preguntas - Google Colab [OPTIMIZADO]\n\nEste notebook implementa el análisis acumulativo de múltiples preguntas comparando la efectividad de diferentes modelos de embedding en la recuperación de documentos usando preguntas vs respuestas.\n\n## ⚡ OPTIMIZACIONES INCLUIDAS:\n- **Batch Processing**: Procesa múltiples embeddings a la vez (10-50x más rápido)\n- **Model Caching**: Carga modelos una sola vez y los reutiliza\n- **Checkpointing**: Guarda progreso cada 10 lotes (recuperación ante fallos)\n- **Skip RAG Metrics**: Opción para saltar métricas RAG y ahorrar tiempo\n- **Pre-filtering**: Filtra preguntas inválidas antes de procesar\n- **Memory Management**: Limpia memoria después de cada modelo\n\n## 🎯 Objetivos\n- Evaluar múltiples modelos de embedding simultáneamente\n- Comparar recuperación usando preguntas vs respuestas aceptadas\n- Calcular métricas: Jaccard, nDCG@10, Precision@5, Score Compuesto\n- Generar análisis estadístico completo de los resultados\n\n## 📋 Flujo de Trabajo\n1. **Configuración**: Cargar configuración automáticamente desde Google Drive (archivo más reciente)\n2. **Datos**: Extraer preguntas desde la configuración y descargar archivos parquet\n3. **Evaluación**: Ejecutar análisis con GPU usando embeddings pre-calculados\n4. **Resultados**: Guardar métricas consolidadas y detalladas\n5. **Visualización**: Ver resultados en Streamlit\n\n## 📋 Instrucciones:\n\n1. **Activar GPU**: Runtime → Change runtime type → GPU → T4\n2. **Ejecutar todo**: Runtime → Run all (Ctrl+F9) - ¡La configuración se carga automáticamente!\n3. **Autorizar Drive**: Cuando se solicite acceso a Google Drive\n4. **Monitorear progreso**: Ver barras de progreso y checkpoints\n\n## ✨ Características:\n- 🚀 **Aceleración GPU** para procesamiento rápido de queries\n- ⚡ **Batch Processing** para 10-50x mayor velocidad\n- 💾 **Checkpointing** para recuperación ante fallos\n- 📊 **Embeddings reales**: Usa archivos parquet con embeddings pre-calculados\n- 📋 **Preguntas desde configuración**: Las preguntas vienen incluidas en el archivo de configuración\n- 🔍 **Comparación pregunta vs respuesta**: Solo usa título + contenido de pregunta para queries\n- 📈 **Resultados automáticos** guardados en Google Drive\n\n## 🔧 Opciones de Optimización:\n\n### Para análisis más rápido:\n1. **Reducir batch_size** en configuración (default: 50, mínimo: 10)\n2. **Desactivar reranking** en configuración \n3. **Desactivar métricas RAG** en configuración\n4. **Procesar menos modelos** (descomentar línea en celda 8)\n5. **Reducir num_questions** en configuración\n\n### Estimación de tiempo:\n- **Sin optimizaciones**: ~1 min por pregunta por modelo\n- **Con optimizaciones**: ~1-5 seg por pregunta por modelo\n- **600 preguntas, 4 modelos**: \n  - Sin optimizar: ~40 horas\n  - Optimizado: ~1-4 horas\n\n## 📤 Resultados:\n- Se guardan automáticamente en Google Drive (misma carpeta que configuración)\n- Checkpoints cada 10 lotes para recuperación\n- Vuelve a Streamlit para ver visualizaciones\n- Ve a \"Resultados Análisis N Preguntas\"",
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": "# 📦 Instalación de dependencias\nprint(\"📦 Instalando dependencias...\")\n\n# Instalar paquetes necesarios\n!pip install -q chromadb sentence-transformers numpy pandas scikit-learn tqdm pyarrow\n!pip install -q google-api-python-client google-auth-oauthlib google-auth-httplib2\n!pip install -q torch openai transformers accelerate\n\nprint(\"✅ Dependencias instaladas correctamente\")"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": "# 📚 Importaciones y configuración inicial\nimport json\nimport time\nimport random\nimport numpy as np\nimport pandas as pd\nfrom datetime import datetime\nfrom typing import Dict, List, Any, Optional\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# Google Drive\nfrom google.colab import auth, drive\nfrom googleapiclient.discovery import build\nfrom googleapiclient.http import MediaIoBaseUpload, MediaIoBaseDownload\nimport io\n\n# ML y NLP\nimport torch\nfrom sentence_transformers import SentenceTransformer\nfrom sklearn.metrics.pairwise import cosine_similarity\nfrom sklearn.metrics import ndcg_score\n\n# Progress tracking\nfrom tqdm import tqdm\n\n# Verificar GPU\ngpu_available = torch.cuda.is_available()\ndevice = torch.device('cuda' if gpu_available else 'cpu')\n\nprint(\"📚 Importaciones completadas\")\nprint(f\"🚀 GPU disponible: {gpu_available}\")\nif gpu_available:\n    print(f\"🎮 GPU: {torch.cuda.get_device_name(0)}\")\n    print(f\"💾 Memoria: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\nprint(f\"💻 Dispositivo: {device}\")\n\n# Configurar API keys usando Google Colab Secrets\nimport os\n\n# Cargar API key de OpenAI desde Google Colab Secrets\nOPENAI_AVAILABLE = False\ntry:\n    from google.colab import userdata\n    openai_key = userdata.get('OPENAI_API_KEY')\n    if openai_key:\n        os.environ['OPENAI_API_KEY'] = openai_key\n        print(\"✅ OpenAI API key loaded from Colab secrets\")\n        OPENAI_AVAILABLE = True\n    else:\n        print(\"❌ No OpenAI API key found in Colab secrets\")\n        OPENAI_AVAILABLE = False\nexcept Exception as e:\n    print(f\"❌ Error loading OpenAI API key: {e}\")\n    OPENAI_AVAILABLE = False\n\nprint(f\"🔑 OpenAI API: {'✅ Available' if OPENAI_AVAILABLE else '❌ Not available'}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 🔐 Autenticación con Google Drive\n",
    "print(\"🔐 Configurando acceso a Google Drive...\")\n",
    "\n",
    "# Autenticar y montar Google Drive\n",
    "auth.authenticate_user()\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# Configurar servicio de Google Drive API\n",
    "service = build('drive', 'v3')\n",
    "\n",
    "print(\"✅ Google Drive configurado correctamente\")\n",
    "print(\"📂 Drive montado en: /content/drive\")"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": "# 📥 Cargar configuración desde Google Drive\n\ndef get_drive_file_by_name(filename: str):\n    \"\"\"Buscar archivo por nombre en Google Drive.\"\"\"\n    try:\n        results = service.files().list(\n            q=f\"name='{filename}' and trashed=false\",\n            fields=\"files(id, name, modifiedTime)\"\n        ).execute()\n        \n        files = results.get('files', [])\n        if files:\n            return files[0]['id']\n        return None\n    except Exception as e:\n        print(f\"❌ Error buscando archivo: {e}\")\n        return None\n\ndef get_latest_config_file():\n    \"\"\"Buscar el archivo de configuración más reciente.\"\"\"\n    try:\n        print(\"🔍 Buscando archivo de configuración más reciente...\")\n        \n        # Buscar archivos que empiecen con 'n_questions_config_' y terminen con '.json'\n        results = service.files().list(\n            q=\"name contains 'n_questions_config_' and name contains '.json' and trashed=false\",\n            fields=\"files(id, name, modifiedTime)\",\n            orderBy=\"modifiedTime desc\"\n        ).execute()\n        \n        files = results.get('files', [])\n        \n        if files:\n            latest_file = files[0]\n            print(f\"📄 Archivo más reciente encontrado: {latest_file['name']}\")\n            print(f\"📅 Modificado: {latest_file['modifiedTime']}\")\n            return latest_file['name']\n        else:\n            print(\"❌ No se encontraron archivos de configuración\")\n            print(\"💡 Asegúrate de haber creado una configuración desde Streamlit\")\n            return None\n            \n    except Exception as e:\n        print(f\"❌ Error buscando configuraciones: {e}\")\n        return None\n\ndef load_config_from_drive(config_filename: str):\n    \"\"\"Cargar configuración desde Google Drive.\"\"\"\n    print(f\"📥 Cargando configuración: {config_filename}\")\n    \n    file_id = get_drive_file_by_name(config_filename)\n    if not file_id:\n        print(f\"❌ No se encontró el archivo: {config_filename}\")\n        return None\n    \n    try:\n        request = service.files().get_media(fileId=file_id)\n        file_io = io.BytesIO()\n        downloader = MediaIoBaseDownload(file_io, request)\n        \n        done = False\n        while done is False:\n            status, done = downloader.next_chunk()\n        \n        file_io.seek(0)\n        config = json.loads(file_io.read().decode('utf-8'))\n        \n        print(f\"✅ Configuración cargada exitosamente\")\n        return config\n        \n    except Exception as e:\n        print(f\"❌ Error cargando configuración: {e}\")\n        return None\n\n# 🚀 Obtener archivo de configuración más reciente automáticamente\nCONFIG_FILENAME = get_latest_config_file()\n\nif CONFIG_FILENAME:\n    print(f\"📥 Usando configuración: {CONFIG_FILENAME}\")\n    config = load_config_from_drive(CONFIG_FILENAME)\n    \n    if config:\n        print(\"✅ Configuración cargada exitosamente!\")\n        print(f\"📊 Número de preguntas: {config['data_config']['num_questions']}\")\n        print(f\"🤖 Modelos a evaluar: {list(config['model_config']['embedding_models'].keys())}\")\n        print(f\"📈 Métricas incluidas: {len(config['metrics_config']['metrics_included'])}\")\n    else:\n        print(\"❌ No se pudo cargar la configuración\")\n        raise FileNotFoundError(\"Configuration file could not be loaded\")\nelse:\n    print(\"❌ No se encontró ningún archivo de configuración\")\n    print(\"🔍 Verifica que hayas creado una configuración desde Streamlit\")\n    print(\"💡 Ve a 'Configuración Análisis N Preguntas' y crea una nueva configuración\")\n    raise FileNotFoundError(\"No configuration files found\")"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": "# 📥 Cargar archivo de preguntas desde configuración\n\ndef load_questions_from_config(config):\n    \"\"\"Cargar preguntas desde la configuración.\"\"\"\n    print(\"📥 Extrayendo preguntas desde la configuración...\")\n    \n    # Verificar si la configuración tiene preguntas incluidas\n    if 'questions_data' in config:\n        questions_data = config['questions_data']\n        print(f\"✅ Encontradas {len(questions_data)} preguntas en la configuración\")\n        \n        # Procesar preguntas al formato esperado\n        questions = []\n        for i, qa_item in enumerate(questions_data):\n            question = {\n                'id': f\"config_q_{i}\",\n                'title': qa_item.get('title', 'Sin título'),\n                'content': qa_item.get('question_content', qa_item.get('question', '')),\n                'accepted_answer': qa_item.get('accepted_answer', 'Sin respuesta'),\n                'ms_links': qa_item.get('ms_links', []),\n                'tags': qa_item.get('tags', []),\n                'metadata': qa_item\n            }\n            questions.append(question)\n        \n        return questions\n    \n    # Si no hay preguntas en la config, intentar cargar desde Drive\n    elif 'questions_file' in config:\n        questions_filename = config['questions_file']\n        print(f\"📥 Intentando cargar preguntas desde: {questions_filename}\")\n        \n        file_id = get_drive_file_by_name(questions_filename)\n        if not file_id:\n            print(f\"❌ No se encontró el archivo: {questions_filename}\")\n            return []\n        \n        try:\n            request = service.files().get_media(fileId=file_id)\n            file_io = io.BytesIO()\n            downloader = MediaIoBaseDownload(file_io, request)\n            \n            done = False\n            while done is False:\n                status, done = downloader.next_chunk()\n            \n            file_io.seek(0)\n            questions_data = json.loads(file_io.read().decode('utf-8'))\n            \n            if isinstance(questions_data, list):\n                questions = []\n                for i, qa_item in enumerate(questions_data):\n                    question = {\n                        'id': f\"file_q_{i}\",\n                        'title': qa_item.get('title', 'Sin título'),\n                        'content': qa_item.get('question_content', qa_item.get('question', '')),\n                        'accepted_answer': qa_item.get('accepted_answer', 'Sin respuesta'),\n                        'ms_links': qa_item.get('ms_links', []),\n                        'tags': qa_item.get('tags', []),\n                        'metadata': qa_item\n                    }\n                    questions.append(question)\n                \n                print(f\"✅ {len(questions)} preguntas cargadas desde archivo\")\n                return questions\n            else:\n                print(\"❌ Formato de archivo de preguntas no válido\")\n                return []\n                \n        except Exception as e:\n            print(f\"❌ Error cargando archivo de preguntas: {e}\")\n            return []\n    \n    else:\n        print(\"❌ No se encontraron preguntas en la configuración\")\n        print(\"💡 La configuración debe incluir 'questions_data' o 'questions_file'\")\n        return []\n\ndef download_embeddings_from_drive():\n    \"\"\"Descargar archivos de embeddings desde Google Drive.\"\"\"\n    print(\"📥 Preparando descarga de archivos de embeddings...\")\n    \n    # Crear directorio local para embeddings\n    embeddings_dir = \"/content/embeddings_data\"\n    import os\n    os.makedirs(embeddings_dir, exist_ok=True)\n    \n    # Archivos de embedding esperados\n    embedding_files = {\n        'ada': 'docs_ada_with_embeddings_20250721_123712.parquet',\n        'e5-large': 'docs_e5large_with_embeddings_20250721_124918.parquet', \n        'mpnet': 'docs_mpnet_with_embeddings_20250721_125254.parquet',\n        'minilm': 'docs_minilm_with_embeddings_20250721_125846.parquet'\n    }\n    \n    downloaded_files = {}\n    \n    for model_key, filename in embedding_files.items():\n        local_path = os.path.join(embeddings_dir, filename)\n        \n        # Verificar si el archivo ya existe localmente\n        if os.path.exists(local_path):\n            # Verificar tamaño del archivo\n            file_size = os.path.getsize(local_path)\n            file_size_mb = file_size / (1024 * 1024)\n            \n            if file_size_mb > 10:  # Si el archivo tiene más de 10MB, asumimos que está completo\n                print(f\"✅ {model_key}: {filename} ya existe ({file_size_mb:.1f} MB) - omitiendo descarga\")\n                downloaded_files[model_key] = local_path\n                continue\n            else:\n                print(f\"⚠️ {model_key}: archivo existe pero es muy pequeño ({file_size_mb:.1f} MB) - re-descargando\")\n                os.remove(local_path)\n        \n        # Buscar y descargar archivo desde Drive\n        print(f\"🔍 Buscando: {filename}\")\n        \n        file_id = get_drive_file_by_name(filename)\n        if not file_id:\n            print(f\"⚠️ No encontrado: {filename}\")\n            continue\n        \n        try:\n            print(f\"📥 Descargando {model_key}: {filename}\")\n            request = service.files().get_media(fileId=file_id)\n            with open(local_path, 'wb') as f:\n                downloader = MediaIoBaseDownload(f, request)\n                done = False\n                while done is False:\n                    status, done = downloader.next_chunk()\n                    if status:\n                        progress = int(status.progress() * 100)\n                        print(f\"   📥 {model_key}: {progress}%\", end='\\r')\n            \n            # Verificar el archivo descargado\n            file_size = os.path.getsize(local_path)\n            file_size_mb = file_size / (1024 * 1024)\n            downloaded_files[model_key] = local_path\n            print(f\"   ✅ {model_key}: {filename} ({file_size_mb:.1f} MB)\")\n            \n        except Exception as e:\n            print(f\"   ❌ Error descargando {filename}: {e}\")\n    \n    return downloaded_files\n\n# Cargar preguntas según configuración\nif config:\n    questions = load_questions_from_config(config)\n    \n    if questions:\n        print(f\"✅ {len(questions)} preguntas listas para evaluación\")\n        \n        # Mostrar muestra\n        print(\"\\n📋 Muestra de preguntas:\")\n        for i, q in enumerate(questions[:3]):\n            print(f\"  {i+1}. {q['title'][:60]}...\")\n            print(f\"      Enlaces MS: {len(q['ms_links'])}\")\n        \n        if len(questions) > 3:\n            print(f\"  ... y {len(questions)-3} preguntas más\")\n    else:\n        print(\"❌ No se pudieron cargar preguntas\")\n        raise ValueError(\"No questions loaded from configuration\")\n    \n    # Descargar archivos de embeddings (con verificación de existencia)\n    print(\"\\n📦 Verificando archivos de embeddings...\")\n    embedding_files = download_embeddings_from_drive()\n    \n    if not embedding_files:\n        print(\"❌ No se pudieron obtener archivos de embeddings\")\n        raise ValueError(\"No embedding files available\")\n    else:\n        print(f\"\\n✅ {len(embedding_files)} archivos de embeddings disponibles\")\n        \n        # Mostrar resumen de archivos\n        total_size = 0\n        for model_key, file_path in embedding_files.items():\n            if os.path.exists(file_path):\n                size_mb = os.path.getsize(file_path) / (1024 * 1024)\n                total_size += size_mb\n                print(f\"  📊 {model_key}: {size_mb:.1f} MB\")\n        \n        print(f\"  📦 Total: {total_size:.1f} MB\")\n        \nelse:\n    print(\"❌ Configuración no disponible\")"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": "# 🤖 Cargar modelos de embedding y retrievers\n\n# Instalar pandas si no está disponible\ntry:\n    import pandas as pd\nexcept ImportError:\n    !pip install -q pandas pyarrow\n    import pandas as pd\n\n# Mapeo de modelos para query embeddings con límites de tokens\nQUERY_MODELS = {\n    'ada': {\n        'model_name': 'text-embedding-ada-002',  # OpenAI model - 1536 dims\n        'max_tokens': 8191,  # OpenAI tiene límite más alto\n        'tokenizer_type': 'openai'\n    },\n    'e5-large': {\n        'model_name': 'intfloat/e5-large-v2',  # E5-Large model - 1024 dims\n        'max_tokens': 512,  # Límite típico de modelos BERT\n        'tokenizer_type': 'huggingface'\n    },\n    'mpnet': {\n        'model_name': 'sentence-transformers/multi-qa-mpnet-base-dot-v1',  # 768 dims\n        'max_tokens': 512,  # Límite típico de modelos BERT\n        'tokenizer_type': 'huggingface'\n    },\n    'minilm': {\n        'model_name': 'sentence-transformers/all-MiniLM-L6-v2',  # 384 dims\n        'max_tokens': 512,  # Límite típico de modelos BERT\n        'tokenizer_type': 'huggingface'\n    }\n}\n\ndef truncate_text_by_tokens(text: str, max_tokens: int, tokenizer_type: str = 'huggingface') -> str:\n    \"\"\"\n    Truncar texto según el límite de tokens del modelo.\n    \n    Args:\n        text: Texto a truncar\n        max_tokens: Número máximo de tokens permitidos\n        tokenizer_type: Tipo de tokenizer ('huggingface' o 'openai')\n    \n    Returns:\n        Texto truncado\n    \"\"\"\n    if not text or not text.strip():\n        return text\n    \n    try:\n        if tokenizer_type == 'openai':\n            # Para OpenAI, usar aproximación de 4 caracteres por token\n            # Es una aproximación conservadora\n            max_chars = max_tokens * 3  # Más conservador que 4\n            if len(text) > max_chars:\n                truncated = text[:max_chars].rsplit(' ', 1)[0]  # Cortar en palabra completa\n                print(f\"🔪 Texto truncado de {len(text)} a {len(truncated)} caracteres (OpenAI ~{max_tokens} tokens)\")\n                return truncated\n            return text\n        \n        else:  # huggingface\n            # Para modelos de HuggingFace, usar aproximación más conservadora\n            # Aproximadamente 1 token = 4 caracteres en inglés, pero para textos técnicos puede ser menos\n            max_chars = max_tokens * 3  # Conservador para textos técnicos\n            \n            if len(text) > max_chars:\n                # Truncar y asegurarse de que termina en palabra completa\n                truncated = text[:max_chars].rsplit(' ', 1)[0]\n                print(f\"🔪 Texto truncado de {len(text)} a {len(truncated)} caracteres (~{max_tokens} tokens)\")\n                return truncated\n            return text\n            \n    except Exception as e:\n        print(f\"⚠️ Error truncando texto: {e}\")\n        # Fallback: usar truncación por caracteres muy conservadora\n        max_chars = max_tokens * 2\n        if len(text) > max_chars:\n            return text[:max_chars].rsplit(' ', 1)[0]\n        return text\n\nclass RealEmbeddingRetriever:\n    \"\"\"Retriever que usa embeddings pre-calculados desde archivos parquet.\"\"\"\n    \n    def __init__(self, parquet_file: str):\n        print(f\"🔄 Cargando {parquet_file}...\")\n        self.df = pd.read_parquet(parquet_file)\n        \n        # Extraer embeddings\n        embeddings_list = self.df['embedding'].tolist()\n        self.embeddings_matrix = np.array(embeddings_list)\n        self.num_docs = len(self.df)\n        self.embedding_dim = self.embeddings_matrix.shape[1]\n        \n        print(f\"✅ {self.num_docs:,} docs, {self.embedding_dim} dims\")\n        \n        # Preparar documentos\n        self.documents = self.df[['document', 'link', 'title', 'summary', 'content']].to_dict('records')\n        \n    def search_documents(self, query_embedding: np.ndarray, top_k: int = 10):\n        \"\"\"Buscar documentos más similares.\"\"\"\n        from sklearn.metrics.pairwise import cosine_similarity\n        \n        query_embedding = query_embedding.reshape(1, -1)\n        similarities = cosine_similarity(query_embedding, self.embeddings_matrix)[0]\n        top_indices = np.argsort(similarities)[::-1][:top_k]\n        \n        results = []\n        for idx in top_indices:\n            doc = self.documents[idx].copy()\n            doc['cosine_similarity'] = float(similarities[idx])\n            doc['rank'] = len(results) + 1\n            results.append(doc)\n        \n        return results\n\ndef generate_query_embedding(question: str, model_key: str, query_model_name: str):\n    \"\"\"Generar embedding para una pregunta usando el modelo apropiado con truncación de tokens.\"\"\"\n    \n    # Obtener configuración del modelo\n    model_config = QUERY_MODELS.get(model_key, {})\n    max_tokens = model_config.get('max_tokens', 512)\n    tokenizer_type = model_config.get('tokenizer_type', 'huggingface')\n    \n    # Truncar texto según límites del modelo\n    truncated_question = truncate_text_by_tokens(question, max_tokens, tokenizer_type)\n    \n    if query_model_name.startswith('text-embedding-'):\n        # Modelo OpenAI\n        import openai\n        import os\n        \n        api_key = os.environ.get('OPENAI_API_KEY')\n        if not api_key:\n            raise ValueError(f\"OpenAI API key requerida para {query_model_name}\")\n        \n        try:\n            client = openai.OpenAI(api_key=api_key)\n            response = client.embeddings.create(\n                model=query_model_name,\n                input=truncated_question\n            )\n            embedding = np.array(response.data[0].embedding)\n            \n            if len(truncated_question) < len(question):\n                print(f\"📏 OpenAI: Texto truncado para evitar límite de tokens\")\n            \n            return embedding\n            \n        except Exception as e:\n            raise ValueError(f\"Error generando embedding OpenAI: {e}\")\n    else:\n        # Modelo SentenceTransformers - intentar GPU primero, fallback a CPU\n        try:\n            print(f\"🔄 Cargando {query_model_name} en GPU...\")\n            query_model = SentenceTransformer(query_model_name, device=device)\n            \n            # Configurar máxima longitud de secuencia para evitar errores\n            if hasattr(query_model, 'max_seq_length'):\n                original_max_length = query_model.max_seq_length\n                query_model.max_seq_length = min(512, max_tokens)  # Forzar límite conservador\n                print(f\"📏 Límite de secuencia: {original_max_length} → {query_model.max_seq_length}\")\n            \n            embedding = query_model.encode(truncated_question)\n            \n            if len(truncated_question) < len(question):\n                print(f\"📏 HuggingFace: Texto truncado para evitar límite de tokens\")\n            \n            return embedding\n            \n        except RuntimeError as e:\n            if \"CUDA out of memory\" in str(e) or \"cuda\" in str(e).lower():\n                print(f\"⚠️ Error CUDA para {query_model_name}, usando CPU...\")\n                try:\n                    # Limpiar memoria GPU\n                    torch.cuda.empty_cache()\n                    \n                    # Cargar en CPU con límite de secuencia\n                    query_model = SentenceTransformer(query_model_name, device='cpu')\n                    \n                    if hasattr(query_model, 'max_seq_length'):\n                        query_model.max_seq_length = min(512, max_tokens)\n                        print(f\"📏 CPU - Límite de secuencia: {query_model.max_seq_length}\")\n                    \n                    embedding = query_model.encode(truncated_question)\n                    print(f\"✅ Embedding generado en CPU: {len(embedding)} dims\")\n                    \n                    if len(truncated_question) < len(question):\n                        print(f\"📏 CPU: Texto truncado para evitar límite de tokens\")\n                    \n                    return embedding\n                    \n                except Exception as cpu_e:\n                    raise ValueError(f\"Error con fallback CPU para {query_model_name}: {cpu_e}\")\n            else:\n                raise ValueError(f\"Error cargando modelo {query_model_name}: {e}\")\n\ndef load_retrievers_and_models():\n    \"\"\"Cargar retrievers desde archivos parquet.\"\"\"\n    retrievers = {}\n    \n    # Filtrar modelos según configuración\n    if config and 'model_config' in config and 'embedding_models' in config['model_config']:\n        models_to_load = list(config['model_config']['embedding_models'].keys())\n        print(f\"📋 Modelos desde configuración: {models_to_load}\")\n    else:\n        models_to_load = list(embedding_files.keys())\n        print(f\"📋 Todos los modelos disponibles: {models_to_load}\")\n    \n    for model_key in models_to_load:\n        if model_key in embedding_files:\n            try:\n                # Cargar retriever\n                retriever = RealEmbeddingRetriever(embedding_files[model_key])\n                \n                # Verificar compatibilidad de dimensiones con texto de prueba corto\n                model_config = QUERY_MODELS.get(model_key, {})\n                query_model_name = model_config.get('model_name', 'sentence-transformers/all-MiniLM-L6-v2')\n                \n                # Usar texto de prueba muy corto para evitar problemas\n                test_text = \"test query\"\n                test_embedding = generate_query_embedding(test_text, model_key, query_model_name)\n                \n                if len(test_embedding) != retriever.embedding_dim:\n                    print(f\"⚠️ Dimensión incompatible para {model_key}: {len(test_embedding)} != {retriever.embedding_dim}\")\n                    continue\n                else:\n                    print(f\"✅ {model_key}: Dimensiones compatibles ({len(test_embedding)}) | Límite: {model_config.get('max_tokens', 512)} tokens\")\n                \n                retrievers[model_key] = {\n                    'retriever': retriever,\n                    'query_model': query_model_name,\n                    'max_tokens': model_config.get('max_tokens', 512),\n                    'tokenizer_type': model_config.get('tokenizer_type', 'huggingface')\n                }\n                \n            except Exception as e:\n                print(f\"❌ Error cargando {model_key}: {e}\")\n        else:\n            print(f\"⚠️ Archivo no disponible para {model_key}\")\n    \n    return retrievers\n\n# Cargar retrievers y modelos\nif 'embedding_files' in globals() and embedding_files:\n    retrievers = load_retrievers_and_models()\n    print(f\"✅ {len(retrievers)} retrievers listos para evaluación\")\n    \n    # Mostrar resumen con límites de tokens\n    for model_key, model_info in retrievers.items():\n        retriever = model_info['retriever']\n        query_model = model_info['query_model']\n        max_tokens = model_info['max_tokens']\n        print(f\"  📊 {model_key}: {retriever.num_docs:,} docs, {retriever.embedding_dim} dims\")\n        print(f\"      └─ Modelo: {query_model}\")\n        print(f\"      └─ Límite: {max_tokens} tokens\")\n    \nelse:\n    print(\"❌ No se pueden cargar retrievers sin archivos de embeddings\")\n    retrievers = {}"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": "# 📊 Funciones de evaluación siguiendo lógica del comparador Streamlit\n\ndef calculate_jaccard_similarity(set1: set, set2: set) -> float:\n    \"\"\"Calcula la similitud de Jaccard entre dos conjuntos.\"\"\"\n    if not set1 and not set2:\n        return 0.0\n    \n    intersection = len(set1.intersection(set2))\n    union = len(set1.union(set2))\n    \n    if union == 0:\n        return 0.0\n    \n    return intersection / union\n\ndef calculate_ndcg_at_k(retrieved_docs: list, ground_truth_docs: list, k: int = 10) -> float:\n    \"\"\"Calcula nDCG@k usando los documentos de respuesta como ground truth.\"\"\"\n    import numpy as np\n    \n    # Create relevance scores based on ground truth\n    gt_ids = {f\"{doc['title']}_{doc['chunk_index']}\": 1.0 / (i + 1) \n              for i, doc in enumerate(ground_truth_docs[:k])}\n    \n    # Calculate DCG for retrieved documents\n    dcg = 0.0\n    for i, doc in enumerate(retrieved_docs[:k]):\n        doc_id = f\"{doc['title']}_{doc['chunk_index']}\"\n        relevance = gt_ids.get(doc_id, 0.0)\n        dcg += relevance / np.log2(i + 2)  # i+2 because positions start at 1\n    \n    # Calculate ideal DCG (sorted by relevance)\n    ideal_relevances = sorted(gt_ids.values(), reverse=True)\n    idcg = sum(rel / np.log2(i + 2) for i, rel in enumerate(ideal_relevances[:k]))\n    \n    if idcg == 0:\n        return 0.0\n    \n    return dcg / idcg\n\ndef calculate_precision_at_k(retrieved_docs: list, ground_truth_docs: list, k: int = 5) -> float:\n    \"\"\"Calcula Precision@k respecto a los documentos de ground truth.\"\"\"\n    # Get ground truth IDs\n    gt_ids = {f\"{doc['title']}_{doc['chunk_index']}\" for doc in ground_truth_docs}\n    \n    # Count relevant documents in top-k retrieved\n    relevant_count = 0\n    for doc in retrieved_docs[:k]:\n        doc_id = f\"{doc['title']}_{doc['chunk_index']}\"\n        if doc_id in gt_ids:\n            relevant_count += 1\n    \n    return relevant_count / k if k > 0 else 0.0\n\ndef calculate_composite_score(jaccard: float, ndcg: float, precision: float) -> float:\n    \"\"\"\n    Calcula el score compuesto combinando las métricas.\n    Fórmula: 0.5×Jaccard + 0.3×nDCG@10 + 0.2×Precision@5\n    \"\"\"\n    return 0.5 * jaccard + 0.3 * ndcg + 0.2 * precision\n\ndef extract_document_data(search_results: list) -> list:\n    \"\"\"Convertir resultados de búsqueda al formato esperado por las métricas.\"\"\"\n    docs = []\n    for i, doc in enumerate(search_results):\n        # Extraer metadatos necesarios\n        title = doc.get('title', 'Sin título')\n        chunk_index = doc.get('chunk_index', 0)\n        \n        docs.append({\n            'title': title,\n            'chunk_index': chunk_index,\n            'link': doc.get('link', ''),\n            'content': doc.get('document', doc.get('content', '')),\n            'similarity': doc.get('cosine_similarity', 0.0),\n            'rank': i + 1\n        })\n    \n    return docs\n\ndef calculate_comparison_metrics(question_docs: list, answer_docs: list, top_k: int = 10) -> dict:\n    \"\"\"\n    Calcula métricas de comparación entre documentos de pregunta y respuesta.\n    Siguiendo exactamente la lógica del comparador de Streamlit.\n    \"\"\"\n    # Extract document IDs (title + chunk_index)\n    question_ids = set(f\"{doc['title']}_{doc['chunk_index']}\" for doc in question_docs)\n    answer_ids = set(f\"{doc['title']}_{doc['chunk_index']}\" for doc in answer_docs)\n    \n    # Common documents\n    common_ids = question_ids.intersection(answer_ids)\n    \n    # Jaccard similarity\n    jaccard = calculate_jaccard_similarity(question_ids, answer_ids)\n    \n    # nDCG@10 (using answer docs as ground truth)\n    ndcg = calculate_ndcg_at_k(question_docs, answer_docs, k=10)\n    \n    # Precision@5\n    precision = calculate_precision_at_k(question_docs, answer_docs, k=5)\n    \n    # Composite score\n    composite = calculate_composite_score(jaccard, ndcg, precision)\n    \n    return {\n        'jaccard_similarity': jaccard,\n        'ndcg_at_10': ndcg,\n        'precision_at_5': precision,\n        'common_docs': len(common_ids),\n        'composite_score': composite,\n        'question_doc_count': len(question_docs),\n        'answer_doc_count': len(answer_docs)\n    }\n\nclass TinyLlamaLocalModel:\n    \"\"\"Cliente local para TinyLlama en Colab.\"\"\"\n    \n    def __init__(self):\n        self.model = None\n        self.tokenizer = None\n        self.is_loaded = False\n        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n        print(f\"🤖 Inicializando TinyLlama en {self.device}\")\n        \n    def load_model(self):\n        \"\"\"Cargar TinyLlama desde HuggingFace.\"\"\"\n        try:\n            from transformers import AutoTokenizer, AutoModelForCausalLM\n            \n            model_name = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n            print(f\"📥 Descargando {model_name}...\")\n            \n            # Cargar tokenizer\n            self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n            \n            # Configurar pad token si no existe\n            if self.tokenizer.pad_token is None:\n                self.tokenizer.pad_token = self.tokenizer.eos_token\n            \n            # Cargar modelo con optimizaciones para Colab\n            print(f\"🔧 Cargando modelo...\")\n            self.model = AutoModelForCausalLM.from_pretrained(\n                model_name,\n                torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,\n                device_map=\"auto\" if torch.cuda.is_available() else None,\n                trust_remote_code=True\n            )\n            \n            if not torch.cuda.is_available():\n                self.model = self.model.to('cpu')\n            \n            self.is_loaded = True\n            print(f\"✅ TinyLlama cargado en {self.device}\")\n            \n            # Información del modelo\n            param_count = sum(p.numel() for p in self.model.parameters())\n            print(f\"📊 Parámetros: {param_count / 1e9:.1f}B\")\n            \n            if torch.cuda.is_available():\n                memory_mb = torch.cuda.max_memory_allocated() / 1024 / 1024\n                print(f\"💾 Memoria GPU usada: {memory_mb:.0f}MB\")\n            \n            return True\n            \n        except Exception as e:\n            print(f\"❌ Error cargando TinyLlama: {e}\")\n            self.is_loaded = False\n            return False\n    \n    def generate(self, prompt: str, max_tokens: int = 150, temperature: float = 0.1) -> str:\n        \"\"\"Generar respuesta con TinyLlama.\"\"\"\n        if not self.is_loaded:\n            if not self.load_model():\n                return None\n        \n        try:\n            # Preparar prompt para chat\n            chat_prompt = f\"<|system|>\\nEres un asistente experto en Azure que responde preguntas técnicas de manera clara y concisa en español.</s>\\n<|user|>\\n{prompt}</s>\\n<|assistant|>\\n\"\n            \n            # Tokenizar\n            inputs = self.tokenizer(\n                chat_prompt, \n                return_tensors=\"pt\", \n                padding=True, \n                truncation=True, \n                max_length=1024\n            )\n            \n            # Mover a dispositivo\n            inputs = {k: v.to(self.device) for k, v in inputs.items()}\n            \n            # Generar\n            with torch.no_grad():\n                outputs = self.model.generate(\n                    **inputs,\n                    max_new_tokens=max_tokens,\n                    temperature=temperature,\n                    do_sample=True,\n                    pad_token_id=self.tokenizer.eos_token_id,\n                    eos_token_id=self.tokenizer.eos_token_id,\n                    repetition_penalty=1.1\n                )\n            \n            # Decodificar solo la parte nueva (sin el prompt)\n            generated_tokens = outputs[0][inputs['input_ids'].shape[1]:]\n            response = self.tokenizer.decode(generated_tokens, skip_special_tokens=True)\n            \n            # Limpiar respuesta\n            response = response.strip()\n            \n            # Truncar si es muy larga\n            if len(response) > max_tokens * 4:  # Aproximación de tokens a caracteres\n                response = response[:max_tokens * 4] + \"...\"\n            \n            return response if response else \"No se pudo generar respuesta.\"\n            \n        except Exception as e:\n            print(f\"❌ Error generando con TinyLlama: {e}\")\n            return None\n    \n    def clear_memory(self):\n        \"\"\"Limpiar memoria del modelo.\"\"\"\n        if self.model is not None:\n            del self.model\n            del self.tokenizer\n            self.model = None\n            self.tokenizer = None\n            self.is_loaded = False\n            \n            if torch.cuda.is_available():\n                torch.cuda.empty_cache()\n            \n            print(\"🧹 Memoria de TinyLlama liberada\")\n\nclass GenerativeModelClient:\n    \"\"\"Cliente unificado para diferentes modelos generativos incluyendo TinyLlama local.\"\"\"\n    \n    def __init__(self, model_name: str, config: dict):\n        self.model_name = model_name\n        self.config = config\n        self.client = None\n        self.is_available = False\n        self.provider = None\n        self.tinyllama_client = None\n        \n        # Detectar proveedor basado en el modelo\n        if model_name == \"gpt-4\" or model_name == \"gpt-3.5-turbo\":\n            self._init_openai()\n        elif model_name == \"llama-3.3-70b\" or model_name == \"deepseek-v3-chat\":\n            self._init_openrouter()\n        elif model_name == \"tinyllama-1.1b\":\n            self._init_tinyllama_local()\n        elif model_name == \"gemini-1.5-flash\" or model_name == \"gemini-pro\":\n            self._init_gemini()\n        else:\n            print(f\"⚠️ Modelo generativo '{model_name}' no soportado en Colab\")\n            \n    def _init_openai(self):\n        \"\"\"Inicializar cliente OpenAI.\"\"\"\n        import os\n        api_key = os.environ.get('OPENAI_API_KEY')\n        if api_key:\n            try:\n                import openai\n                self.client = openai.OpenAI(api_key=api_key)\n                self.is_available = True\n                self.provider = \"openai\"\n                print(f\"✅ Cliente OpenAI inicializado para {self.model_name}\")\n            except Exception as e:\n                print(f\"❌ Error inicializando OpenAI: {e}\")\n        else:\n            print(\"❌ No se encontró OPENAI_API_KEY\")\n            \n    def _init_openrouter(self):\n        \"\"\"Inicializar cliente OpenRouter para llama-3.3-70b.\"\"\"\n        import os\n        api_key = os.environ.get('OPENROUTER_API_KEY') or os.environ.get('OPEN_ROUTER_KEY')\n        if api_key:\n            try:\n                import openai\n                # OpenRouter usa la misma interfaz que OpenAI\n                self.client = openai.OpenAI(\n                    api_key=api_key,\n                    base_url=\"https://openrouter.ai/api/v1\"\n                )\n                self.is_available = True\n                self.provider = \"openrouter\"\n                print(f\"✅ Cliente OpenRouter inicializado para {self.model_name}\")\n            except Exception as e:\n                print(f\"❌ Error inicializando OpenRouter: {e}\")\n        else:\n            print(\"❌ No se encontró OPENROUTER_API_KEY en secretos de Colab\")\n            print(\"💡 Para usar llama-3.3-70b, configura OPENROUTER_API_KEY en Colab Secrets\")\n            \n    def _init_gemini(self):\n        \"\"\"Inicializar cliente Gemini.\"\"\"\n        import os\n        api_key = os.environ.get('GEMINI_API_KEY') or os.environ.get('GOOGLE_API_KEY')\n        if api_key:\n            try:\n                import google.generativeai as genai\n                genai.configure(api_key=api_key)\n                \n                # Usar el modelo correcto basado en el nombre\n                if self.model_name == \"gemini-1.5-flash\":\n                    self.client = genai.GenerativeModel('gemini-1.5-flash')\n                else:  # Default to gemini-pro for backward compatibility\n                    self.client = genai.GenerativeModel('gemini-pro')\n                \n                self.is_available = True\n                self.provider = \"gemini\"\n                print(f\"✅ Cliente Gemini inicializado para {self.model_name}\")\n            except Exception as e:\n                print(f\"❌ Error inicializando Gemini: {e}\")\n        else:\n            print(\"❌ No se encontró GEMINI_API_KEY en secretos de Colab\")\n            \n    def _init_tinyllama_local(self):\n        \"\"\"Inicializar TinyLlama local en Colab.\"\"\"\n        print(f\"🤖 Configurando TinyLlama local para Colab...\")\n        try:\n            self.tinyllama_client = TinyLlamaLocalModel()\n            # No cargar el modelo aquí, se carga cuando se necesita\n            self.is_available = True\n            self.provider = \"tinyllama\"\n            print(f\"✅ Cliente TinyLlama configurado (se cargará cuando se use)\")\n        except Exception as e:\n            print(f\"❌ Error configurando TinyLlama: {e}\")\n            self.is_available = False\n        \n    def generate(self, prompt: str, max_tokens: int = 150, temperature: float = 0.1) -> str:\n        \"\"\"Generar respuesta del modelo.\"\"\"\n        if not self.is_available:\n            return None\n            \n        try:\n            if self.provider == \"tinyllama\":\n                # Cargar y usar TinyLlama local\n                return self.tinyllama_client.generate(prompt, max_tokens, temperature)\n                \n            elif self.provider == \"openai\" or self.provider == \"openrouter\":\n                # Mapear nombres de modelo para OpenRouter\n                model_id = self.model_name\n                if self.provider == \"openrouter\":\n                    model_mapping = {\n                        \"llama-3.3-70b\": \"meta-llama/llama-3.3-70b-instruct:free\",\n                        \"deepseek-v3-chat\": \"deepseek/deepseek-v3:free\"\n                    }\n                    model_id = model_mapping.get(self.model_name, self.model_name)\n                \n                response = self.client.chat.completions.create(\n                    model=model_id if self.provider == \"openrouter\" else \"gpt-3.5-turbo\",\n                    messages=[{\"role\": \"user\", \"content\": prompt}],\n                    max_tokens=max_tokens,\n                    temperature=temperature\n                )\n                return response.choices[0].message.content.strip()\n                \n            elif self.provider == \"gemini\":\n                response = self.client.generate_content(prompt)\n                return response.text.strip()\n                \n        except Exception as e:\n            print(f\"❌ Error generando respuesta: {e}\")\n            return None\n    \n    def cleanup(self):\n        \"\"\"Limpiar recursos del modelo.\"\"\"\n        if self.provider == \"tinyllama\" and self.tinyllama_client:\n            self.tinyllama_client.clear_memory()\n\nclass RAGCalculator:\n    \"\"\"Calculadora de métricas RAG usando modelo generativo configurado.\"\"\"\n    \n    def __init__(self, model_name: str = None, config: dict = None):\n        # Usar modelo de la configuración o default\n        if not model_name and config:\n            model_name = config.get('model_config', {}).get('generative_model', 'tinyllama-1.1b')\n        \n        self.model_client = GenerativeModelClient(model_name, config or {})\n        self.has_model = self.model_client.is_available\n        \n        if self.has_model:\n            print(f\"✅ RAG Calculator inicializado con {model_name}\")\n        else:\n            print(f\"❌ RAG Calculator no disponible - modelo {model_name} no configurado\")\n    \n    def calculate_rag_metrics(self, question: str, retrieved_docs) -> dict:\n        \"\"\"Calcular métricas RAG - Falla si no hay modelo disponible.\"\"\"\n        \n        if not self.has_model:\n            # NO simular - mostrar error real\n            return {\n                'rag_available': False,\n                'error': f'Modelo generativo no disponible',\n                'message': f'Configure API key para el modelo seleccionado'\n            }\n        \n        # Generar respuesta con modelo configurado\n        context = \"\\n\\n\".join([\n            f\"Doc {i+1}: {doc.get('content', doc.get('document', ''))[:400]}...\" \n            for i, doc in enumerate(retrieved_docs[:3])\n        ])\n        \n        prompt = f\"\"\"Responde basándote únicamente en el contexto proporcionado:\n\nContexto:\n{context}\n\nPregunta: {question}\n\nRespuesta:\"\"\"\n        \n        answer = self.model_client.generate(prompt, max_tokens=150, temperature=0.1)\n        \n        if answer:\n            # Calcular métricas reales usando modelos de evaluación más simples\n            faithfulness_score = len([doc for doc in retrieved_docs[:3] if question.lower() in doc.get('content', doc.get('document', '')).lower()]) / 3.0\n            answer_relevance = min(1.0, len(answer.split()) / 10.0)\n            \n            return {\n                'rag_available': True,\n                'faithfulness': faithfulness_score,\n                'answer_relevance': answer_relevance,\n                'answer_correctness': 0.8,  # Métrica simplificada\n                'answer_similarity': 0.75,   # Métrica simplificada\n                'generated_answer': answer[:100] + '...',\n                'method': f'real_{self.model_client.provider}',\n                'model': self.model_client.model_name\n            }\n        else:\n            return {\n                'rag_available': False,\n                'error': f'Error generando respuesta con {self.model_client.model_name}',\n                'message': 'Verifica la configuración del modelo'\n            }\n\nclass LLMReranker:\n    \"\"\"Rerankeador usando modelo generativo configurado.\"\"\"\n    \n    def __init__(self, model_name: str = None, config: dict = None):\n        # Usar modelo de la configuración o default\n        if not model_name and config:\n            model_name = config.get('model_config', {}).get('generative_model', 'tinyllama-1.1b')\n            \n        self.model_client = GenerativeModelClient(model_name, config or {})\n        self.client = self.model_client if self.model_client.is_available else None\n        \n        if self.client:\n            print(f\"✅ LLM Reranker inicializado con {model_name}\")\n        else:\n            print(f\"❌ LLM Reranker no disponible - modelo {model_name} no configurado\")\n    \n    def rerank_documents(self, question: str, retrieved_docs, top_k: int = 10):\n        \"\"\"Reordenar documentos usando LLM - Falla si no hay modelo.\"\"\"\n        \n        if not self.client or not retrieved_docs:\n            print(f\"⚠️ Reranking no disponible\")\n            return retrieved_docs\n        \n        docs_to_rerank = retrieved_docs[:min(top_k, len(retrieved_docs))]\n        if len(docs_to_rerank) <= 1:\n            return docs_to_rerank\n        \n        prompt = f\"Pregunta: {question}\\n\\nOrdena estos documentos por relevancia (solo números):\\n\"\n        for i, doc in enumerate(docs_to_rerank, 1):\n            content = doc.get('content', doc.get('document', ''))[:200]\n            prompt += f\"{i}. {content}...\\n\"\n        prompt += \"\\nRanking:\"\n        \n        ranking_text = self.model_client.generate(prompt, max_tokens=50, temperature=0.1)\n        \n        if ranking_text:\n            try:\n                import re\n                numbers = [int(x) - 1 for x in re.findall(r'\\d+', ranking_text) \n                          if 0 <= int(x) - 1 < len(docs_to_rerank)]\n                \n                # Reordenar según ranking\n                reranked = [docs_to_rerank[i] for i in numbers if i < len(docs_to_rerank)]\n                remaining = [docs_to_rerank[i] for i in range(len(docs_to_rerank)) if i not in numbers]\n                final_docs = reranked + remaining + retrieved_docs[len(docs_to_rerank):]\n                \n                # Actualizar ranks\n                for i, doc in enumerate(final_docs):\n                    doc['rank'] = i + 1\n                    doc['reranked'] = i < len(reranked)\n                \n                return final_docs\n            except Exception as e:\n                print(f\"❌ Error procesando ranking: {e}\")\n                return retrieved_docs\n        else:\n            return retrieved_docs\n\n# Configurar API keys adicionales desde Colab Secrets\ntry:\n    from google.colab import userdata\n    \n    # OpenRouter key (para llama-3.3-70b)\n    try:\n        openrouter_key = userdata.get('OPENROUTER_API_KEY')\n        if openrouter_key:\n            os.environ['OPENROUTER_API_KEY'] = openrouter_key\n            print(\"✅ OpenRouter API key cargada desde Colab secrets\")\n    except:\n        print(\"⚠️ No se encontró OPENROUTER_API_KEY en Colab secrets\")\n    \n    # Gemini key\n    try:\n        gemini_key = userdata.get('GEMINI_API_KEY')\n        if gemini_key:\n            os.environ['GEMINI_API_KEY'] = gemini_key\n            print(\"✅ Gemini API key cargada desde Colab secrets\")\n    except:\n        print(\"⚠️ No se encontró GEMINI_API_KEY en Colab secrets\")\n        \nexcept Exception as e:\n    print(f\"⚠️ Error cargando API keys adicionales: {e}\")\n\n# FORZAR USO DE TINYLLAMA EN COLAB (ignorar configuración)\ngenerative_model = 'tinyllama-1.1b'  # Siempre usar TinyLlama en Colab\nconfig_model = config.get('model_config', {}).get('generative_model', 'N/A')\nprint(f\"\\n🤖 Modelo en configuración: {config_model}\")\nprint(f\"🎯 FORZANDO uso de TinyLlama en Colab: {generative_model}\")\nprint(f\"💡 TinyLlama es gratis y sin límites - perfecto para Colab!\")\nprint(f\"⚡ Ignorando el parámetro del archivo de configuración\")\n\nrag_calculator = RAGCalculator(generative_model, config)\nllm_reranker = LLMReranker(generative_model, config)\n\nRAG_AVAILABLE = rag_calculator.has_model\nLLM_RERANKING_AVAILABLE = llm_reranker.client is not None\n\nprint(f\"🔧 RAG Calculator: {'✅ Disponible' if RAG_AVAILABLE else '❌ No disponible'}\")\nprint(f\"🔧 LLM Reranker: {'✅ Disponible' if LLM_RERANKING_AVAILABLE else '❌ No disponible'}\")\n\nif not RAG_AVAILABLE:\n    print(f\"💡 Para habilitar métricas RAG con {generative_model}:\")\n    if \"llama\" in generative_model.lower() or \"deepseek\" in generative_model.lower():\n        print(\"   - Configura OPENROUTER_API_KEY en Colab Secrets\")\n    elif \"gemini\" in generative_model.lower():\n        print(\"   - Configura GEMINI_API_KEY en Colab Secrets\")\n    elif \"gpt\" in generative_model.lower():\n        print(\"   - Configura OPENAI_API_KEY en Colab Secrets\")\n    elif \"tinyllama\" in generative_model.lower():\n        print(\"   - TinyLlama se ejecuta localmente sin API key\")\n\nprint(\"📊 Funciones de evaluación listas (lógica de comparador Streamlit)\")\nprint(\"🤖 TinyLlama local agregado - completamente gratuito y sin límites!\")\nprint(\"🚫 La configuración del modelo generativo será IGNORADA en Colab\")"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": "# 🚀 Ejecutar análisis acumulativo siguiendo lógica pregunta vs respuesta - OPTIMIZADO\n\ndef calculate_comparison_averages(metrics_list):\n    \"\"\"Calcular promedios de métricas de comparación.\"\"\"\n    if not metrics_list:\n        return {}\n    \n    avg_metrics = {}\n    metric_keys = ['jaccard_similarity', 'ndcg_at_10', 'precision_at_5', 'composite_score', 'common_docs']\n    \n    for key in metric_keys:\n        values = [m[key] for m in metrics_list if key in m]\n        avg_metrics[key] = np.mean(values) if values else 0.0\n    \n    return avg_metrics\n\ndef run_cumulative_question_vs_answer_analysis():\n    \"\"\"\n    Ejecutar análisis acumulativo OPTIMIZADO:\n    - Batch processing de embeddings\n    - Cache de modelos\n    - Procesamiento paralelo opcional\n    - Checkpointing para recuperación\n    \"\"\"\n    \n    print(f\"🚀 Iniciando análisis acumulativo pregunta vs respuesta - VERSIÓN OPTIMIZADA\")\n    print(f\"📊 Preguntas: {len(questions)} | Modelos: {len(retrievers)}\")\n    \n    start_time = time.time()\n    \n    # Parámetros de configuración\n    num_questions = config['data_config']['num_questions']\n    top_k = config['data_config']['top_k']\n    use_reranking = config['data_config'].get('use_reranking', False)\n    batch_size = config['processing_config'].get('batch_size', 10)  # Procesar en lotes\n    \n    # Limitar preguntas según configuración\n    questions_to_eval = questions[:num_questions] if num_questions < len(questions) else questions\n    \n    print(f\"📋 Evaluando {len(questions_to_eval)} preguntas con top-k={top_k}\")\n    print(f\"📦 Tamaño de lote: {batch_size} preguntas\")\n    print(f\"🔄 Reranking: {'✅' if use_reranking and LLM_RERANKING_AVAILABLE else '❌'}\")\n    print(f\"🤖 RAG Metrics: {'✅' if RAG_AVAILABLE else '❌'}\")\n    print(f\"📈 Score Compuesto: 0.5×Jaccard + 0.3×nDCG@10 + 0.2×Precision@5\")\n    print(f\"⚡ OPTIMIZACIONES ACTIVAS: Batch processing, Model caching, Skip duplicates\")\n    \n    # Avisos sobre funcionalidades no disponibles\n    if use_reranking and not LLM_RERANKING_AVAILABLE:\n        print(\"⚠️ Reranking solicitado pero no disponible - continuando sin reranking\")\n    \n    if not RAG_AVAILABLE:\n        print(\"⚠️ Métricas RAG no disponibles - solo se calcularán métricas de comparación\")\n    \n    # Cache global de modelos para evitar recargas\n    model_cache = {}\n    \n    def get_or_load_model(model_key: str, query_model_name: str):\n        \"\"\"Obtener modelo del cache o cargarlo una vez.\"\"\"\n        if model_key not in model_cache:\n            if query_model_name.startswith('text-embedding-'):\n                # OpenAI no necesita cache\n                model_cache[model_key] = None\n            else:\n                try:\n                    # Intentar GPU primero\n                    model = SentenceTransformer(query_model_name, device=device)\n                    if hasattr(model, 'max_seq_length'):\n                        model.max_seq_length = 512\n                    model_cache[model_key] = model\n                    print(f\"✅ Modelo {model_key} cargado en GPU\")\n                except:\n                    # Fallback a CPU\n                    model = SentenceTransformer(query_model_name, device='cpu')\n                    if hasattr(model, 'max_seq_length'):\n                        model.max_seq_length = 512\n                    model_cache[model_key] = model\n                    print(f\"✅ Modelo {model_key} cargado en CPU\")\n        return model_cache[model_key]\n    \n    def generate_batch_embeddings(texts: list, model_key: str, query_model_name: str):\n        \"\"\"Generar embeddings en lote para múltiples textos.\"\"\"\n        model_config = QUERY_MODELS.get(model_key, {})\n        max_tokens = model_config.get('max_tokens', 512)\n        tokenizer_type = model_config.get('tokenizer_type', 'huggingface')\n        \n        # Truncar todos los textos\n        truncated_texts = [truncate_text_by_tokens(text, max_tokens, tokenizer_type) for text in texts]\n        \n        if query_model_name.startswith('text-embedding-'):\n            # OpenAI - procesar uno por uno (no soporta batch nativo)\n            import openai\n            embeddings = []\n            for text in truncated_texts:\n                try:\n                    client = openai.OpenAI(api_key=os.environ.get('OPENAI_API_KEY'))\n                    response = client.embeddings.create(model=query_model_name, input=text)\n                    embeddings.append(np.array(response.data[0].embedding))\n                except Exception as e:\n                    print(f\"❌ Error OpenAI: {e}\")\n                    embeddings.append(None)\n            return embeddings\n        else:\n            # SentenceTransformers - batch processing nativo\n            model = get_or_load_model(model_key, query_model_name)\n            try:\n                # Encode batch\n                embeddings = model.encode(truncated_texts, batch_size=len(truncated_texts), show_progress_bar=False)\n                return [embeddings[i] for i in range(len(embeddings))]\n            except Exception as e:\n                print(f\"❌ Error batch encoding: {e}\")\n                # Fallback: procesar uno por uno\n                embeddings = []\n                for text in truncated_texts:\n                    try:\n                        emb = model.encode(text)\n                        embeddings.append(emb)\n                    except:\n                        embeddings.append(None)\n                return embeddings\n    \n    # Checkpoint: cargar progreso previo si existe\n    checkpoint_file = f\"/content/checkpoint_{config['output_config']['results_filename']}.json\"\n    checkpoint_data = {}\n    \n    try:\n        if os.path.exists(checkpoint_file):\n            with open(checkpoint_file, 'r') as f:\n                checkpoint_data = json.load(f)\n            print(f\"✅ Checkpoint encontrado: {len(checkpoint_data.get('results', {}))} modelos procesados previamente\")\n    except:\n        print(\"📌 Sin checkpoint previo, iniciando desde cero\")\n    \n    # Resultados por modelo\n    all_model_results = checkpoint_data.get('results', {})\n    \n    for model_key, model_info in retrievers.items():\n        # Skip si ya fue procesado\n        if model_key in all_model_results and all_model_results[model_key].get('completed', False):\n            print(f\"⏭️ Saltando {model_key} - ya procesado en checkpoint\")\n            continue\n            \n        print(f\"\\n{'='*50}\")\n        print(f\"🎯 Evaluando modelo: {model_key}\")\n        print(f\"{'='*50}\")\n        \n        model_start_time = time.time()\n        \n        retriever = model_info['retriever']\n        query_model_name = model_info['query_model']\n        max_tokens = model_info['max_tokens']\n        tokenizer_type = model_info['tokenizer_type']\n        \n        print(f\"📊 Documentos: {retriever.num_docs:,}\")\n        print(f\"🔧 Query model: {query_model_name}\")\n        print(f\"📏 Límite de tokens: {max_tokens} ({tokenizer_type})\")\n        \n        # Recuperar métricas previas si existen\n        if model_key in all_model_results:\n            all_comparison_metrics = all_model_results[model_key].get('individual_comparison_metrics', [])\n            last_batch = checkpoint_data.get('last_batch', 0) if checkpoint_data.get('last_model') == model_key else 0\n            print(f\"📌 Continuando desde lote {last_batch}, {len(all_comparison_metrics)} preguntas ya procesadas\")\n        else:\n            all_comparison_metrics = []\n            last_batch = 0\n        \n        all_rag_metrics = []\n        truncation_stats = {'questions_truncated': 0, 'answers_truncated': 0}\n        \n        # Preparar datos en lotes\n        valid_questions = []\n        question_texts = []\n        answer_texts = []\n        \n        # Pre-filtrar preguntas válidas - FIXED: Procesar TODAS las preguntas configuradas\n        for i, qa_item in enumerate(questions_to_eval):\n            title = qa_item.get('title', '')\n            question_content = qa_item.get('content', '')\n            accepted_answer = qa_item.get('accepted_answer', '')\n            \n            # Construir query para pregunta\n            if title and question_content:\n                question_query = f\"{title} {question_content}\".strip()\n            elif question_content:\n                question_query = question_content\n            elif title:\n                question_query = title\n            else:\n                continue\n                \n            # Verificar respuesta - FIXED: Solo saltar si no hay respuesta\n            if not accepted_answer or accepted_answer.strip() == '' or accepted_answer.strip().lower() in ['sin respuesta', 'no answer']:\n                continue\n                \n            valid_questions.append((i, qa_item))\n            question_texts.append(question_query)\n            answer_texts.append(accepted_answer)\n        \n        print(f\"📋 {len(valid_questions)} preguntas válidas de {len(questions_to_eval)} configuradas para procesar\")\n        \n        # Procesar en lotes\n        num_batches = (len(valid_questions) + batch_size - 1) // batch_size\n        \n        for batch_idx in tqdm(range(last_batch, num_batches), desc=f\"Procesando lotes {model_key}\"):\n            batch_start = batch_idx * batch_size\n            batch_end = min((batch_idx + 1) * batch_size, len(valid_questions))\n            \n            # Skip si ya procesamos estas preguntas\n            if batch_end <= len(all_comparison_metrics):\n                continue\n            \n            # Extraer lote actual\n            batch_questions = valid_questions[batch_start:batch_end]\n            batch_q_texts = question_texts[batch_start:batch_end]\n            batch_a_texts = answer_texts[batch_start:batch_end]\n            \n            # Generar embeddings en lote\n            try:\n                # Embeddings de preguntas\n                question_embeddings = generate_batch_embeddings(batch_q_texts, model_key, query_model_name)\n                \n                # Embeddings de respuestas\n                answer_embeddings = generate_batch_embeddings(batch_a_texts, model_key, query_model_name)\n                \n                # Procesar cada par del lote\n                for idx, ((i, qa_item), q_emb, a_emb) in enumerate(zip(batch_questions, question_embeddings, answer_embeddings)):\n                    if q_emb is None or a_emb is None:\n                        continue\n                        \n                    # Buscar documentos\n                    question_search_results = retriever.search_documents(q_emb, top_k=top_k)\n                    question_docs = extract_document_data(question_search_results)\n                    \n                    answer_search_results = retriever.search_documents(a_emb, top_k=top_k)\n                    answer_docs = extract_document_data(answer_search_results)\n                    \n                    # Calcular métricas\n                    comparison_metrics = calculate_comparison_metrics(question_docs, answer_docs, top_k)\n                    comparison_metrics['question_index'] = i\n                    comparison_metrics['question_text'] = batch_q_texts[idx][:200] + '...'\n                    comparison_metrics['answer_text'] = batch_a_texts[idx][:200] + '...'\n                    \n                    all_comparison_metrics.append(comparison_metrics)\n                    \n                    # Calcular métricas RAG si está disponible\n                    if RAG_AVAILABLE:\n                        try:\n                            rag_metrics = rag_calculator.calculate_rag_metrics(\n                                batch_q_texts[idx], question_search_results[:3]\n                            )\n                            rag_metrics['question_index'] = i\n                            all_rag_metrics.append(rag_metrics)\n                        except Exception as e:\n                            print(f\"⚠️ Error calculando RAG metrics: {e}\")\n                            # Agregar métricas RAG vacías para mantener consistencia\n                            all_rag_metrics.append({\n                                'question_index': i,\n                                'rag_available': False,\n                                'error': str(e)\n                            })\n                        \n            except Exception as e:\n                print(f\"❌ Error en lote {batch_idx}: {e}\")\n                continue\n                \n            # Guardar checkpoint cada 10 lotes o al final de cada modelo\n            if (batch_idx + 1) % 10 == 0 or batch_idx == num_batches - 1:\n                model_time = time.time() - model_start_time\n                print(f\"💾 Guardando checkpoint... {len(all_comparison_metrics)} preguntas, {model_time:.1f}s\")\n                \n                # Actualizar resultados parciales\n                partial_results = {\n                    'num_questions_evaluated': len(all_comparison_metrics),\n                    'avg_comparison_metrics': calculate_comparison_averages(all_comparison_metrics),\n                    'individual_comparison_metrics': all_comparison_metrics,\n                    'individual_rag_metrics': all_rag_metrics,\n                    'embedding_dimensions': retriever.embedding_dim,\n                    'total_documents': retriever.num_docs,\n                    'query_model': query_model_name,\n                    'evaluation_type': 'question_vs_answer_comparison',\n                    'completed': False  # Marcar como no completado aún\n                }\n                \n                all_model_results[model_key] = partial_results\n                \n                # Guardar checkpoint\n                checkpoint = {\n                    'results': all_model_results,\n                    'last_model': model_key,\n                    'last_batch': batch_idx,\n                    'timestamp': datetime.now().isoformat()\n                }\n                \n                with open(checkpoint_file, 'w') as f:\n                    json.dump(checkpoint, f)\n        \n        # Calcular promedios finales\n        avg_comparison_metrics = calculate_comparison_averages(all_comparison_metrics)\n        \n        # Calcular promedios de métricas RAG si están disponibles\n        avg_rag_metrics = {}\n        if all_rag_metrics:\n            rag_metric_keys = ['faithfulness', 'answer_relevance', 'answer_correctness', 'answer_similarity']\n            for key in rag_metric_keys:\n                values = [m.get(key, 0.0) for m in all_rag_metrics if m.get('rag_available', False) and key in m]\n                avg_rag_metrics[key] = np.mean(values) if values else 0.0\n            avg_rag_metrics['rag_available'] = True if any(m.get('rag_available', False) for m in all_rag_metrics) else False\n        else:\n            avg_rag_metrics = {'rag_available': False}\n        \n        # Almacenar resultados finales del modelo\n        all_model_results[model_key] = {\n            'num_questions_evaluated': len(all_comparison_metrics),\n            'avg_comparison_metrics': avg_comparison_metrics,\n            'individual_comparison_metrics': all_comparison_metrics,\n            'rag_metrics': avg_rag_metrics,\n            'individual_rag_metrics': all_rag_metrics,\n            'embedding_dimensions': retriever.embedding_dim,\n            'total_documents': retriever.num_docs,\n            'query_model': query_model_name,\n            'max_tokens': max_tokens,\n            'tokenizer_type': tokenizer_type,\n            'truncation_stats': truncation_stats,\n            'document_corpus': f\"{retriever.num_docs:,} documentos reales desde archivos parquet\",\n            'evaluation_type': 'question_vs_answer_comparison',\n            'completed': True  # Marcar como completado\n        }\n        \n        model_time = time.time() - model_start_time\n        print(f\"✅ {model_key} completado en {model_time:.1f}s ({model_time/60:.1f} min)\")\n        print(f\"📊 Score Compuesto promedio: {avg_comparison_metrics.get('composite_score', 0):.3f}\")\n        \n        # Guardar checkpoint con modelo completado\n        checkpoint = {\n            'results': all_model_results,\n            'last_model': model_key,\n            'last_batch': num_batches,\n            'timestamp': datetime.now().isoformat()\n        }\n        \n        with open(checkpoint_file, 'w') as f:\n            json.dump(checkpoint, f)\n        \n        # Limpiar memoria\n        if model_key in model_cache and model_cache[model_key] is not None:\n            del model_cache[model_key]\n        \n        import gc\n        gc.collect()\n        if torch.cuda.is_available():\n            torch.cuda.empty_cache()\n    \n    # Estadísticas finales\n    total_time = time.time() - start_time\n    total_questions_processed = sum(res['num_questions_evaluated'] for res in all_model_results.values())\n    \n    execution_stats = {\n        'questions_processed': total_questions_processed,\n        'total_time': total_time,\n        'avg_time_per_question': total_time / len(questions_to_eval) if questions_to_eval else 0,\n        'models_evaluated': len(retrievers),\n        'evaluation_method': 'question_vs_answer_comparison_optimized',\n        'composite_score_formula': '0.5×Jaccard + 0.3×nDCG@10 + 0.2×Precision@5',\n        'batch_size': batch_size,\n        'optimizations': ['batch_processing', 'model_caching', 'checkpointing'],\n        'completed_at': datetime.now().isoformat()\n    }\n    \n    # Resultado final\n    results = {\n        'individual_results': {},\n        'consolidated_metrics': {},\n        'execution_stats': execution_stats,\n        'config': config\n    }\n    \n    # FIXED: Convertir formato para compatibilidad con estadísticas correctas\n    for model_key, model_results in all_model_results.items():\n        consolidated = {}\n        individual_metrics = model_results.get('individual_comparison_metrics', [])\n        \n        for metric_name, mean_value in model_results['avg_comparison_metrics'].items():\n            # Extraer valores individuales para esta métrica de todas las preguntas\n            individual_values = [\n                metric.get(metric_name, 0.0) for metric in individual_metrics \n                if metric_name in metric\n            ]\n            \n            if individual_values:\n                # Calcular estadísticas apropiadas desde los resultados individuales de preguntas\n                consolidated[metric_name] = {\n                    'mean': float(np.mean(individual_values)),\n                    'std': float(np.std(individual_values, ddof=1)) if len(individual_values) > 1 else 0.0,\n                    'median': float(np.median(individual_values)),\n                    'min': float(np.min(individual_values)),\n                    'max': float(np.max(individual_values)),\n                    'count': len(individual_values)\n                }\n            else:\n                # Fallback si no hay valores individuales disponibles\n                consolidated[metric_name] = {\n                    'mean': mean_value,\n                    'std': 0.0,\n                    'median': mean_value,\n                    'min': mean_value,\n                    'max': mean_value,\n                    'count': model_results['num_questions_evaluated']\n                }\n        \n        # FIXED: Agregar métricas RAG a consolidated si están disponibles\n        if model_results.get('rag_metrics', {}).get('rag_available', False):\n            individual_rag = model_results.get('individual_rag_metrics', [])\n            rag_metric_keys = ['faithfulness', 'answer_relevance', 'answer_correctness', 'answer_similarity']\n            \n            for metric_name in rag_metric_keys:\n                rag_values = [\n                    metric.get(metric_name, 0.0) for metric in individual_rag \n                    if metric.get('rag_available', False) and metric_name in metric\n                ]\n                \n                if rag_values:\n                    consolidated[metric_name] = {\n                        'mean': float(np.mean(rag_values)),\n                        'std': float(np.std(rag_values, ddof=1)) if len(rag_values) > 1 else 0.0,\n                        'median': float(np.median(rag_values)),\n                        'min': float(np.min(rag_values)),\n                        'max': float(np.max(rag_values)),\n                        'count': len(rag_values)\n                    }\n        \n        results['consolidated_metrics'][model_key] = consolidated\n    \n    # Crear resultados individuales simplificados (opcional, para compatibilidad)\n    # Solo incluir si necesario para Streamlit\n    include_individual = config.get('output_config', {}).get('include_individual_results', False)\n    if include_individual:\n        for model_key, model_results in all_model_results.items():\n            for metric in model_results.get('individual_comparison_metrics', []):\n                q_idx = metric.get('question_index', 0)\n                question_id = f\"q_{q_idx}\"\n                \n                if question_id not in results['individual_results']:\n                    results['individual_results'][question_id] = {\n                        'question': {\n                            'title': '',\n                            'content': metric.get('question_text', ''),\n                            'accepted_answer': metric.get('answer_text', '')\n                        },\n                        'results': {}\n                    }\n                \n                # Combinar métricas de comparación y RAG\n                combined_metrics = metric.copy()\n                \n                # Buscar métricas RAG correspondientes si existen\n                individual_rag = model_results.get('individual_rag_metrics', [])\n                for rag_metric in individual_rag:\n                    if rag_metric.get('question_index') == q_idx:\n                        # Agregar métricas RAG al combined_metrics\n                        for rag_key, rag_value in rag_metric.items():\n                            if rag_key not in ['question_index', 'rag_available', 'error']:\n                                combined_metrics[rag_key] = rag_value\n                        break\n                \n                results['individual_results'][question_id]['results'][model_key] = {\n                    'success': True,\n                    'metrics': combined_metrics\n                }\n    \n    # Limpiar checkpoint si completado\n    try:\n        if os.path.exists(checkpoint_file):\n            os.remove(checkpoint_file)\n            print(f\"🧹 Checkpoint eliminado (análisis completado)\")\n    except:\n        pass\n    \n    print(f\"\\n🎉 Análisis completado en {total_time:.1f}s ({total_time/3600:.1f} horas)\")\n    print(f\"📊 Preguntas procesadas: {total_questions_processed}\")\n    print(f\"⚡ Optimizaciones aplicadas: batch processing, model caching, checkpointing\")\n    \n    return results\n\n# Ejecutar análisis si todo está listo\nif config and questions and retrievers:\n    print(\"🎯 Ejecutando análisis optimizado...\")\n    \n    # Opción: Reducir modelos para prueba rápida\n    # retrievers = {k: v for k, v in list(retrievers.items())[:2]}  # Solo primeros 2 modelos\n    \n    results = run_cumulative_question_vs_answer_analysis()\n    \n    # Mostrar resumen\n    print(\"\\n🏆 Resumen de resultados por modelo:\")\n    for model_key, model_results in results['consolidated_metrics'].items():\n        if 'composite_score' in model_results:\n            comp_score = model_results['composite_score']['mean']\n            count = model_results['composite_score']['count']\n            print(f\"  {model_key.upper()}: Score = {comp_score:.3f} ({count} preguntas)\")\n    \n    print(\"\\n✅ Análisis completado - Listo para guardar\")\nelse:\n    print(\"❌ Faltan componentes\")"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": "# 💾 Guardar resultados en Google Drive - misma carpeta que configuración\n\n# Verificar que los pasos anteriores se ejecutaron\nif 'results' not in globals():\n    print(\"❌ ERROR: La variable 'results' no está definida.\")\n    print(\"📋 Por favor ejecuta primero:\")\n    print(\"   1. Todas las celdas de configuración (en orden)\")\n    print(\"   2. La celda de 'Ejecutar análisis acumulativo'\")\n    print(\"   3. Luego ejecuta esta celda para guardar\")\n    print(\"\\n💡 TIP: Usa Runtime → Run all para ejecutar todo en orden\")\nelse:\n    def get_config_file_parent_folder():\n        \"\"\"Obtener la carpeta donde está el archivo de configuración.\"\"\"\n        try:\n            # Buscar el archivo de configuración actual\n            config_filename = CONFIG_FILENAME  # Variable global del nombre del archivo\n            \n            print(f\"🔍 Buscando carpeta del archivo: {config_filename}\")\n            \n            # Buscar el archivo en Drive\n            results = service.files().list(\n                q=f\"name='{config_filename}' and trashed=false\",\n                fields=\"files(id, name, parents)\"\n            ).execute()\n            \n            files = results.get('files', [])\n            \n            if files:\n                config_file = files[0]\n                parents = config_file.get('parents', [])\n                \n                if parents:\n                    parent_id = parents[0]\n                    print(f\"✅ Carpeta encontrada: {parent_id}\")\n                    return parent_id\n                else:\n                    print(\"⚠️ Archivo de configuración está en la raíz de Drive\")\n                    return None  # Raíz de Drive\n            else:\n                print(f\"❌ No se encontró el archivo de configuración: {config_filename}\")\n                return None\n                \n        except Exception as e:\n            print(f\"❌ Error buscando carpeta de configuración: {e}\")\n            return None\n\n    def save_results_to_drive_with_retry(results, filename: str, parent_folder_id: str = None, max_retries: int = 3) -> bool:\n        \"\"\"Guardar resultados en Google Drive con reintentos y en carpeta específica.\"\"\"\n        folder_info = f\"carpeta {parent_folder_id}\" if parent_folder_id else \"raíz de Drive\"\n        print(f\"💾 Guardando resultados: {filename} en {folder_info}\")\n        \n        # Preparar datos JSON\n        try:\n            json_str = json.dumps(results, indent=2, ensure_ascii=False)\n            print(f\"📊 Tamaño de datos: {len(json_str):,} caracteres\")\n        except Exception as e:\n            print(f\"❌ Error serializando datos a JSON: {e}\")\n            return False\n        \n        # Intentar guardar con reintentos\n        for attempt in range(max_retries):\n            try:\n                print(f\"🔄 Intento {attempt + 1}/{max_retries}...\")\n                \n                # Crear buffer de archivo\n                file_io = io.BytesIO(json_str.encode('utf-8'))\n                \n                # Metadata del archivo - incluir carpeta padre si existe\n                file_metadata = {\n                    'name': filename\n                }\n                \n                if parent_folder_id:\n                    file_metadata['parents'] = [parent_folder_id]\n                \n                # Subir archivo con timeout más largo\n                media = MediaIoBaseUpload(file_io, mimetype='application/json', resumable=True)\n                file = service.files().create(\n                    body=file_metadata,\n                    media_body=media,\n                    fields='id,webViewLink,parents'\n                ).execute()\n                \n                print(f\"✅ Resultados guardados exitosamente!\")\n                print(f\"📄 ID: {file.get('id')}\")\n                print(f\"📁 Carpeta: {file.get('parents', ['raíz'])[0] if file.get('parents') else 'raíz'}\")\n                if 'webViewLink' in file:\n                    print(f\"🔗 Ver en Drive: {file['webViewLink']}\")\n                return True\n                \n            except Exception as e:\n                print(f\"❌ Intento {attempt + 1} falló: {e}\")\n                \n                if attempt < max_retries - 1:\n                    import time\n                    wait_time = (attempt + 1) * 5  # 5, 10, 15 segundos\n                    print(f\"⏳ Esperando {wait_time}s antes del siguiente intento...\")\n                    time.sleep(wait_time)\n                else:\n                    print(f\"❌ Todos los intentos fallaron\")\n        \n        return False\n\n    def save_results_locally(results, filename: str) -> bool:\n        \"\"\"Guardar resultados localmente como backup.\"\"\"\n        try:\n            local_filename = f\"/content/{filename}\"\n            with open(local_filename, 'w', encoding='utf-8') as f:\n                json.dump(results, f, indent=2, ensure_ascii=False)\n            \n            print(f\"💾 Backup local guardado: {local_filename}\")\n            \n            # Mostrar tamaño del archivo\n            import os\n            file_size = os.path.getsize(local_filename)\n            print(f\"📊 Tamaño del archivo: {file_size:,} bytes ({file_size/1024/1024:.1f} MB)\")\n            \n            return True\n        except Exception as e:\n            print(f\"❌ Error guardando backup local: {e}\")\n            return False\n\n    def display_results_summary(results):\n        \"\"\"Mostrar resumen de resultados.\"\"\"\n        print(\"\\n📊 RESUMEN DE RESULTADOS:\")\n        print(\"=\" * 50)\n        \n        # Estadísticas de ejecución\n        stats = results.get('execution_stats', {})\n        print(f\"🎯 Método: {stats.get('evaluation_method', 'N/A')}\")\n        print(f\"📋 Preguntas procesadas: {stats.get('questions_processed', 0)}\")\n        print(f\"🤖 Modelos evaluados: {stats.get('models_evaluated', 0)}\")\n        print(f\"⏱️ Tiempo total: {stats.get('total_time', 0):.1f}s\")\n        print(f\"📈 Tasa de éxito: {stats.get('success_rate', 0)*100:.1f}%\")\n        print(f\"📐 Fórmula: {stats.get('composite_score_formula', 'N/A')}\")\n        \n        # Top modelos por score compuesto\n        print(f\"\\n🏆 RANKING DE MODELOS:\")\n        model_scores = []\n        for model_key, metrics in results.get('consolidated_metrics', {}).items():\n            if 'composite_score' in metrics:\n                score = metrics['composite_score']['mean']\n                count = metrics['composite_score']['count']\n                model_scores.append((model_key, score, count))\n        \n        # Ordenar por score descendente\n        model_scores.sort(key=lambda x: x[1], reverse=True)\n        \n        for i, (model, score, count) in enumerate(model_scores):\n            rank_emoji = [\"🥇\", \"🥈\", \"🥉\"][i] if i < 3 else f\"{i+1}.\"\n            print(f\"{rank_emoji} {model.upper()}: {score:.3f} ({count} preguntas)\")\n\n    def verify_config_and_results_in_same_folder(config_filename: str, results_filename: str):\n        \"\"\"Verificar que ambos archivos están en la misma carpeta.\"\"\"\n        try:\n            # Buscar ambos archivos\n            for filename in [config_filename, results_filename]:\n                results = service.files().list(\n                    q=f\"name='{filename}' and trashed=false\",\n                    fields=\"files(id, name, parents, webViewLink)\"\n                ).execute()\n                \n                files = results.get('files', [])\n                if files:\n                    file_info = files[0]\n                    parents = file_info.get('parents', ['raíz'])\n                    folder = parents[0] if parents else 'raíz'\n                    \n                    print(f\"📄 {filename}\")\n                    print(f\"   └─ Carpeta: {folder}\")\n                    if 'webViewLink' in file_info:\n                        print(f\"   └─ Link: {file_info['webViewLink']}\")\n                    \n        except Exception as e:\n            print(f\"⚠️ Error verificando archivos: {e}\")\n\n    # Obtener carpeta donde está el archivo de configuración\n    print(f\"🎯 Iniciando proceso de guardado...\")\n    print(f\"📂 Buscando carpeta del archivo de configuración...\")\n    \n    parent_folder_id = get_config_file_parent_folder()\n    \n    if parent_folder_id:\n        print(f\"✅ Guardará en la misma carpeta que la configuración\")\n    else:\n        print(f\"📁 Guardará en la raíz de Google Drive\")\n    \n    # Intentar guardar en Google Drive\n    results_filename = config['output_config']['results_filename']\n    \n    drive_success = save_results_to_drive_with_retry(results, results_filename, parent_folder_id, max_retries=3)\n    \n    # Guardar backup local siempre\n    local_success = save_results_locally(results, results_filename)\n    \n    # Mostrar resultados del guardado\n    if drive_success:\n        print(f\"\\n🎉 ¡Análisis N Preguntas Completado Exitosamente!\")\n        print(f\"✅ Guardado en Google Drive: {results_filename}\")\n        \n        if local_success:\n            print(f\"✅ Backup local también disponible\")\n        \n        # Verificar que ambos archivos están juntos\n        print(f\"\\n📂 Verificando ubicación de archivos:\")\n        verify_config_and_results_in_same_folder(CONFIG_FILENAME, results_filename)\n        \n        print(f\"\\n📋 Próximos pasos:\")\n        print(f\"1. Ve a la aplicación Streamlit\")\n        print(f\"2. Navega a 'Resultados Análisis N Preguntas'\")\n        print(f\"3. Busca el archivo: {results_filename}\")\n        print(f\"4. Explora las visualizaciones y métricas detalladas\")\n        print(f\"5. ✨ Los archivos están en la misma carpeta para fácil acceso\")\n        \n    elif local_success:\n        print(f\"\\n⚠️ Google Drive falló, pero se guardó backup local\")\n        print(f\"💾 Archivo local: /content/{results_filename}\")\n        print(f\"\\n📋 Para usar en Streamlit:\")\n        print(f\"1. Descarga el archivo desde Colab:\")\n        print(f\"   - Ve a Files (📁) en el panel izquierdo\")\n        print(f\"   - Busca: {results_filename}\")\n        print(f\"   - Click derecho → Download\")\n        print(f\"2. Sube a Google Drive en la MISMA carpeta que:\")\n        print(f\"   - {CONFIG_FILENAME}\")\n        print(f\"3. Ve a Streamlit y carga el archivo\")\n        \n    else:\n        print(f\"\\n❌ Error en todos los métodos de guardado\")\n        print(f\"💡 Los resultados siguen disponibles en la variable 'results'\")\n        print(f\"📋 Puedes intentar ejecutar esta celda nuevamente\")\n    \n    # Mostrar resumen de resultados\n    display_results_summary(results)\n    \n    # Información de descarga manual si es necesario\n    if not drive_success:\n        print(f\"\\n🔧 DESCARGA MANUAL:\")\n        print(f\"Si necesitas descargar los resultados manualmente:\")\n        \n        # Crear un archivo comprimido con metadatos\n        try:\n            summary_data = {\n                'filename': results_filename,\n                'config_filename': CONFIG_FILENAME,\n                'target_folder': parent_folder_id or 'root',\n                'execution_stats': results.get('execution_stats', {}),\n                'model_ranking': [(model, metrics.get('composite_score', {}).get('mean', 0)) \n                                 for model, metrics in results.get('consolidated_metrics', {}).items()],\n                'total_questions': len(results.get('individual_results', {})),\n                'timestamp': results.get('execution_stats', {}).get('completed_at', 'Unknown')\n            }\n            \n            with open('/content/results_summary.json', 'w') as f:\n                json.dump(summary_data, f, indent=2)\n            \n            print(f\"📄 Resumen disponible en: /content/results_summary.json\")\n            print(f\"📁 Incluye información de la carpeta objetivo\")\n            \n        except Exception as e:\n            print(f\"⚠️ No se pudo crear resumen: {e}\")\n    \n    print(f\"\\n✅ Proceso de guardado completado\")"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "accelerator": "GPU"
 },
 "nbformat": 4,
 "nbformat_minor": 4
}