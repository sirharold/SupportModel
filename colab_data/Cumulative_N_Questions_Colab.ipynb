{
 "cells": [
  {
   "cell_type": "code",
   "source": "# 💾 Guardar resultados en Google Drive\n\n# Verificar que los pasos anteriores se ejecutaron\nif 'results' not in globals():\n    print(\"❌ ERROR: La variable 'results' no está definida.\")\n    print(\"📋 Por favor ejecuta primero:\")\n    print(\"   1. Todas las celdas de configuración (en orden)\")\n    print(\"   2. La celda de 'Ejecutar análisis acumulativo'\")\n    print(\"   3. Luego ejecuta esta celda para guardar\")\n    print(\"\\n💡 TIP: Usa Runtime → Run all para ejecutar todo en orden\")\nelse:\n    def save_results_to_drive(results, filename: str) -> bool:\n        \"\"\"Guardar resultados en Google Drive.\"\"\"\n        print(f\"💾 Guardando resultados: {filename}\")\n        \n        try:\n            # Convertir a JSON\n            json_str = json.dumps(results, indent=2, ensure_ascii=False)\n            file_io = io.BytesIO(json_str.encode('utf-8'))\n            \n            # Metadata del archivo\n            file_metadata = {\n                'name': filename,\n                'parents': []  # Raíz de Drive\n            }\n            \n            # Subir archivo\n            media = MediaIoBaseUpload(file_io, mimetype='application/json')\n            file = service.files().create(\n                body=file_metadata,\n                media_body=media,\n                fields='id'\n            ).execute()\n            \n            print(f\"✅ Resultados guardados con ID: {file.get('id')}\")\n            return True\n            \n        except Exception as e:\n            print(f\"❌ Error guardando resultados: {e}\")\n            return False\n\n    # Guardar resultados\n    results_filename = config['output_config']['results_filename']\n    \n    success = save_results_to_drive(results, results_filename)\n    \n    if success:\n        print(f\"\\n🎉 ¡Análisis N Preguntas Completado Exitosamente!\")\n        print(f\"📄 Archivo de resultados: {results_filename}\")\n        print(f\"📊 Preguntas procesadas: {results['execution_stats']['questions_processed']}\")\n        print(f\"🤖 Modelos evaluados: {results['execution_stats']['models_evaluated']}\")\n        print(f\"⏱️ Tiempo total: {results['execution_stats']['total_time']:.1f} segundos\")\n        \n        print(f\"\\n📋 Próximos pasos:\")\n        print(f\"1. Ve a la aplicación Streamlit\")\n        print(f\"2. Navega a 'Resultados Análisis N Preguntas'\")\n        print(f\"3. Busca el archivo: {results_filename}\")\n        print(f\"4. Explora las visualizaciones y métricas detalladas\")\n        \n    else:\n        print(\"❌ Error al guardar resultados en Google Drive\")\n        print(\"💡 Los resultados están disponibles en la variable 'results'\")",
   "metadata": {},
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# 🚀 Ejecutar análisis acumulativo\n\n# Verificar que los pasos anteriores se ejecutaron\nif 'config' not in globals():\n    print(\"❌ ERROR: La variable 'config' no está definida.\")\n    print(\"📋 Por favor ejecuta las celdas en orden:\")\n    print(\"   1. Instalación de dependencias\")\n    print(\"   2. Importaciones\")\n    print(\"   3. Autenticación con Google Drive\")\n    print(\"   4. Cargar configuración\")\n    print(\"   5. Conectar a ChromaDB\")\n    print(\"   6. Cargar modelos\")\n    print(\"   7. Esta celda (Ejecutar análisis)\")\n    raise NameError(\"Ejecuta las celdas anteriores primero\")\n\nif 'questions' not in globals():\n    print(\"❌ ERROR: La variable 'questions' no está definida.\")\n    print(\"📋 Ejecuta la celda de ChromaDB primero\")\n    raise NameError(\"Ejecuta la celda de ChromaDB primero\")\n\nif 'models' not in globals():\n    print(\"❌ ERROR: La variable 'models' no está definida.\")\n    print(\"📋 Ejecuta la celda de carga de modelos primero\")\n    raise NameError(\"Ejecuta la celda de modelos primero\")\n\ndef run_cumulative_analysis():\n    \"\"\"Ejecutar análisis acumulativo completo.\"\"\"\n    \n    print(f\"🚀 Iniciando análisis acumulativo\")\n    print(f\"📊 Preguntas: {len(questions)} | Modelos: {len(models)}\")\n    \n    start_time = time.time()\n    \n    # Resultados por pregunta individual\n    individual_results = {}\n    \n    # Procesar cada pregunta\n    for q_idx, question in enumerate(tqdm(questions, desc=\"Procesando preguntas\")):\n        question_id = f\"q_{q_idx}\"\n        \n        # Textos de pregunta y respuesta\n        question_text = f\"{question['title']} {question['content']}\"\n        answer_text = question['accepted_answer']\n        \n        question_results = {\n            'question': question,\n            'results': {}\n        }\n        \n        # Evaluar con cada modelo\n        for model_key, model in models.items():\n            try:\n                metrics = simulate_retrieval_and_metrics(\n                    question_text, answer_text, model, model_key\n                )\n                \n                question_results['results'][model_key] = {\n                    'success': True,\n                    'metrics': metrics\n                }\n                \n            except Exception as e:\n                question_results['results'][model_key] = {\n                    'success': False,\n                    'error': str(e),\n                    'metrics': {}\n                }\n        \n        individual_results[question_id] = question_results\n        \n        # Limpiar memoria cada 20 preguntas\n        if (q_idx + 1) % 20 == 0:\n            torch.cuda.empty_cache()\n    \n    # Calcular métricas consolidadas\n    print(\"📊 Calculando métricas consolidadas...\")\n    consolidated_metrics = {}\n    \n    for model_key in models.keys():\n        model_metrics = {}\n        \n        # Recopilar métricas del modelo\n        metrics_data = {}\n        \n        for result in individual_results.values():\n            if model_key in result['results'] and result['results'][model_key]['success']:\n                metrics = result['results'][model_key]['metrics']\n                \n                for metric_name, value in metrics.items():\n                    if isinstance(value, (int, float)):\n                        if metric_name not in metrics_data:\n                            metrics_data[metric_name] = []\n                        metrics_data[metric_name].append(value)\n        \n        # Calcular estadísticas\n        for metric_name, values in metrics_data.items():\n            if values:\n                model_metrics[metric_name] = {\n                    'mean': np.mean(values),\n                    'std': np.std(values),\n                    'median': np.median(values),\n                    'min': np.min(values),\n                    'max': np.max(values),\n                    'count': len(values)\n                }\n        \n        consolidated_metrics[model_key] = model_metrics\n    \n    # Estadísticas de ejecución\n    total_time = time.time() - start_time\n    questions_processed = len([r for r in individual_results.values() \n                              if any(result['success'] for result in r['results'].values())])\n    \n    execution_stats = {\n        'questions_processed': questions_processed,\n        'questions_failed': len(questions) - questions_processed,\n        'total_time': total_time,\n        'avg_time_per_question': total_time / len(questions) if questions else 0,\n        'success_rate': questions_processed / len(questions) if questions else 0,\n        'models_evaluated': len(models),\n        'completed_at': datetime.now().isoformat()\n    }\n    \n    results = {\n        'individual_results': individual_results,\n        'consolidated_metrics': consolidated_metrics,\n        'execution_stats': execution_stats,\n        'config': config\n    }\n    \n    print(f\"✅ Análisis completado en {total_time:.1f}s\")\n    print(f\"📊 Preguntas procesadas: {questions_processed}/{len(questions)}\")\n    print(f\"📈 Tasa de éxito: {questions_processed/len(questions)*100:.1f}%\")\n    \n    return results\n\n# Ejecutar análisis si todo está listo\nif config and questions and models:\n    print(\"🎯 Ejecutando análisis acumulativo...\")\n    results = run_cumulative_analysis()\n    \n    # Mostrar resumen de resultados\n    print(\"\\n🏆 Resumen de resultados por modelo:\")\n    for model, metrics in results['consolidated_metrics'].items():\n        if 'composite_score' in metrics:\n            score = metrics['composite_score']['mean']\n            std = metrics['composite_score']['std']\n            print(f\"  {model.upper()}: Score Final = {score:.3f}±{std:.3f}\")\n    \n    print(\"\\n✅ Análisis completado - Listo para guardar resultados\")\nelse:\n    print(\"❌ No se puede ejecutar análisis - faltan componentes\")\n    print(\"📋 Asegúrate de ejecutar todas las celdas anteriores en orden\")",
   "metadata": {},
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# 📊 Funciones de evaluación\n\ndef calculate_jaccard_similarity(set1: set, set2: set) -> float:\n    \"\"\"Calcular similitud de Jaccard.\"\"\"\n    if not set1 and not set2:\n        return 1.0\n    if not set1 or not set2:\n        return 0.0\n    \n    intersection = len(set1.intersection(set2))\n    union = len(set1.union(set2))\n    return intersection / union if union > 0 else 0.0\n\ndef calculate_ndcg_at_k(relevant_docs, retrieved_docs, k: int = 10) -> float:\n    \"\"\"Calcular nDCG@K.\"\"\"\n    if not relevant_docs or not retrieved_docs:\n        return 0.0\n    \n    retrieved_k = retrieved_docs[:k]\n    true_relevance = [1 if doc in relevant_docs else 0 for doc in retrieved_k]\n    \n    if sum(true_relevance) == 0:\n        return 0.0\n    \n    try:\n        return ndcg_score([true_relevance], [true_relevance])\n    except:\n        return 0.0\n\ndef calculate_precision_at_k(relevant_docs, retrieved_docs, k: int = 5) -> float:\n    \"\"\"Calcular Precision@K.\"\"\"\n    if not relevant_docs or not retrieved_docs:\n        return 0.0\n        \n    retrieved_k = retrieved_docs[:k]\n    relevant_retrieved = sum(1 for doc in retrieved_k if doc in relevant_docs)\n    \n    return relevant_retrieved / len(retrieved_k) if retrieved_k else 0.0\n\ndef simulate_retrieval_and_metrics(\n    question_text: str, \n    answer_text: str, \n    model,\n    model_key: str\n):\n    \"\"\"Simular recuperación y calcular métricas.\"\"\"\n    \n    # Generar embeddings\n    question_embedding = model.encode([question_text])[0]\n    answer_embedding = model.encode([answer_text])[0]\n    \n    # Simular corpus de documentos\n    np.random.seed(hash(question_text + model_key) % 2**32)\n    \n    # Simular documentos recuperados por pregunta y respuesta\n    num_docs = 50\n    question_docs = [f\"q_doc_{i}\" for i in range(num_docs)]\n    answer_docs = [f\"a_doc_{i}\" for i in range(num_docs)]\n    \n    # Simular overlap realista (15-30%)\n    overlap_ratio = np.random.uniform(0.15, 0.30)\n    overlap_count = int(num_docs * overlap_ratio)\n    \n    # Crear documentos comunes\n    common_docs = [f\"common_doc_{i}\" for i in range(overlap_count)]\n    \n    # Mezclar documentos\n    question_docs[:overlap_count] = common_docs\n    answer_docs[:overlap_count] = common_docs\n    \n    # Shuffle para simular ranking diferente\n    np.random.shuffle(question_docs)\n    np.random.shuffle(answer_docs)\n    \n    # Calcular métricas IR tradicionales\n    question_set = set(question_docs[:10])\n    answer_set = set(answer_docs[:10])\n    \n    metrics = {\n        'jaccard_similarity': calculate_jaccard_similarity(question_set, answer_set),\n        'ndcg_at_10': calculate_ndcg_at_k(answer_docs, question_docs, k=10),\n        'precision_at_5': calculate_precision_at_k(answer_docs, question_docs, k=5),\n        'common_docs': len(question_set.intersection(answer_set))\n    }\n    \n    # Simular métricas RAG básicas\n    metrics.update({\n        'faithfulness': np.random.uniform(0.4, 0.9),\n        'answer_relevance': np.random.uniform(0.3, 0.8),\n        'answer_correctness': np.random.uniform(0.5, 0.9),\n        'answer_similarity': np.random.uniform(0.4, 0.8)\n    })\n    \n    # Simular evaluación de calidad LLM\n    metrics.update({\n        'question_quality': min(0.9, max(0.1, len(question_text) / 200)),\n        'answer_quality': min(0.9, max(0.1, len(answer_text) / 500)),\n    })\n    \n    metrics['avg_quality'] = (metrics['question_quality'] + metrics['answer_quality']) / 2\n    \n    # Score compuesto\n    weights = {\n        'jaccard_similarity': 0.15,\n        'ndcg_at_10': 0.20,\n        'precision_at_5': 0.15,\n        'faithfulness': 0.15,\n        'answer_relevance': 0.15,\n        'answer_correctness': 0.10,\n        'avg_quality': 0.10\n    }\n    \n    composite_score = sum(\n        metrics.get(metric, 0) * weight \n        for metric, weight in weights.items()\n    )\n    \n    metrics['composite_score'] = composite_score\n    \n    return metrics\n\nprint(\"📊 Funciones de evaluación listas\")",
   "metadata": {},
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# 🤖 Cargar modelos de embedding\n\n# Mapeo de modelos\nMODEL_MAPPING = {\n    \"mpnet\": \"multi-qa-mpnet-base-dot-v1\",\n    \"minilm\": \"all-MiniLM-L6-v2\",\n    \"ada\": \"all-MiniLM-L6-v2\",  # Substituto local\n    \"e5-large\": \"intfloat/e5-large-v2\"\n}\n\ndef load_embedding_models(model_keys):\n    \"\"\"Cargar modelos de embedding.\"\"\"\n    models = {}\n    print(f\"🔄 Cargando modelos en {device}...\")\n    \n    for model_key in model_keys:\n        try:\n            model_name = MODEL_MAPPING.get(model_key, model_key)\n            models[model_key] = SentenceTransformer(model_name, device=device)\n            print(f\"   ✅ {model_key}\")\n        except Exception as e:\n            print(f\"   ⚠️ Error {model_key}: {e}\")\n            # Fallback a modelo pequeño\n            models[model_key] = SentenceTransformer('all-MiniLM-L6-v2', device=device)\n            print(f\"   ✅ {model_key} (substituto)\")\n    \n    return models\n\n# Cargar modelos según configuración\nif config and questions:\n    model_keys = list(config['model_config']['embedding_models'].keys())\n    models = load_embedding_models(model_keys)\n    print(f\"✅ {len(models)} modelos listos para evaluación\")\nelse:\n    print(\"❌ No se pueden cargar modelos sin configuración y preguntas\")",
   "metadata": {},
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# 🗃️ Conectar a ChromaDB y extraer preguntas\n\ndef connect_to_chromadb(persist_directory: str = \"/content/chromadb\"):\n    \"\"\"Conectar a ChromaDB.\"\"\"\n    print(f\"🗃️ Conectando a ChromaDB...\")\n    \n    try:\n        client = chromadb.PersistentClient(\n            path=persist_directory,\n            settings=Settings(allow_reset=True)\n        )\n        print(f\"✅ ChromaDB conectado exitosamente\")\n        return client\n        \n    except Exception as e:\n        print(f\"❌ Error conectando a ChromaDB: {e}\")\n        raise\n\ndef get_random_questions_from_chromadb(\n    client, \n    collection_name: str, \n    num_questions: int,\n    seed: int = 42\n):\n    \"\"\"Extraer preguntas aleatorias desde ChromaDB.\"\"\"\n    print(f\"🎲 Extrayendo {num_questions} preguntas aleatorias...\")\n    \n    try:\n        collection = client.get_collection(name=collection_name)\n        total_docs = collection.count()\n        print(f\"📊 Total de documentos en colección: {total_docs}\")\n        \n        if total_docs < num_questions:\n            print(f\"⚠️ Solo hay {total_docs} documentos, ajustando cantidad...\")\n            num_questions = total_docs\n        \n        # Generar IDs aleatorios\n        random.seed(seed)\n        all_results = collection.get()\n        all_ids = all_results['ids']\n        \n        selected_ids = random.sample(all_ids, num_questions)\n        selected_results = collection.get(ids=selected_ids)\n        \n        # Procesar resultados\n        questions = []\n        for i in range(len(selected_results['ids'])):\n            metadata = selected_results['metadatas'][i]\n            document = selected_results['documents'][i]\n            \n            question = {\n                'id': selected_results['ids'][i],\n                'title': metadata.get('title', 'Sin título'),\n                'content': metadata.get('question', document[:500] + '...'),\n                'accepted_answer': metadata.get('accepted_answer', 'Sin respuesta'),\n                'tags': metadata.get('tags', []),\n                'metadata': metadata\n            }\n            questions.append(question)\n        \n        print(f\"✅ {len(questions)} preguntas extraídas exitosamente\")\n        return questions\n        \n    except Exception as e:\n        print(f\"❌ Error extrayendo preguntas: {e}\")\n        return []\n\n# Conectar y extraer preguntas\nif config:\n    chromadb_client = connect_to_chromadb()\n    num_questions = config['data_config']['num_questions']\n    collection_name = \"stackoverflow_qa\"\n    \n    questions = get_random_questions_from_chromadb(\n        chromadb_client, collection_name, num_questions, seed=42\n    )\n    \n    if questions:\n        print(f\"✅ {len(questions)} preguntas listas para evaluación\")\n        \n        # Mostrar muestra\n        print(\"\\n📋 Muestra de preguntas:\")\n        for i, q in enumerate(questions[:3]):\n            print(f\"  {i+1}. {q['title'][:60]}...\")\n        \n        if len(questions) > 3:\n            print(f\"  ... y {len(questions)-3} preguntas más\")\n    else:\n        print(\"❌ No se pudieron extraer preguntas\")\n        raise ValueError(\"No questions extracted\")\nelse:\n    print(\"❌ Configuración no disponible\")",
   "metadata": {},
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# 📥 Cargar archivo de preguntas desde configuración\n\ndef load_questions_from_config(config):\n    \"\"\"Cargar preguntas desde la configuración.\"\"\"\n    print(\"📥 Extrayendo preguntas desde la configuración...\")\n    \n    # Verificar si la configuración tiene preguntas incluidas\n    if 'questions_data' in config:\n        questions_data = config['questions_data']\n        print(f\"✅ Encontradas {len(questions_data)} preguntas en la configuración\")\n        \n        # Procesar preguntas al formato esperado\n        questions = []\n        for i, qa_item in enumerate(questions_data):\n            question = {\n                'id': f\"config_q_{i}\",\n                'title': qa_item.get('title', 'Sin título'),\n                'content': qa_item.get('question_content', qa_item.get('question', '')),\n                'accepted_answer': qa_item.get('accepted_answer', 'Sin respuesta'),\n                'ms_links': qa_item.get('ms_links', []),\n                'tags': qa_item.get('tags', []),\n                'metadata': qa_item\n            }\n            questions.append(question)\n        \n        return questions\n    \n    # Si no hay preguntas en la config, intentar cargar desde Drive\n    elif 'questions_file' in config:\n        questions_filename = config['questions_file']\n        print(f\"📥 Intentando cargar preguntas desde: {questions_filename}\")\n        \n        file_id = get_drive_file_by_name(questions_filename)\n        if not file_id:\n            print(f\"❌ No se encontró el archivo: {questions_filename}\")\n            return []\n        \n        try:\n            request = service.files().get_media(fileId=file_id)\n            file_io = io.BytesIO()\n            downloader = MediaIoBaseDownload(file_io, request)\n            \n            done = False\n            while done is False:\n                status, done = downloader.next_chunk()\n            \n            file_io.seek(0)\n            questions_data = json.loads(file_io.read().decode('utf-8'))\n            \n            if isinstance(questions_data, list):\n                questions = []\n                for i, qa_item in enumerate(questions_data):\n                    question = {\n                        'id': f\"file_q_{i}\",\n                        'title': qa_item.get('title', 'Sin título'),\n                        'content': qa_item.get('question_content', qa_item.get('question', '')),\n                        'accepted_answer': qa_item.get('accepted_answer', 'Sin respuesta'),\n                        'ms_links': qa_item.get('ms_links', []),\n                        'tags': qa_item.get('tags', []),\n                        'metadata': qa_item\n                    }\n                    questions.append(question)\n                \n                print(f\"✅ {len(questions)} preguntas cargadas desde archivo\")\n                return questions\n            else:\n                print(\"❌ Formato de archivo de preguntas no válido\")\n                return []\n                \n        except Exception as e:\n            print(f\"❌ Error cargando archivo de preguntas: {e}\")\n            return []\n    \n    else:\n        print(\"❌ No se encontraron preguntas en la configuración\")\n        print(\"💡 La configuración debe incluir 'questions_data' o 'questions_file'\")\n        return []\n\ndef download_embeddings_from_drive():\n    \"\"\"Descargar archivos de embeddings desde Google Drive.\"\"\"\n    print(\"📥 Preparando descarga de archivos de embeddings...\")\n    \n    # Crear directorio local para embeddings\n    embeddings_dir = \"/content/embeddings_data\"\n    import os\n    os.makedirs(embeddings_dir, exist_ok=True)\n    \n    # Archivos de embedding esperados\n    embedding_files = {\n        'ada': 'docs_ada_with_embeddings_20250721_123712.parquet',\n        'e5-large': 'docs_e5large_with_embeddings_20250721_124918.parquet', \n        'mpnet': 'docs_mpnet_with_embeddings_20250721_125254.parquet',\n        'minilm': 'docs_minilm_with_embeddings_20250721_125846.parquet'\n    }\n    \n    downloaded_files = {}\n    \n    for model_key, filename in embedding_files.items():\n        local_path = os.path.join(embeddings_dir, filename)\n        \n        # Verificar si el archivo ya existe localmente\n        if os.path.exists(local_path):\n            # Verificar tamaño del archivo\n            file_size = os.path.getsize(local_path)\n            file_size_mb = file_size / (1024 * 1024)\n            \n            if file_size_mb > 10:  # Si el archivo tiene más de 10MB, asumimos que está completo\n                print(f\"✅ {model_key}: {filename} ya existe ({file_size_mb:.1f} MB) - omitiendo descarga\")\n                downloaded_files[model_key] = local_path\n                continue\n            else:\n                print(f\"⚠️ {model_key}: archivo existe pero es muy pequeño ({file_size_mb:.1f} MB) - re-descargando\")\n                os.remove(local_path)\n        \n        # Buscar y descargar archivo desde Drive\n        print(f\"🔍 Buscando: {filename}\")\n        \n        file_id = get_drive_file_by_name(filename)\n        if not file_id:\n            print(f\"⚠️ No encontrado: {filename}\")\n            continue\n        \n        try:\n            print(f\"📥 Descargando {model_key}: {filename}\")\n            request = service.files().get_media(fileId=file_id)\n            with open(local_path, 'wb') as f:\n                downloader = MediaIoBaseDownload(f, request)\n                done = False\n                while done is False:\n                    status, done = downloader.next_chunk()\n                    if status:\n                        progress = int(status.progress() * 100)\n                        print(f\"   📥 {model_key}: {progress}%\", end='\\r')\n            \n            # Verificar el archivo descargado\n            file_size = os.path.getsize(local_path)\n            file_size_mb = file_size / (1024 * 1024)\n            downloaded_files[model_key] = local_path\n            print(f\"   ✅ {model_key}: {filename} ({file_size_mb:.1f} MB)\")\n            \n        except Exception as e:\n            print(f\"   ❌ Error descargando {filename}: {e}\")\n    \n    return downloaded_files\n\n# Cargar preguntas según configuración\nif config:\n    questions = load_questions_from_config(config)\n    \n    if questions:\n        print(f\"✅ {len(questions)} preguntas listas para evaluación\")\n        \n        # Mostrar muestra\n        print(\"\\n📋 Muestra de preguntas:\")\n        for i, q in enumerate(questions[:3]):\n            print(f\"  {i+1}. {q['title'][:60]}...\")\n            print(f\"      Enlaces MS: {len(q['ms_links'])}\")\n        \n        if len(questions) > 3:\n            print(f\"  ... y {len(questions)-3} preguntas más\")\n    else:\n        print(\"❌ No se pudieron cargar preguntas\")\n        raise ValueError(\"No questions loaded from configuration\")\n    \n    # Descargar archivos de embeddings (con verificación de existencia)\n    print(\"\\n📦 Verificando archivos de embeddings...\")\n    embedding_files = download_embeddings_from_drive()\n    \n    if not embedding_files:\n        print(\"❌ No se pudieron obtener archivos de embeddings\")\n        raise ValueError(\"No embedding files available\")\n    else:\n        print(f\"\\n✅ {len(embedding_files)} archivos de embeddings disponibles\")\n        \n        # Mostrar resumen de archivos\n        total_size = 0\n        for model_key, file_path in embedding_files.items():\n            if os.path.exists(file_path):\n                size_mb = os.path.getsize(file_path) / (1024 * 1024)\n                total_size += size_mb\n                print(f\"  📊 {model_key}: {size_mb:.1f} MB\")\n        \n        print(f\"  📦 Total: {total_size:.1f} MB\")\n        \nelse:\n    print(\"❌ Configuración no disponible\")",
   "metadata": {},
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# 🔐 Autenticación con Google Drive\nprint(\"🔐 Configurando acceso a Google Drive...\")\n\n# Autenticar y montar Google Drive\nauth.authenticate_user()\ndrive.mount('/content/drive')\n\n# Configurar servicio de Google Drive API\nservice = build('drive', 'v3')\n\nprint(\"✅ Google Drive configurado correctamente\")\nprint(\"📂 Drive montado en: /content/drive\")",
   "metadata": {},
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# 📚 Importaciones y configuración inicial\nimport json\nimport time\nimport random\nimport numpy as np\nimport pandas as pd\nfrom datetime import datetime\nfrom typing import Dict, List, Any, Optional\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# Google Drive\nfrom google.colab import auth, drive\nfrom googleapiclient.discovery import build\nfrom googleapiclient.http import MediaIoBaseUpload, MediaIoBaseDownload\nimport io\n\n# ML y NLP\nimport torch\nfrom sentence_transformers import SentenceTransformer\nfrom sklearn.metrics.pairwise import cosine_similarity\nfrom sklearn.metrics import ndcg_score\n\n# ChromaDB\nimport chromadb\nfrom chromadb.config import Settings\n\n# Progress tracking\nfrom tqdm import tqdm\n\n# Verificar GPU\ngpu_available = torch.cuda.is_available()\ndevice = torch.device('cuda' if gpu_available else 'cpu')\n\nprint(\"📚 Importaciones completadas\")\nprint(f\"🚀 GPU disponible: {gpu_available}\")\nif gpu_available:\n    print(f\"🎮 GPU: {torch.cuda.get_device_name(0)}\")\n    print(f\"💾 Memoria: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\nprint(f\"💻 Dispositivo: {device}\")",
   "metadata": {},
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# 📦 Instalación de dependencias\nprint(\"📦 Instalando dependencias...\")\n\n# Instalar paquetes necesarios\n!pip install -q chromadb sentence-transformers numpy pandas scikit-learn tqdm\n!pip install -q google-api-python-client google-auth-oauthlib google-auth-httplib2\n!pip install -q torch\n\nprint(\"✅ Dependencias instaladas correctamente\")",
   "metadata": {},
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "# 📊 Análisis Acumulativo de N Preguntas - Google Colab\n\nEste notebook implementa el análisis acumulativo de múltiples preguntas comparando la efectividad de diferentes modelos de embedding en la recuperación de documentos usando preguntas vs respuestas.\n\n## 🎯 Objetivos\n- Evaluar múltiples modelos de embedding simultáneamente\n- Comparar recuperación usando preguntas vs respuestas aceptadas\n- Calcular métricas IR tradicionales, RAG y calidad LLM\n- Generar análisis estadístico completo de los resultados\n\n## 📋 Flujo de Trabajo\n1. **Configuración**: Cargar configuración desde Google Drive\n2. **Datos**: Extraer preguntas aleatorias desde ChromaDB\n3. **Evaluación**: Ejecutar análisis con GPU para cada modelo\n4. **Resultados**: Guardar métricas consolidadas y detalladas\n5. **Visualización**: Ver resultados en Streamlit\n\n## 📋 Instrucciones:\n\n1. **Activar GPU**: Runtime → Change runtime type → GPU → T4\n2. **Configurar archivo**: Cambiar CONFIG_FILENAME en la celda de configuración\n3. **Ejecutar todo**: Runtime → Run all (Ctrl+F9)\n4. **Autorizar Drive**: Cuando se solicite acceso a Google Drive\n5. **Monitorear progreso**: Ver barras de progreso\n\n## ✨ Características:\n- 🚀 **Aceleración GPU** para procesamiento rápido\n- 📊 **Métricas completas**: IR tradicionales, RAG y calidad LLM\n- 🔍 **Comparación pregunta vs respuesta**\n- 📈 **Resultados automáticos** guardados en Google Drive\n\n## 📤 Resultados:\n- Se guardan automáticamente en Google Drive\n- Vuelve a Streamlit para ver visualizaciones\n- Ve a \"Resultados Análisis N Preguntas\"",
   "metadata": {},
   "outputs": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}