#!/usr/bin/env python3
"""
RAG Evaluation Library - RAGAS integration and LLM reranking
"""

import json
import re
import gc
import openai
from typing import List, Dict, Tuple, Optional, Any
from datetime import datetime
import pytz
import numpy as np
import pandas as pd
from datasets import Dataset

# RAGAS imports
from ragas import evaluate
from ragas.metrics import (
    faithfulness,
    answer_relevancy,
    context_precision,
    context_recall,
    answer_correctness,
    answer_similarity
)

# BERTScore import
try:
    from bert_score import score as bert_score
    BERTSCORE_AVAILABLE = True
except ImportError:
    BERTSCORE_AVAILABLE = False
    print("⚠️ BERTScore not available. Install with: pip install bert-score")

class RAGCalculator:
    """RAGAS-based RAG metrics calculator"""
    
    def __init__(self, model_name: str = "gpt-3.5-turbo", debug: bool = False):
        self.model_name = model_name
        self.debug = debug
        
        # Configure RAGAS metrics
        self.metrics = [
            faithfulness,           # How grounded the answer is in the context
            answer_relevancy,       # How relevant the answer is to the question
            answer_correctness,     # Overall correctness of the answer
            answer_similarity       # Semantic similarity to reference answer
        ]
        
        # Optional metrics (computationally expensive)
        self.optional_metrics = [
            context_precision,      # Precision of retrieved context
            context_recall         # Recall of retrieved context
        ]
    
    def prepare_ragas_dataset(self, question: str, context_docs: List[Dict], 
                             generated_answer: str, reference_answer: str = None) -> Dataset:
        """
        Prepare dataset for RAGAS evaluation
        
        Args:
            question: The input question
            context_docs: List of retrieved documents with 'content' field
            generated_answer: Answer generated by the RAG system
            reference_answer: Optional reference answer for comparison
            
        Returns:
            RAGAS Dataset object
        """
        # Extract context from documents
        contexts = []
        for doc in context_docs:
            if isinstance(doc, dict):
                content = doc.get('content', str(doc))
            else:
                content = str(doc)
            contexts.append(content)
        
        # Prepare data for RAGAS
        data = {
            'question': [question],
            'answer': [generated_answer],
            'contexts': [contexts],  # RAGAS expects list of lists
            'ground_truth': [reference_answer] if reference_answer else [generated_answer]
        }
        
        if self.debug:
            print(f"🔍 RAGAS Dataset prepared:")
            print(f"  Question: {question[:100]}...")
            print(f"  Answer: {generated_answer[:100]}...")
            print(f"  Contexts: {len(contexts)} documents")
            print(f"  Ground truth: {reference_answer[:100] if reference_answer else 'Using generated answer'}...")
        
        return Dataset.from_dict(data)
    
    def calculate_rag_metrics(self, question: str, context_docs: List[Dict], 
                             generated_answer: str, reference_answer: str = None) -> Dict[str, float]:
        """
        Calculate RAG metrics using RAGAS framework
        
        Args:
            question: Input question
            context_docs: Retrieved context documents
            generated_answer: Generated answer
            reference_answer: Reference answer (optional)
            
        Returns:
            Dictionary of RAG metrics
        """
        try:
            # Prepare dataset
            dataset = self.prepare_ragas_dataset(
                question, context_docs, generated_answer, reference_answer
            )
            
            # Run RAGAS evaluation
            if self.debug:
                print("🚀 Running RAGAS evaluation...")
            
            result = evaluate(dataset, metrics=self.metrics)
            
            # Extract scores from result
            scores = self._extract_ragas_scores(result)
            
            if self.debug:
                print(f"✅ RAGAS evaluation completed: {scores}")
            
            return scores
            
        except Exception as e:
            print(f"❌ RAGAS evaluation error: {e}")
            
            # Fallback to dummy values for debugging
            return {
                'faithfulness': 0.0,
                'answer_relevancy': 0.0,
                'answer_correctness': 0.0,
                'answer_similarity': 0.0
            }
    
    def _extract_ragas_scores(self, result) -> Dict[str, float]:
        """Extract scores from RAGAS evaluation result"""
        scores = {}
        
        try:
            # Method 1: Check if result has to_pandas method
            if hasattr(result, 'to_pandas'):
                df = result.to_pandas()
                for metric in self.metrics:
                    metric_name = metric.__name__ if hasattr(metric, '__name__') else str(metric)
                    if metric_name in df.columns:
                        # Get the first (and only) row value
                        scores[metric_name] = float(df[metric_name].iloc[0])
            
            # Method 2: Check if result has scores attribute
            elif hasattr(result, 'scores'):
                result_scores = result.scores
                for metric in self.metrics:
                    metric_name = metric.__name__ if hasattr(metric, '__name__') else str(metric)
                    if metric_name in result_scores:
                        scores[metric_name] = float(result_scores[metric_name])
            
            # Method 3: Try accessing as dictionary
            elif hasattr(result, '__dict__'):
                result_dict = result.__dict__
                for metric in self.metrics:
                    metric_name = metric.__name__ if hasattr(metric, '__name__') else str(metric)
                    if metric_name in result_dict:
                        scores[metric_name] = float(result_dict[metric_name])
            
            else:
                # Method 4: Direct dictionary access
                for metric in self.metrics:
                    metric_name = metric.__name__ if hasattr(metric, '__name__') else str(metric)
                    if metric_name in result:
                        scores[metric_name] = float(result[metric_name])
            
        except Exception as e:
            print(f"⚠️ Error extracting RAGAS scores: {e}")
            print(f"🔍 Result type: {type(result)}")
            print(f"🔍 Result attributes: {dir(result)}")
            
            # Fallback: return zeros
            for metric in self.metrics:
                metric_name = metric.__name__ if hasattr(metric, '__name__') else str(metric)
                scores[metric_name] = 0.0
        
        return scores


class LLMReranker:
    """LLM-based document reranker using OpenAI API with enhanced content limits"""
    
    def __init__(self, model_name: str = "gpt-3.5-turbo", debug: bool = False, max_content_length: int = 4000):
        self.model_name = model_name
        self.debug = debug
        self.max_content_length = max_content_length  # Enhanced from 3000 to 4000
        self.client = openai.OpenAI()  # Uses OPENAI_API_KEY from environment
    
    def rerank_documents(self, query: str, documents: List[Dict], top_k: int = 5) -> List[Dict]:
        """
        Rerank documents using LLM relevance scoring
        
        Args:
            query: Search query
            documents: List of documents to rerank
            top_k: Number of top documents to return
            
        Returns:
            List of reranked documents
        """
        if not documents:
            return []
        
        try:
            # Prepare documents for LLM evaluation
            doc_texts = []
            for i, doc in enumerate(documents):
                content = doc.get('content', str(doc))
                # Truncate very long documents (configurable limit for better document context)
                if len(content) > self.max_content_length:
                    # Use intelligent truncation: keep beginning and end
                    half_length = self.max_content_length // 2
                    content = content[:half_length] + "\n\n[...CONTENIDO MEDIO OMITIDO...]\n\n" + content[-half_length:]
                doc_texts.append(f"Document {i+1}: {content}")
            
            # Create prompt for LLM
            prompt = self._create_reranking_prompt(query, doc_texts)
            
            if self.debug:
                print(f"🤖 LLM Reranking prompt length: {len(prompt)} chars")
                print(f"🤖 Processing {len(documents)} documents")
            
            # Call LLM
            response = self.client.chat.completions.create(
                model=self.model_name,
                messages=[
                    {"role": "system", "content": "You are a document relevance expert. Rank documents by relevance to the query."},
                    {"role": "user", "content": prompt}
                ],
                temperature=0.1,  # Low temperature for consistent ranking
                max_tokens=200
            )
            
            llm_response = response.choices[0].message.content.strip()
            
            if self.debug:
                print(f"🤖 LLM Response: {llm_response}")
            
            # Parse LLM response to get rankings
            ranked_indices = self._parse_llm_ranking(llm_response, len(documents))
            
            if self.debug:
                print(f"🤖 Parsed rankings: {ranked_indices}")
            
            # Reorder documents based on LLM ranking
            reranked_docs = []
            for idx in ranked_indices[:top_k]:
                if 0 <= idx < len(documents):
                    reranked_docs.append(documents[idx])
            
            # Fill remaining slots if needed
            remaining_docs = [doc for i, doc in enumerate(documents) 
                            if i not in ranked_indices[:top_k]]
            reranked_docs.extend(remaining_docs[:top_k - len(reranked_docs)])
            
            if self.debug:
                print(f"✅ Reranked {len(documents)} → {len(reranked_docs)} documents")
            
            return reranked_docs[:top_k]
            
        except Exception as e:
            print(f"❌ LLM reranking failed: {e}")
            # Return original documents as fallback
            return documents[:top_k]
    
    def _create_reranking_prompt(self, query: str, doc_texts: List[str]) -> str:
        """Create prompt for LLM document reranking"""
        
        docs_text = "\n\n".join(doc_texts)
        
        prompt = f"""Query: {query}

Documents to rank:
{docs_text}

Please rank these documents by relevance to the query. Respond with only the document numbers in order of relevance (most relevant first), separated by commas.

Example response: 3, 1, 5, 2, 4

Your ranking:"""
        
        return prompt
    
    def _parse_llm_ranking(self, llm_response: str, num_docs: int) -> List[int]:
        """Parse LLM response to extract document ranking"""
        
        try:
            # Extract numbers from response
            # Fixed regex pattern: use single backslash
            numbers = re.findall(r'\d+', llm_response)
            
            if self.debug:
                print(f"🔍 Extracted numbers from LLM: {numbers}")
            
            # Convert to 0-based indices
            indices = []
            for num_str in numbers:
                try:
                    idx = int(num_str) - 1  # Convert to 0-based
                    if 0 <= idx < num_docs:
                        indices.append(idx)
                except ValueError:
                    continue
            
            # Fill missing indices
            all_indices = set(range(num_docs))
            used_indices = set(indices)
            remaining_indices = list(all_indices - used_indices)
            
            # Combine parsed indices with remaining ones
            final_indices = indices + remaining_indices
            
            return final_indices[:num_docs]
            
        except Exception as e:
            print(f"⚠️ Error parsing LLM ranking: {e}")
            # Return original order as fallback
            return list(range(num_docs))


class BERTScoreEvaluator:
    """BERTScore-based semantic similarity evaluator"""
    
    def __init__(self, model_type: str = "microsoft/deberta-xlarge-mnli", debug: bool = False):
        self.model_type = model_type
        self.debug = debug
        
        if not BERTSCORE_AVAILABLE:
            print("⚠️ BERTScore not available. Install with: pip install bert-score")
    
    def calculate_similarity(self, predictions: List[str], references: List[str]) -> Dict[str, float]:
        """
        Calculate BERTScore similarity between predictions and references
        
        Args:
            predictions: List of predicted texts
            references: List of reference texts
            
        Returns:
            Dictionary with precision, recall, and F1 scores
        """
        if not BERTSCORE_AVAILABLE:
            return {'precision': 0.0, 'recall': 0.0, 'f1': 0.0}
        
        try:
            # Calculate BERTScore
            P, R, F1 = bert_score(
                predictions, 
                references, 
                model_type=self.model_type,
                verbose=self.debug
            )
            
            # Convert to float and take mean
            precision = float(P.mean())
            recall = float(R.mean())
            f1 = float(F1.mean())
            
            if self.debug:
                print(f"📊 BERTScore results: P={precision:.3f}, R={recall:.3f}, F1={f1:.3f}")
            
            return {
                'precision': precision,
                'recall': recall,
                'f1': f1
            }
            
        except Exception as e:
            print(f"❌ BERTScore calculation failed: {e}")
            return {'precision': 0.0, 'recall': 0.0, 'f1': 0.0}


class RAGEvaluationPipeline:
    """Complete RAG evaluation pipeline combining all components with enhanced content limits"""
    
    def __init__(self, llm_model: str = "gpt-3.5-turbo", debug: bool = False, max_content_length: int = 4000):
        self.llm_model = llm_model
        self.debug = debug
        
        # Initialize components with enhanced content limits
        self.rag_calculator = RAGCalculator(llm_model, debug)
        self.llm_reranker = LLMReranker(llm_model, debug, max_content_length)  # Enhanced from 3000 to 4000
        self.bert_evaluator = BERTScoreEvaluator(debug=debug)
        
    def evaluate_single_question(self, question: str, retrieved_docs: List[Dict], 
                                generated_answer: str, reference_answer: str = None,
                                use_llm_reranking: bool = True) -> Dict[str, Any]:
        """
        Complete evaluation of a single question
        
        Args:
            question: Input question
            retrieved_docs: Retrieved documents
            generated_answer: Generated answer
            reference_answer: Reference answer (optional)
            use_llm_reranking: Whether to use LLM reranking
            
        Returns:
            Complete evaluation results
        """
        results = {
            'question': question,
            'generated_answer': generated_answer,
            'reference_answer': reference_answer,
            'num_retrieved_docs': len(retrieved_docs),
            'use_llm_reranking': use_llm_reranking
        }
        
        try:
            # Calculate RAG metrics on original documents
            rag_metrics_before = self.rag_calculator.calculate_rag_metrics(
                question, retrieved_docs, generated_answer, reference_answer
            )
            results['rag_metrics_before'] = rag_metrics_before
            
            # Apply LLM reranking if enabled
            if use_llm_reranking:
                reranked_docs = self.llm_reranker.rerank_documents(
                    question, retrieved_docs, top_k=min(10, len(retrieved_docs))
                )
                
                # Calculate RAG metrics on reranked documents
                rag_metrics_after = self.rag_calculator.calculate_rag_metrics(
                    question, reranked_docs, generated_answer, reference_answer
                )
                results['rag_metrics_after'] = rag_metrics_after
                results['num_reranked_docs'] = len(reranked_docs)
            else:
                results['rag_metrics_after'] = rag_metrics_before
                results['num_reranked_docs'] = len(retrieved_docs)
            
            # Calculate BERTScore if reference answer available
            if reference_answer and BERTSCORE_AVAILABLE:
                bert_scores = self.bert_evaluator.calculate_similarity(
                    [generated_answer], [reference_answer]
                )
                results['bert_scores'] = bert_scores
            
            if self.debug:
                print(f"✅ Evaluation completed for question: {question[:50]}...")
                
        except Exception as e:
            print(f"❌ Evaluation failed for question: {e}")
            
            # Fallback values
            fallback_metrics = {
                'faithfulness': 0.0,
                'answer_relevancy': 0.0,
                'answer_correctness': 0.0,
                'answer_similarity': 0.0
            }
            results['rag_metrics_before'] = fallback_metrics
            results['rag_metrics_after'] = fallback_metrics
        
        return results
    
    def evaluate_batch(self, questions_data: List[Dict], use_llm_reranking: bool = True) -> Dict[str, Any]:
        """
        Evaluate a batch of questions
        
        Args:
            questions_data: List of question dictionaries with required fields
            use_llm_reranking: Whether to use LLM reranking
            
        Returns:
            Batch evaluation results
        """
        results = {
            'timestamp': datetime.now(pytz.timezone('America/Santiago')).isoformat(),
            'num_questions': len(questions_data),
            'use_llm_reranking': use_llm_reranking,
            'individual_results': [],
            'summary_metrics': {}
        }
        
        if self.debug:
            print(f"🚀 Starting batch evaluation of {len(questions_data)} questions")
        
        # Process each question
        for i, question_data in enumerate(questions_data):
            if self.debug:
                print(f"📝 Processing question {i+1}/{len(questions_data)}")
            
            try:
                result = self.evaluate_single_question(
                    question=question_data['question'],
                    retrieved_docs=question_data.get('retrieved_docs', []),
                    generated_answer=question_data.get('generated_answer', ''),
                    reference_answer=question_data.get('reference_answer'),
                    use_llm_reranking=use_llm_reranking
                )
                results['individual_results'].append(result)
                
            except Exception as e:
                print(f"❌ Failed to process question {i+1}: {e}")
                continue
        
        # Calculate summary metrics
        if results['individual_results']:
            results['summary_metrics'] = self._calculate_summary_metrics(
                results['individual_results']
            )
        
        # Cleanup
        gc.collect()
        
        if self.debug:
            print(f"✅ Batch evaluation completed: {len(results['individual_results'])} questions processed")
        
        return results
    
    def _calculate_summary_metrics(self, individual_results: List[Dict]) -> Dict[str, Any]:
        """Calculate summary metrics from individual results"""
        
        # Collect all RAG metrics
        before_metrics = []
        after_metrics = []
        
        for result in individual_results:
            if 'rag_metrics_before' in result:
                before_metrics.append(result['rag_metrics_before'])
            if 'rag_metrics_after' in result:
                after_metrics.append(result['rag_metrics_after'])
        
        summary = {}
        
        # Calculate averages for before metrics
        if before_metrics:
            summary['avg_before_rag_metrics'] = self._average_metrics(before_metrics)
        
        # Calculate averages for after metrics
        if after_metrics:
            summary['avg_after_rag_metrics'] = self._average_metrics(after_metrics)
        
        # Calculate improvements
        if before_metrics and after_metrics:
            summary['improvements'] = self._calculate_improvements(
                summary['avg_before_rag_metrics'],
                summary['avg_after_rag_metrics']
            )
        
        return summary
    
    def _average_metrics(self, metrics_list: List[Dict]) -> Dict[str, float]:
        """Calculate average metrics from a list of metric dictionaries"""
        if not metrics_list:
            return {}
        
        # Get all metric names
        all_keys = set()
        for metrics in metrics_list:
            all_keys.update(metrics.keys())
        
        # Calculate averages
        averages = {}
        for key in all_keys:
            values = [m.get(key, 0.0) for m in metrics_list if key in m]
            averages[key] = np.mean(values) if values else 0.0
        
        return averages
    
    def _calculate_improvements(self, before_metrics: Dict, after_metrics: Dict) -> Dict[str, float]:
        """Calculate percentage improvements between before and after metrics"""
        improvements = {}
        
        for metric_name in before_metrics:
            if metric_name in after_metrics:
                before_val = before_metrics[metric_name]
                after_val = after_metrics[metric_name]
                
                if before_val > 0:
                    improvement = ((after_val - before_val) / before_val) * 100
                else:
                    improvement = 0.0
                
                improvements[f"{metric_name}_improvement"] = improvement
        
        return improvements


# Factory functions for easy instantiation
def create_rag_calculator(model_name: str = "gpt-3.5-turbo", debug: bool = False) -> RAGCalculator:
    """Create and return a RAGCalculator instance"""
    return RAGCalculator(model_name, debug)

def create_llm_reranker(model_name: str = "gpt-3.5-turbo", debug: bool = False, max_content_length: int = 4000) -> LLMReranker:
    """Create and return a LLMReranker instance with enhanced content limits"""
    return LLMReranker(model_name, debug, max_content_length)

def create_bert_evaluator(model_type: str = "microsoft/deberta-xlarge-mnli", debug: bool = False) -> BERTScoreEvaluator:
    """Create and return a BERTScoreEvaluator instance"""
    return BERTScoreEvaluator(model_type, debug)

def create_rag_pipeline(llm_model: str = "gpt-3.5-turbo", debug: bool = False, max_content_length: int = 4000) -> RAGEvaluationPipeline:
    """Create and return a RAGEvaluationPipeline instance with enhanced content limits"""
    return RAGEvaluationPipeline(llm_model, debug, max_content_length)


if __name__ == "__main__":
    # Test the RAG evaluation components
    print("🧪 Testing RAG Evaluation Library...")
    
    # Sample data for testing
    question = "What is machine learning?"
    context_docs = [
        {"content": "Machine learning is a subset of artificial intelligence that focuses on algorithms."},
        {"content": "Deep learning uses neural networks with multiple layers."},
        {"content": "Supervised learning requires labeled training data."}
    ]
    generated_answer = "Machine learning is a branch of AI that uses algorithms to learn from data."
    reference_answer = "Machine learning is a subset of artificial intelligence focused on learning from data."
    
    # Test RAG calculator
    print("📊 Testing RAG Calculator...")
    rag_calc = create_rag_calculator(debug=True)
    rag_metrics = rag_calc.calculate_rag_metrics(
        question, context_docs, generated_answer, reference_answer
    )
    print(f"✅ RAG metrics: {rag_metrics}")
    
    # Test LLM reranker
    print("🤖 Testing LLM Reranker...")
    reranker = create_llm_reranker(debug=True)
    # Note: This will fail without OpenAI API key
    try:
        reranked = reranker.rerank_documents(question, context_docs, top_k=2)
        print(f"✅ Reranked {len(context_docs)} → {len(reranked)} documents")
    except Exception as e:
        print(f"⚠️ LLM reranking test skipped (API key needed): {e}")
    
    # Test BERTScore evaluator
    print("📏 Testing BERTScore Evaluator...")
    bert_eval = create_bert_evaluator(debug=True)
    bert_scores = bert_eval.calculate_similarity([generated_answer], [reference_answer])
    print(f"✅ BERTScore: {bert_scores}")
    
    print("🎉 RAG Evaluation Library test completed!")