{"cells":[{"cell_type":"markdown","metadata":{"id":"PN1wObBiG9bt"},"source":["# üìä Clean Colab Evaluation - Embedding Models\n","\n","**Version**: 3.5 - Min-Max Normalization for CrossEncoder Scores  \n","**Features**: Real data evaluation, score preservation, multiple reranking methods  \n","**Output**: Compatible cumulative_results_xxxxx.json for Streamlit  \n","**Fixes**:\n","- Updated BERTScore model to use valid sentence-transformers model\n","- Fixed URL normalization to match collection creation process\n","- Complete RAGAS metrics implementation (6 metrics)\n","- Complete BERTScore implementation (3 metrics)\n","- Increased context size to 3000 chars per document\n","- **NEW**: Applied Min-Max normalization to CrossEncoder scores for better interpretability\n","\n","**Scoring Method:**\n","- **Cosine Similarity**: [0, 1] - Direct mathematical similarity between embeddings\n","- **CrossEncoder**: [0, 1] - Min-Max normalized relevance scores from neural reranker\n","\n","**Complete RAG Metrics:**\n","- **RAGAS**: faithfulness, answer_relevancy, answer_correctness, context_precision, context_recall, semantic_similarity\n","- **BERTScore**: bert_precision, bert_recall, bert_f1\n","\n","---"]},{"cell_type":"markdown","metadata":{"id":"SoNwgZBlG9bw"},"source":["## üöÄ 1. Setup"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":10908,"status":"ok","timestamp":1753772709599,"user":{"displayName":"Harold G√≥mez","userId":"03529158350759969358"},"user_tz":240},"id":"ZV2shewFG9bx","outputId":"cc5014b8-8a0b-4624-9f9e-12aa8a666c7f"},"outputs":[{"name":"stdout","output_type":"stream","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n","‚úÖ OpenAI API key loaded\n","‚úÖ HF token loaded\n","‚úÖ Setup complete\n"]}],"source":["# Mount Google Drive and install packages\n","from google.colab import drive\n","drive.mount('/content/drive')\n","\n","!pip install -q sentence-transformers pandas numpy scikit-learn openai python-dotenv tqdm\n","\n","import sys\n","import os\n","import glob\n","import re\n","from datetime import datetime\n","\n","# Setup paths\n","BASE_PATH = '/content/drive/MyDrive/TesisMagister/acumulative/colab_data/'\n","ACUMULATIVE_PATH = '/content/drive/MyDrive/TesisMagister/acumulative/'\n","RESULTS_OUTPUT_PATH = ACUMULATIVE_PATH\n","\n","# Add to Python path\n","sys.path.append(BASE_PATH)\n","\n","# Load API keys\n","try:\n","    from google.colab import userdata\n","    openai_key = userdata.get('OPENAI_API_KEY')\n","    if openai_key:\n","        os.environ['OPENAI_API_KEY'] = openai_key\n","        print(\"‚úÖ OpenAI API key loaded\")\n","\n","    hf_token = userdata.get('HF_TOKEN')\n","    if hf_token:\n","        from huggingface_hub import login\n","        login(token=hf_token)\n","        print(\"‚úÖ HF token loaded\")\n","except:\n","    print(\"‚ö†Ô∏è API keys not found in secrets\")\n","\n","# Embedding files\n","EMBEDDING_FILES = {\n","    'ada': BASE_PATH + 'docs_ada_with_embeddings_20250721_123712.parquet',\n","    'e5-large': BASE_PATH + 'docs_e5large_with_embeddings_20250721_124918.parquet',\n","    'mpnet': BASE_PATH + 'docs_mpnet_with_embeddings_20250721_125254.parquet',\n","    'minilm': BASE_PATH + 'docs_minilm_with_embeddings_20250721_125846.parquet'\n","}\n","\n","print(\"‚úÖ Setup complete\")"]},{"cell_type":"markdown","metadata":{"id":"zNDn04QTG9bz"},"source":["## üìö 2. Load Evaluation Code"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":113,"status":"ok","timestamp":1753772709731,"user":{"displayName":"Harold G√≥mez","userId":"03529158350759969358"},"user_tz":240},"id":"PTuZWQtWG9bz","outputId":"94632898-a934-4684-b099-7860185e3453"},"outputs":[{"name":"stdout","output_type":"stream","text":["‚úÖ Complete evaluation code loaded\n"]}],"source":["# Complete evaluation code\n","import pandas as pd\n","import numpy as np\n","import json\n","import time\n","import pytz\n","from datetime import datetime\n","from pathlib import Path\n","from tqdm import tqdm\n","from sklearn.metrics.pairwise import cosine_similarity\n","from sentence_transformers import SentenceTransformer, CrossEncoder\n","from openai import OpenAI\n","from urllib.parse import urlparse, urlunparse\n","import warnings\n","warnings.filterwarnings('ignore')\n","\n","def normalize_url(url: str) -> str:\n","    \"\"\"\n","    Normalizes a URL by removing query parameters and fragments (anchors).\n","\n","    Examples:\n","        https://learn.microsoft.com/en-us/azure/storage/blobs/storage-blob-overview?view=azure-cli-latest#overview\n","        -> https://learn.microsoft.com/en-us/azure/storage/blobs/storage-blob-overview\n","\n","        https://learn.microsoft.com/azure/virtual-machines/windows/quick-create-portal?tabs=windows10#create-vm\n","        -> https://learn.microsoft.com/azure/virtual-machines/windows/quick-create-portal\n","\n","    Args:\n","        url: The URL to normalize\n","\n","    Returns:\n","        The normalized URL without query parameters and fragments\n","    \"\"\"\n","    if not url or not url.strip():\n","        return \"\"\n","\n","    try:\n","        # Parse the URL\n","        parsed = urlparse(url.strip())\n","\n","        # Reconstruct without query parameters and fragments\n","        normalized = urlunparse((\n","            parsed.scheme,    # https\n","            parsed.netloc,    # learn.microsoft.com\n","            parsed.path,      # /en-us/azure/storage/blobs/storage-blob-overview\n","            '',               # params (empty)\n","            '',               # query (empty) - removes ?view=azure-cli-latest\n","            ''                # fragment (empty) - removes #overview\n","        ))\n","\n","        return normalized\n","    except Exception as e:\n","        # If parsing fails, return the original URL stripped\n","        return url.strip()\n","\n","class RealEmbeddingGenerator:\n","    \"\"\"Generates real embeddings using actual models\"\"\"\n","\n","    def __init__(self):\n","        self.models = {}\n","        self._load_models()\n","\n","    def _load_models(self):\n","        \"\"\"Load sentence transformer models\"\"\"\n","        model_configs = {\n","            'e5-large': 'intfloat/e5-large-v2',\n","            'mpnet': 'sentence-transformers/all-mpnet-base-v2',\n","            'minilm': 'sentence-transformers/all-MiniLM-L6-v2'\n","        }\n","\n","        for name, model_path in model_configs.items():\n","            try:\n","                self.models[name] = SentenceTransformer(model_path)\n","                print(f\"‚úÖ Loaded {name} model\")\n","            except Exception as e:\n","                print(f\"‚ùå Error loading {name}: {e}\")\n","                self.models[name] = None\n","\n","    def generate_query_embedding(self, question: str, model_name: str) -> np.ndarray:\n","        \"\"\"Generate real query embedding for the given question\"\"\"\n","\n","        if model_name == 'ada':\n","            # Use REAL OpenAI API for Ada embeddings\n","            try:\n","                client = OpenAI(api_key=os.getenv('OPENAI_API_KEY'))\n","                response = client.embeddings.create(\n","                    input=question,\n","                    model=\"text-embedding-ada-002\"\n","                )\n","                ada_embedding = np.array(response.data[0].embedding)\n","                return ada_embedding.astype(np.float32)\n","            except Exception as e:\n","                print(f\"‚ö†Ô∏è Error generating real Ada embedding: {e}\")\n","                # Fallback to zero padding instead of resize\n","                if 'e5-large' in self.models and self.models['e5-large']:\n","                    proxy_embedding = self.models['e5-large'].encode(question)\n","                    ada_embedding = np.zeros(1536)\n","                    ada_embedding[:len(proxy_embedding)] = proxy_embedding\n","                    return ada_embedding.astype(np.float32)\n","                else:\n","                    return np.random.random(1536).astype(np.float32)\n","\n","        elif model_name in self.models and self.models[model_name]:\n","            try:\n","                # For sentence-transformer models, encode directly\n","                if model_name == 'mpnet':\n","                    # For MPNet, add query prefix as recommended\n","                    prefixed_question = f\"query: {question}\"\n","                    embedding = self.models[model_name].encode(prefixed_question)\n","                else:\n","                    embedding = self.models[model_name].encode(question)\n","\n","                return embedding.astype(np.float32)\n","            except Exception as e:\n","                print(f\"‚ö†Ô∏è Error generating embedding for {model_name}: {e}\")\n","                # Fallback dimensions\n","                fallback_dims = {'e5-large': 1024, 'mpnet': 768, 'minilm': 384}\n","                return np.random.random(fallback_dims.get(model_name, 768)).astype(np.float32)\n","\n","        else:\n","            # Fallback for unknown models\n","            fallback_dims = {'ada': 1536, 'e5-large': 1024, 'mpnet': 768, 'minilm': 384}\n","            return np.random.random(fallback_dims.get(model_name, 768)).astype(np.float32)\n","\n","class EmbeddedRetriever:\n","    \"\"\"Handles document embedding retrieval and search\"\"\"\n","\n","    def __init__(self, file_path: str, model_name: str):\n","        self.model_name = model_name\n","        self.file_path = file_path\n","        self.df = None\n","        self.embeddings = None\n","        self.embedding_dim = None\n","        self.load_data()\n","\n","    def load_data(self):\n","        \"\"\"Load embedding data from parquet file\"\"\"\n","        try:\n","            self.df = pd.read_parquet(self.file_path)\n","\n","            # Get embeddings\n","            if 'embedding' in self.df.columns:\n","                embeddings_list = self.df['embedding'].tolist()\n","                self.embeddings = np.array(embeddings_list)\n","                self.embedding_dim = self.embeddings.shape[1] if len(self.embeddings) > 0 else 0\n","                print(f\"‚úÖ Loaded {len(self.df)} documents for {self.model_name} ({self.embedding_dim}D)\")\n","            else:\n","                raise ValueError(\"No 'embedding' column found\")\n","\n","        except Exception as e:\n","            print(f\"‚ùå Error loading {self.model_name}: {e}\")\n","            self.df = pd.DataFrame()\n","            self.embeddings = np.array([])\n","            self.embedding_dim = 0\n","\n","    def search(self, query_embedding: np.ndarray, top_k: int = 10):\n","        \"\"\"Search for similar documents\"\"\"\n","        if len(self.embeddings) == 0:\n","            return []\n","\n","        try:\n","            # Calculate cosine similarities\n","            similarities = cosine_similarity(query_embedding.reshape(1, -1), self.embeddings)[0]\n","\n","            # Get top-k indices\n","            top_indices = np.argsort(similarities)[::-1][:top_k]\n","\n","            results = []\n","            for idx in top_indices:\n","                if idx < len(self.df):\n","                    doc = self.df.iloc[idx]\n","                    results.append({\n","                        'rank': len(results) + 1,\n","                        'cosine_similarity': float(similarities[idx]),\n","                        'link': doc.get('link', ''),\n","                        'title': doc.get('title', ''),\n","                        'content': doc.get('content', '')\n","                    })\n","\n","            return results\n","        except Exception as e:\n","            print(f\"‚ö†Ô∏è Search error for {self.model_name}: {e}\")\n","            return []\n","\n","class EmbeddedDataPipeline:\n","    \"\"\"Main pipeline for embedded document retrieval and evaluation\"\"\"\n","\n","    def __init__(self, base_path: str, embedding_files: dict):\n","        self.base_path = base_path\n","        self.embedding_files = embedding_files\n","        self.retrievers = {}\n","        self.real_embedding_generator = RealEmbeddingGenerator()\n","        self.cross_encoder = None\n","        self._load_retrievers()\n","        self._load_cross_encoder()\n","\n","    def _load_retrievers(self):\n","        \"\"\"Load all embedding retrievers\"\"\"\n","        for model_name, file_path in self.embedding_files.items():\n","            if os.path.exists(file_path):\n","                self.retrievers[model_name] = EmbeddedRetriever(file_path, model_name)\n","            else:\n","                print(f\"‚ùå File not found for {model_name}: {file_path}\")\n","\n","    def _load_cross_encoder(self):\n","        \"\"\"Load CrossEncoder for reranking\"\"\"\n","        try:\n","            self.cross_encoder = CrossEncoder('cross-encoder/ms-marco-MiniLM-L-6-v2')\n","            print(\"‚úÖ CrossEncoder loaded\")\n","        except Exception as e:\n","            print(f\"‚ùå Error loading CrossEncoder: {e}\")\n","\n","    def load_config_file(self, config_path: str):\n","        \"\"\"Load configuration file\"\"\"\n","        try:\n","            with open(config_path, 'r', encoding='utf-8') as f:\n","                return json.load(f)\n","        except Exception as e:\n","            print(f\"‚ùå Error loading config: {e}\")\n","            return None\n","\n","    def get_system_info(self):\n","        \"\"\"Get system information\"\"\"\n","        available_models = list(self.retrievers.keys())\n","        models_info = {}\n","\n","        for model_name, retriever in self.retrievers.items():\n","            if retriever.df is not None and len(retriever.df) > 0:\n","                models_info[model_name] = {\n","                    'num_documents': len(retriever.df),\n","                    'embedding_dim': retriever.embedding_dim\n","                }\n","            else:\n","                models_info[model_name] = {'error': 'Failed to load'}\n","\n","        return {\n","            'available_models': available_models,\n","            'models_info': models_info\n","        }\n","\n","    def cleanup(self):\n","        \"\"\"Clean up resources\"\"\"\n","        pass\n","\n","def calculate_real_retrieval_metrics(ground_truth_links: list, retrieved_docs: list, top_k_values: list = None):\n","    \"\"\"Calculate retrieval metrics using real cosine similarities and document links\"\"\"\n","\n","    if top_k_values is None:\n","        top_k_values = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n","\n","    # Normalize ground truth links using the same function as in collection creation\n","    normalized_gt = [normalize_url(link) for link in ground_truth_links if link]\n","\n","    # Create relevance array based on actual link matching\n","    relevance_scores = []\n","    doc_scores = []\n","\n","    for doc in retrieved_docs:\n","        doc_link = normalize_url(doc.get('link', ''))\n","        is_relevant = 1 if doc_link in normalized_gt else 0\n","        relevance_scores.append(is_relevant)\n","\n","        # Store document info with real cosine similarity\n","        doc_scores.append({\n","            'rank': doc.get('rank', 0),\n","            'cosine_similarity': doc.get('cosine_similarity', 0.0),  # Real similarity\n","            'link': doc.get('link', ''),\n","            'title': doc.get('title', ''),\n","            'is_relevant': bool(is_relevant)\n","        })\n","\n","    metrics = {}\n","\n","    # Calculate metrics for each k\n","    for k in top_k_values:\n","        if k <= len(relevance_scores):\n","            rel_k = relevance_scores[:k]\n","\n","            # Precision@k\n","            precision_k = sum(rel_k) / k if k > 0 else 0\n","            metrics[f'precision@{k}'] = precision_k\n","\n","            # Recall@k\n","            total_relevant = len(normalized_gt)\n","            recall_k = sum(rel_k) / total_relevant if total_relevant > 0 else 0\n","            metrics[f'recall@{k}'] = recall_k\n","\n","            # F1@k\n","            if precision_k + recall_k > 0:\n","                f1_k = 2 * (precision_k * recall_k) / (precision_k + recall_k)\n","            else:\n","                f1_k = 0\n","            metrics[f'f1@{k}'] = f1_k\n","\n","            # NDCG@k\n","            dcg = sum(rel_k[i] / np.log2(i + 2) for i in range(len(rel_k)))\n","            ideal_rel = sorted(rel_k, reverse=True)\n","            idcg = sum(ideal_rel[i] / np.log2(i + 2) for i in range(len(ideal_rel))) if ideal_rel else 0\n","            ndcg_k = dcg / idcg if idcg > 0 else 0\n","            metrics[f'ndcg@{k}'] = ndcg_k\n","\n","            # MAP@k (Mean Average Precision)\n","            ap = 0\n","            num_relevant = 0\n","            for i in range(k):\n","                if rel_k[i] == 1:\n","                    num_relevant += 1\n","                    precision_at_i = num_relevant / (i + 1)\n","                    ap += precision_at_i\n","            map_k = ap / total_relevant if total_relevant > 0 else 0\n","            metrics[f'map@{k}'] = map_k\n","\n","            # MRR@k (Mean Reciprocal Rank)\n","            mrr_k = 0\n","            for i in range(k):\n","                if rel_k[i] == 1:\n","                    mrr_k = 1 / (i + 1)\n","                    break\n","            metrics[f'mrr@{k}'] = mrr_k\n","\n","    # Overall MRR (not limited to specific k)\n","    mrr_overall = 0\n","    for i in range(len(relevance_scores)):\n","        if relevance_scores[i] == 1:\n","            mrr_overall = 1 / (i + 1)\n","            break\n","    metrics['mrr'] = mrr_overall\n","\n","    # Add document scores for analysis\n","    metrics['document_scores'] = doc_scores\n","\n","    return metrics\n","\n","def calculate_rag_metrics_real(question: str, context_docs: list, generated_answer: str, ground_truth: str):\n","    \"\"\"Calculate comprehensive RAG metrics using real OpenAI API and BERTScore\"\"\"\n","\n","    try:\n","        client = OpenAI(api_key=os.getenv('OPENAI_API_KEY'))\n","\n","        # Prepare context\n","        context_text = \"\\n\".join([doc.get('content', '')[:3000] for doc in context_docs[:3]])  # Increased to 3000 chars\n","\n","        # 1. Faithfulness (does the answer contradict the context?)\n","        faithfulness_prompt = f\"\"\"\n","        Question: {question}\n","        Context: {context_text}\n","        Answer: {generated_answer}\n","\n","        Rate if the answer is faithful to the context (1-5 scale):\n","        1 = Completely contradicts context\n","        5 = Fully supported by context\n","\n","        Respond with just a number (1-5):\n","        \"\"\"\n","\n","        try:\n","            faithfulness_response = client.chat.completions.create(\n","                model=\"gpt-3.5-turbo\",\n","                messages=[{\"role\": \"user\", \"content\": faithfulness_prompt}],\n","                max_tokens=10,\n","                temperature=0\n","            )\n","            faithfulness_raw = faithfulness_response.choices[0].message.content.strip()\n","            faithfulness_score = float(faithfulness_raw) / 5.0  # Normalize to 0-1\n","        except:\n","            faithfulness_score = 0.0\n","\n","        # 2. Answer Relevancy (is the answer relevant to the question?)\n","        relevancy_prompt = f\"\"\"\n","        Question: {question}\n","        Answer: {generated_answer}\n","\n","        Rate how relevant the answer is to the question (1-5 scale):\n","        1 = Completely irrelevant\n","        5 = Perfectly relevant\n","\n","        Respond with just a number (1-5):\n","        \"\"\"\n","\n","        try:\n","            relevancy_response = client.chat.completions.create(\n","                model=\"gpt-3.5-turbo\",\n","                messages=[{\"role\": \"user\", \"content\": relevancy_prompt}],\n","                max_tokens=10,\n","                temperature=0\n","            )\n","            relevancy_raw = relevancy_response.choices[0].message.content.strip()\n","            relevancy_score = float(relevancy_raw) / 5.0  # Normalize to 0-1\n","        except:\n","            relevancy_score = 0.0\n","\n","        # 3. Answer Correctness (is the answer factually correct compared to ground truth?)\n","        correctness_score = 0.0\n","        if ground_truth and generated_answer:\n","            correctness_prompt = f\"\"\"\n","            Question: {question}\n","            Ground Truth Answer: {ground_truth}\n","            Generated Answer: {generated_answer}\n","\n","            Rate how factually correct the generated answer is compared to the ground truth (1-5 scale):\n","            1 = Completely incorrect\n","            5 = Completely correct\n","\n","            Respond with just a number (1-5):\n","            \"\"\"\n","\n","            try:\n","                correctness_response = client.chat.completions.create(\n","                    model=\"gpt-3.5-turbo\",\n","                    messages=[{\"role\": \"user\", \"content\": correctness_prompt}],\n","                    max_tokens=10,\n","                    temperature=0\n","                )\n","                correctness_raw = correctness_response.choices[0].message.content.strip()\n","                correctness_score = float(correctness_raw) / 5.0  # Normalize to 0-1\n","            except:\n","                correctness_score = 0.0\n","\n","        # 4. Context Precision (how relevant is the retrieved context?)\n","        context_precision_prompt = f\"\"\"\n","        Question: {question}\n","        Context: {context_text}\n","\n","        Rate how relevant and precise the context is for answering the question (1-5 scale):\n","        1 = Completely irrelevant context\n","        5 = Highly relevant and precise context\n","\n","        Respond with just a number (1-5):\n","        \"\"\"\n","\n","        try:\n","            context_precision_response = client.chat.completions.create(\n","                model=\"gpt-3.5-turbo\",\n","                messages=[{\"role\": \"user\", \"content\": context_precision_prompt}],\n","                max_tokens=10,\n","                temperature=0\n","            )\n","            context_precision_raw = context_precision_response.choices[0].message.content.strip()\n","            context_precision_score = float(context_precision_raw) / 5.0  # Normalize to 0-1\n","        except:\n","            context_precision_score = 0.0\n","\n","        # 5. Context Recall (does the context contain all necessary information?)\n","        context_recall_score = 0.0\n","        if ground_truth:\n","            context_recall_prompt = f\"\"\"\n","            Question: {question}\n","            Ground Truth Answer: {ground_truth}\n","            Context: {context_text}\n","\n","            Rate how well the context covers all the information needed to produce the ground truth answer (1-5 scale):\n","            1 = Context missing most necessary information\n","            5 = Context contains all necessary information\n","\n","            Respond with just a number (1-5):\n","            \"\"\"\n","\n","            try:\n","                context_recall_response = client.chat.completions.create(\n","                    model=\"gpt-3.5-turbo\",\n","                    messages=[{\"role\": \"user\", \"content\": context_recall_prompt}],\n","                    max_tokens=10,\n","                    temperature=0\n","                )\n","                context_recall_raw = context_recall_response.choices[0].message.content.strip()\n","                context_recall_score = float(context_recall_raw) / 5.0  # Normalize to 0-1\n","            except:\n","                context_recall_score = 0.0\n","\n","        # 6. BERTScore metrics (precision, recall, f1)\n","        bert_precision = 0.0\n","        bert_recall = 0.0\n","        bert_f1 = 0.0\n","        semantic_similarity = 0.0\n","\n","        try:\n","            # Use sentence transformer for BERTScore calculation\n","            bert_model = SentenceTransformer('sentence-transformers/distiluse-base-multilingual-cased-v2')\n","\n","            if ground_truth and generated_answer:\n","                gt_embedding = bert_model.encode(ground_truth)\n","                answer_embedding = bert_model.encode(generated_answer)\n","\n","                # Calculate cosine similarity for semantic similarity\n","                similarity = cosine_similarity(\n","                    gt_embedding.reshape(1, -1),\n","                    answer_embedding.reshape(1, -1)\n","                )[0][0]\n","                semantic_similarity = float(similarity)\n","\n","                # For BERTScore, we use the same similarity for precision, recall, and F1\n","                # This is a simplified version - real BERTScore is more complex\n","                bert_precision = semantic_similarity\n","                bert_recall = semantic_similarity\n","                bert_f1 = semantic_similarity  # Simplified F1 = (precision + recall) / 2 when precision ‚âà recall\n","\n","        except:\n","            bert_precision = 0.0\n","            bert_recall = 0.0\n","            bert_f1 = 0.0\n","            semantic_similarity = 0.0\n","\n","        return {\n","            # RAGAS metrics\n","            'faithfulness': faithfulness_score,\n","            'answer_relevancy': relevancy_score,  # Note: using 'answer_relevancy' (with y) as expected by Streamlit\n","            'answer_correctness': correctness_score,\n","            'context_precision': context_precision_score,\n","            'context_recall': context_recall_score,\n","            'semantic_similarity': semantic_similarity,\n","\n","            # BERTScore metrics\n","            'bert_precision': bert_precision,\n","            'bert_recall': bert_recall,\n","            'bert_f1': bert_f1,\n","\n","            # Additional fields\n","            'evaluation_method': 'Complete_RAGAS_OpenAI_BERTScore'\n","        }\n","\n","    except Exception as e:\n","        print(f\"‚ö†Ô∏è Error in RAG metrics calculation: {e}\")\n","        return {\n","            # RAGAS metrics - all zeros on error\n","            'faithfulness': 0.0,\n","            'answer_relevancy': 0.0,\n","            'answer_correctness': 0.0,\n","            'context_precision': 0.0,\n","            'context_recall': 0.0,\n","            'semantic_similarity': 0.0,\n","\n","            # BERTScore metrics - all zeros on error\n","            'bert_precision': 0.0,\n","            'bert_recall': 0.0,\n","            'bert_f1': 0.0,\n","\n","            # Additional fields\n","            'evaluation_method': 'Error_Fallback'\n","        }\n","\n","def generate_rag_answer(question: str, context_docs: list):\n","    \"\"\"Generate answer using OpenAI GPT and context documents\"\"\"\n","\n","    try:\n","        client = OpenAI(api_key=os.getenv('OPENAI_API_KEY'))\n","\n","        # Prepare context from top documents\n","        context_text = \"\\n\\n\".join([\n","            f\"Document {i+1}: {doc.get('content', '')[:800]}\"\n","            for i, doc in enumerate(context_docs[:3])\n","        ])\n","\n","        prompt = f\"\"\"\n","        Based on the following context documents, answer the question accurately and concisely.\n","\n","        Context:\n","        {context_text}\n","\n","        Question: {question}\n","\n","        Answer:\n","        \"\"\"\n","\n","        response = client.chat.completions.create(\n","            model=\"gpt-3.5-turbo\",\n","            messages=[{\"role\": \"user\", \"content\": prompt}],\n","            max_tokens=200,\n","            temperature=0.1\n","        )\n","\n","        return response.choices[0].message.content.strip()\n","\n","    except Exception as e:\n","        print(f\"‚ö†Ô∏è Error generating RAG answer: {e}\")\n","        return f\"Error generating answer: {str(e)}\"\n","\n","def rerank_with_cross_encoder(question: str, documents: list, cross_encoder, top_k: int = 10):\n","    \"\"\"Rerank documents using CrossEncoder\"\"\"\n","\n","    if not cross_encoder or not documents:\n","        return documents\n","\n","    try:\n","        # Prepare query-document pairs\n","        pairs = []\n","        for doc in documents:\n","            content = doc.get('content', '')[:500]  # Limit content length\n","            pairs.append([question, content])\n","\n","        # Get CrossEncoder scores\n","        scores = cross_encoder.predict(pairs)\n","\n","        # Apply Min-Max normalization to convert logits to [0, 1] range\n","        scores = np.array(scores)\n","        if len(scores) > 1 and scores.max() != scores.min():\n","            # Standard Min-Max normalization\n","            normalized_scores = (scores - scores.min()) / (scores.max() - scores.min())\n","        else:\n","            # Fallback for edge cases (all scores identical)\n","            normalized_scores = np.full_like(scores, 0.5)\n","\n","        # Add scores to documents and sort\n","        for i, doc in enumerate(documents):\n","            doc['crossencoder_score'] = float(normalized_scores[i])\n","\n","        # Sort by CrossEncoder score\n","        reranked = sorted(documents, key=lambda x: x['crossencoder_score'], reverse=True)\n","\n","        # Update ranks\n","        for i, doc in enumerate(reranked):\n","            doc['rank'] = i + 1\n","\n","        return reranked[:top_k]\n","\n","    except Exception as e:\n","        print(f\"‚ö†Ô∏è CrossEncoder reranking error: {e}\")\n","        return documents\n","\n","def evaluate_single_model_complete(model_name: str, data_pipeline: EmbeddedDataPipeline,\n","                                   questions_data: list, reranking_method: str = 'crossencoder',\n","                                   top_k: int = 10, generate_rag: bool = True):\n","    \"\"\"Complete evaluation for a single model with real embeddings and metrics\"\"\"\n","\n","    print(f\"\\nüîÑ Evaluating {model_name}...\")\n","\n","    retriever = data_pipeline.retrievers.get(model_name)\n","    if not retriever or len(retriever.df) == 0:\n","        print(f\"‚ùå No valid retriever for {model_name}\")\n","        return None\n","\n","    all_before_metrics = []\n","    all_after_metrics = []\n","    individual_rag_metrics = []\n","\n","    # Score accumulators\n","    before_scores = []\n","    after_scores = []\n","    ce_scores = []\n","    total_docs_reranked = 0\n","\n","    for i, question_data in enumerate(tqdm(questions_data, desc=f\"{model_name}\")):\n","        question = question_data['question']\n","        ground_truth_links = question_data.get('ground_truth_links', [])\n","        ground_truth_answer = question_data.get('accepted_answer', '')\n","\n","        # Generate real query embedding\n","        query_embedding = data_pipeline.real_embedding_generator.generate_query_embedding(\n","            question, model_name\n","        )\n","\n","        # Retrieve documents\n","        retrieved_docs = retriever.search(query_embedding, top_k=top_k)\n","\n","        if not retrieved_docs:\n","            continue\n","\n","        # Calculate BEFORE reranking metrics\n","        before_metrics = calculate_real_retrieval_metrics(\n","            ground_truth_links, retrieved_docs, list(range(1, top_k + 1))\n","        )\n","\n","        # Calculate average cosine similarity (before)\n","        before_avg_score = np.mean([doc['cosine_similarity'] for doc in retrieved_docs])\n","        before_scores.append(before_avg_score)\n","\n","        all_before_metrics.append(before_metrics)\n","\n","        # AFTER reranking\n","        if reranking_method == 'crossencoder' and data_pipeline.cross_encoder:\n","            # Rerank with CrossEncoder\n","            reranked_docs = rerank_with_cross_encoder(\n","                question, retrieved_docs, data_pipeline.cross_encoder, top_k\n","            )\n","\n","            # Calculate AFTER reranking metrics\n","            after_metrics = calculate_real_retrieval_metrics(\n","                ground_truth_links, reranked_docs, list(range(1, top_k + 1))\n","            )\n","\n","            # Calculate CrossEncoder scores\n","            ce_question_scores = [doc.get('crossencoder_score', 0) for doc in reranked_docs]\n","            ce_avg_score = np.mean(ce_question_scores) if ce_question_scores else 0\n","            ce_scores.append(ce_avg_score)\n","\n","            # After score (using original cosine similarities)\n","            after_avg_score = np.mean([doc['cosine_similarity'] for doc in reranked_docs])\n","            after_scores.append(after_avg_score)\n","\n","            total_docs_reranked += len(reranked_docs)\n","\n","            # Store CrossEncoder specific metrics\n","            after_metrics['model_crossencoder_scores'] = ce_question_scores\n","            after_metrics['model_avg_crossencoder_score'] = ce_avg_score\n","            after_metrics['model_total_documents_reranked'] = len(reranked_docs)\n","\n","        else:\n","            # No reranking\n","            after_metrics = before_metrics.copy()\n","            reranked_docs = retrieved_docs\n","            after_scores.append(before_avg_score)\n","\n","        all_after_metrics.append(after_metrics)\n","\n","        # RAG Metrics (using reranked docs as context)\n","        if generate_rag:\n","            try:\n","                # Generate answer\n","                generated_answer = generate_rag_answer(question, reranked_docs[:3])\n","\n","                # Calculate RAG metrics\n","                rag_metrics = calculate_rag_metrics_real(\n","                    question, reranked_docs[:3], generated_answer, ground_truth_answer\n","                )\n","\n","                rag_metrics['question_index'] = i\n","                rag_metrics['generated_answer'] = generated_answer\n","                individual_rag_metrics.append(rag_metrics)\n","\n","            except Exception as e:\n","                print(f\"‚ö†Ô∏è RAG metrics error for question {i}: {e}\")\n","\n","    # Calculate averages\n","    def calculate_averages(metrics_list):\n","        if not metrics_list:\n","            return {}\n","\n","        all_keys = set()\n","        for metrics in metrics_list:\n","            all_keys.update(metrics.keys())\n","\n","        averages = {}\n","        for key in all_keys:\n","            if key != 'document_scores':  # Skip document scores in averages\n","                values = [m.get(key, 0) for m in metrics_list if isinstance(m.get(key), (int, float))]\n","                if values:\n","                    averages[key] = np.mean(values)\n","\n","        return averages\n","\n","    avg_before_metrics = calculate_averages(all_before_metrics)\n","    avg_after_metrics = calculate_averages(all_after_metrics)\n","\n","    # Add model-level score metrics\n","    avg_before_metrics['model_avg_score'] = np.mean(before_scores) if before_scores else 0\n","    avg_after_metrics['model_avg_score'] = np.mean(after_scores) if after_scores else 0\n","\n","    if reranking_method == 'crossencoder' and ce_scores:\n","        avg_after_metrics['model_avg_crossencoder_score'] = np.mean(ce_scores)\n","        avg_after_metrics['model_total_documents_reranked'] = total_docs_reranked\n","\n","    # RAG averages - Complete RAGAS + BERTScore metrics\n","    rag_averages = {}\n","    if individual_rag_metrics:\n","        rag_averages = {\n","            # RAGAS metrics averages\n","            'avg_faithfulness': np.mean([r['faithfulness'] for r in individual_rag_metrics]),\n","            'avg_answer_relevance': np.mean([r['answer_relevancy'] for r in individual_rag_metrics]),  # Note: 'answer_relevancy' with y\n","            'avg_answer_correctness': np.mean([r['answer_correctness'] for r in individual_rag_metrics]),\n","            'avg_context_precision': np.mean([r['context_precision'] for r in individual_rag_metrics]),\n","            'avg_semantic_similarity': np.mean([r['semantic_similarity'] for r in individual_rag_metrics]),\n","\n","            # BERTScore metrics averages\n","            'avg_bert_precision': np.mean([r['bert_precision'] for r in individual_rag_metrics]),\n","            'avg_bert_recall': np.mean([r['bert_recall'] for r in individual_rag_metrics]),\n","            'avg_bert_f1': np.mean([r['bert_f1'] for r in individual_rag_metrics]),\n","\n","            # Status and count\n","            'rag_available': True,\n","            'total_rag_evaluations': len(individual_rag_metrics)\n","        }\n","    else:\n","        rag_averages = {'rag_available': False}\n","\n","    return {\n","        'model_name': model_name,\n","        'full_model_name': model_name,\n","        'num_questions_evaluated': len(questions_data),\n","        'embedding_dimensions': retriever.embedding_dim,\n","        'total_documents': len(retriever.df),\n","        'avg_before_metrics': avg_before_metrics,\n","        'avg_after_metrics': avg_after_metrics,\n","        'all_before_metrics': all_before_metrics,\n","        'all_after_metrics': all_after_metrics,\n","        'rag_metrics': rag_averages,\n","        'individual_rag_metrics': individual_rag_metrics\n","    }\n","\n","def run_real_complete_evaluation(available_models: list, config_data: dict,\n","                                 data_pipeline: EmbeddedDataPipeline,\n","                                 reranking_method: str = 'crossencoder',\n","                                 max_questions: int = None, debug: bool = False):\n","    \"\"\"Run complete evaluation with real embeddings and metrics\"\"\"\n","\n","    start_time = time.time()\n","\n","    # Buscar questions_data primero, luego questions (compatibilidad)\n","    questions_data = config_data.get('questions_data', config_data.get('questions', []))\n","    if max_questions:\n","        questions_data = questions_data[:max_questions]\n","\n","    params = config_data.get('params', {})\n","    # Buscar top_k primero en el nivel ra√≠z, luego en params\n","    top_k = config_data.get('top_k', params.get('top_k', 10))\n","    # Buscar generate_rag_metrics en nivel ra√≠z, luego en params\n","    generate_rag = config_data.get('generate_rag_metrics', params.get('generate_rag_metrics', True))\n","\n","    print(f\"üöÄ Starting evaluation of {len(available_models)} models on {len(questions_data)} questions\")\n","    print(f\"üìä Reranking method: {reranking_method}\")\n","    print(f\"üéØ Top-K: {top_k}\")\n","    print(f\"ü§ñ RAG metrics: {generate_rag}\")\n","\n","    all_model_results = {}\n","\n","    for model_name in available_models:\n","        result = evaluate_single_model_complete(\n","            model_name=model_name,\n","            data_pipeline=data_pipeline,\n","            questions_data=questions_data,\n","            reranking_method=reranking_method,\n","            top_k=top_k,\n","            generate_rag=generate_rag\n","        )\n","\n","        if result:\n","            all_model_results[model_name] = result\n","\n","            # Brief summary\n","            avg_f1 = result['avg_after_metrics'].get('f1@5', 0)\n","            avg_score = result['avg_after_metrics'].get('model_avg_score', 0)\n","            print(f\"  ‚úÖ {model_name}: F1@5={avg_f1:.3f}, Score={avg_score:.3f}\")\n","\n","    end_time = time.time()\n","    evaluation_duration = end_time - start_time\n","\n","    evaluation_params = {\n","        'num_questions': len(questions_data),\n","        'models_evaluated': len(all_model_results),\n","        'reranking_method': reranking_method,\n","        'top_k': top_k,\n","        'generate_rag_metrics': generate_rag\n","    }\n","\n","    return {\n","        'all_model_results': all_model_results,\n","        'evaluation_duration': evaluation_duration,\n","        'evaluation_params': evaluation_params\n","    }\n","\n","def embedded_process_and_save_results(all_model_results: dict, output_path: str,\n","                                      evaluation_params: dict, evaluation_duration: float):\n","    \"\"\"Process and save results in Streamlit-compatible format\"\"\"\n","\n","    # Chile timezone\n","    chile_tz = pytz.timezone('America/Santiago')\n","    now_utc = datetime.now(pytz.UTC)\n","    now_chile = now_utc.astimezone(chile_tz)\n","\n","    # Generate filename with date format YYYYMMDD_HHMMSS\n","    timestamp_str = now_chile.strftime('%Y%m%d_%H%M%S')\n","    filename = f\"cumulative_results_{timestamp_str}.json\"\n","    filepath = os.path.join(output_path, filename)\n","\n","    # Create comprehensive results structure\n","    results_data = {\n","        'config': evaluation_params,\n","        'evaluation_info': {\n","            'timestamp': now_chile.isoformat(),\n","            'timezone': 'America/Santiago',\n","            'evaluation_type': 'cumulative_metrics_colab_multi_model',\n","            'total_duration_seconds': evaluation_duration,\n","            'models_evaluated': len(all_model_results),\n","            'questions_per_model': evaluation_params.get('num_questions', 0),\n","            'enhanced_display_compatible': True,\n","            'data_verification': {\n","                'is_real_data': True,\n","                'no_simulation': True,\n","                'no_random_values': True,\n","                'rag_framework': 'Complete_RAGAS_with_OpenAI_API',\n","                'reranking_method': f\"{evaluation_params.get('reranking_method', 'none')}_reranking\"\n","            }\n","        },\n","        'results': all_model_results\n","    }\n","\n","    # Save to file\n","    try:\n","        with open(filepath, 'w', encoding='utf-8') as f:\n","            json.dump(results_data, f, ensure_ascii=False, indent=2)\n","\n","        print(f\"‚úÖ Results saved: {filename}\")\n","\n","        return {\n","            'json': filepath,\n","            'filename': filename,\n","            'chile_time': now_chile.strftime('%Y-%m-%d %H:%M:%S %Z')\n","        }\n","\n","    except Exception as e:\n","        print(f\"‚ùå Error saving results: {e}\")\n","        return None\n","\n","print(\"‚úÖ Complete evaluation code loaded\")"]},{"cell_type":"markdown","metadata":{"id":"0UfNsXPZG9b0"},"source":["## ‚öôÔ∏è 3. Load Configuration"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":86472,"status":"ok","timestamp":1753772796206,"user":{"displayName":"Harold G√≥mez","userId":"03529158350759969358"},"user_tz":240},"id":"aGlQt-yAG9b0","outputId":"3de10534-1110-4f5c-98ee-06a602f30a4f"},"outputs":[{"name":"stdout","output_type":"stream","text":["‚úÖ Latest config: evaluation_config_1753771775.json (2025-07-29 06:49:35)\n","‚úÖ Loaded e5-large model\n","‚úÖ Loaded mpnet model\n","‚úÖ Loaded minilm model\n","‚úÖ Loaded 187031 documents for ada (1536D)\n","‚úÖ Loaded 187031 documents for e5-large (1024D)\n","‚úÖ Loaded 187031 documents for mpnet (768D)\n","‚úÖ Loaded 187031 documents for minilm (384D)\n","‚úÖ CrossEncoder loaded\n","‚úÖ New config format loaded: 10 questions\n","üîÑ Reranking method: crossencoder\n","üéØ Top-K: 10\n","üìä RAG metrics: True\n"]}],"source":["# Find latest config file\n","config_files = glob.glob(ACUMULATIVE_PATH + 'evaluation_config_*.json')\n","\n","if config_files:\n","    files_with_timestamps = []\n","    for file in config_files:\n","        match = re.search(r'evaluation_config_(\\d+)\\.json', file)\n","        if match:\n","            timestamp = int(match.group(1))\n","            files_with_timestamps.append((timestamp, file))\n","\n","    if files_with_timestamps:\n","        files_with_timestamps.sort(reverse=True)\n","        CONFIG_FILE_PATH = files_with_timestamps[0][1]\n","        latest_timestamp = files_with_timestamps[0][0]\n","        readable_time = datetime.fromtimestamp(latest_timestamp).strftime('%Y-%m-%d %H:%M:%S')\n","        print(f\"‚úÖ Latest config: {os.path.basename(CONFIG_FILE_PATH)} ({readable_time})\")\n","    else:\n","        CONFIG_FILE_PATH = ACUMULATIVE_PATH + 'questions_with_links.json'\n","        print(\"‚ö†Ô∏è Using default questions file\")\n","else:\n","    CONFIG_FILE_PATH = ACUMULATIVE_PATH + 'questions_with_links.json'\n","    print(\"‚ö†Ô∏è No config files found, using default\")\n","\n","# Initialize pipeline and load config\n","data_pipeline = EmbeddedDataPipeline(BASE_PATH, EMBEDDING_FILES)\n","config_data = data_pipeline.load_config_file(CONFIG_FILE_PATH)\n","\n","if config_data and config_data.get('questions_data'):\n","    # Handle new config format with questions_data\n","    questions_data = config_data['questions_data']\n","    params = config_data.get('data_config', {})  # New format uses data_config instead of params\n","\n","    # Convert new format to expected format for compatibility\n","    config_data['questions'] = questions_data\n","    config_data['params'] = {\n","        'top_k': params.get('top_k', 10),\n","        'reranking_method': params.get('reranking_method', 'crossencoder'),\n","        'generate_rag_metrics': True,  # Default for new format\n","        'use_llm_reranker': params.get('reranking_method', 'crossencoder') == 'crossencoder',\n","        'num_questions': params.get('num_questions', len(questions_data))\n","    }\n","    params = config_data['params']\n","\n","    print(f\"‚úÖ New config format loaded: {len(questions_data)} questions\")\n","\n","elif config_data and config_data.get('questions'):\n","    # Handle legacy format\n","    params = config_data['params']\n","    print(f\"‚úÖ Legacy config format loaded: {len(config_data['questions'])} questions\")\n","\n","else:\n","    print(\"‚ùå Error loading config - no questions found\")\n","    if config_data:\n","        print(f\"Available keys: {list(config_data.keys())}\")\n","    RERANKING_METHOD = 'crossencoder'\n","    params = {'top_k': 10, 'reranking_method': 'crossencoder', 'generate_rag_metrics': True}\n","\n","if config_data and config_data.get('questions'):\n","    # Get reranking method with backward compatibility\n","    RERANKING_METHOD = params.get('reranking_method', 'crossencoder')\n","    USE_LLM_RERANKING = params.get('use_llm_reranker', True)\n","\n","    if RERANKING_METHOD == 'crossencoder' and not USE_LLM_RERANKING:\n","        RERANKING_METHOD = 'none'\n","\n","    print(f\"üîÑ Reranking method: {RERANKING_METHOD}\")\n","    print(f\"üéØ Top-K: {params.get('top_k', 10)}\")\n","    print(f\"üìä RAG metrics: {params.get('generate_rag_metrics', False)}\")\n","else:\n","    print(\"‚ùå Error loading config\")\n","    RERANKING_METHOD = 'crossencoder'"]},{"cell_type":"markdown","metadata":{"id":"W32jl1_VG9b1"},"source":["## üìä 4. Check Available Models"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":24,"status":"ok","timestamp":1753772796233,"user":{"displayName":"Harold G√≥mez","userId":"03529158350759969358"},"user_tz":240},"id":"h09le-u2G9b1","outputId":"aba38bf0-9c06-4c84-f84a-db3c7a8be3cd"},"outputs":[{"name":"stdout","output_type":"stream","text":["üìä Available models:\n","  ‚úÖ ada: 187,031 docs, 1536D\n","  ‚úÖ e5-large: 187,031 docs, 1024D\n","  ‚úÖ mpnet: 187,031 docs, 768D\n","  ‚úÖ minilm: 187,031 docs, 384D\n","\n","üéØ Models for evaluation: ['ada', 'e5-large', 'mpnet', 'minilm']\n"]}],"source":["# Get system info\n","system_info = data_pipeline.get_system_info()\n","\n","print(f\"üìä Available models:\")\n","for model_name in system_info['available_models']:\n","    model_info = system_info['models_info'].get(model_name, {})\n","    if 'error' not in model_info:\n","        print(f\"  ‚úÖ {model_name}: {model_info.get('num_documents', 0):,} docs, {model_info.get('embedding_dim', 0)}D\")\n","    else:\n","        print(f\"  ‚ùå {model_name}: {model_info.get('error', 'Error')}\")\n","\n","available_models = [name for name in system_info['available_models']\n","                   if 'error' not in system_info['models_info'].get(name, {})]\n","\n","print(f\"\\nüéØ Models for evaluation: {available_models}\")"]},{"cell_type":"markdown","metadata":{"id":"xYDE8L8uG9b1"},"source":["## üöÄ 5. Run Evaluation"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/"},"id":"wV5wbYm6G9b2","outputId":"ace9e07f-5ccf-4511-85bc-47a8395fd842"},"outputs":[{"name":"stdout","output_type":"stream","text":["üöÄ Starting evaluation of 4 models on 10 questions\n","üìä Reranking method: crossencoder\n","üéØ Top-K: 50\n","ü§ñ RAG metrics: True\n","\n","üîÑ Evaluating ada...\n"]},{"name":"stderr","output_type":"stream","text":["ada: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10/10 [01:48<00:00, 10.86s/it]\n"]},{"name":"stdout","output_type":"stream","text":["  ‚úÖ ada: F1@5=0.000, Score=0.819\n","\n","üîÑ Evaluating e5-large...\n"]},{"name":"stderr","output_type":"stream","text":["e5-large: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10/10 [01:34<00:00,  9.45s/it]\n"]},{"name":"stdout","output_type":"stream","text":["  ‚úÖ e5-large: F1@5=0.000, Score=0.843\n","\n","üîÑ Evaluating mpnet...\n"]},{"name":"stderr","output_type":"stream","text":["mpnet: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10/10 [01:34<00:00,  9.41s/it]\n"]},{"name":"stdout","output_type":"stream","text":["  ‚úÖ mpnet: F1@5=0.000, Score=0.324\n","\n","üîÑ Evaluating minilm...\n"]},{"name":"stderr","output_type":"stream","text":["minilm: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10/10 [01:35<00:00,  9.54s/it]"]},{"name":"stdout","output_type":"stream","text":["  ‚úÖ minilm: F1@5=0.000, Score=0.541\n","\n","‚úÖ Evaluation completed in 6.54 minutes\n"]},{"name":"stderr","output_type":"stream","text":["\n"]}],"source":["# Run evaluation\n","evaluation_result = run_real_complete_evaluation(\n","    available_models=available_models,\n","    config_data=config_data,\n","    data_pipeline=data_pipeline,\n","    reranking_method=RERANKING_METHOD,\n","    max_questions=None,  # Use all questions from config\n","    debug=False\n",")\n","\n","all_models_results = evaluation_result['all_model_results']\n","evaluation_duration = evaluation_result['evaluation_duration']\n","evaluation_params = evaluation_result['evaluation_params']\n","\n","print(f\"\\n‚úÖ Evaluation completed in {evaluation_duration/60:.2f} minutes\")"]},{"cell_type":"markdown","metadata":{"id":"EPEFcMcxG9b2"},"source":["## üíæ 6. Save Results"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"eKAVamhsG9b2","outputId":"ae1b6f7f-0d29-4590-d82c-e0acddc51033"},"outputs":[{"name":"stdout","output_type":"stream","text":["‚úÖ Results saved: cumulative_results_20250729_031308.json\n","‚úÖ Results saved:\n","  üìÑ File: cumulative_results_20250729_031308.json\n","  üåç Time: 2025-07-29 03:13:08 -04\n","  ‚úÖ Format: Streamlit compatible\n"]}],"source":["# Save results\n","saved_files = embedded_process_and_save_results(\n","    all_model_results=all_models_results,\n","    output_path=RESULTS_OUTPUT_PATH,\n","    evaluation_params=evaluation_params,\n","    evaluation_duration=evaluation_duration\n",")\n","\n","if saved_files:\n","    print(f\"‚úÖ Results saved:\")\n","    print(f\"  üìÑ File: {os.path.basename(saved_files['json'])}\")\n","    print(f\"  üåç Time: {saved_files['chile_time']}\")\n","    print(f\"  ‚úÖ Format: Streamlit compatible\")\n","else:\n","    print(\"‚ùå Error saving results\")"]},{"cell_type":"markdown","metadata":{"id":"S5pSdjwBG9b2"},"source":["## üìà 7. Results Summary"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"uit0dGgtG9b3","outputId":"5902ef01-65e3-4da7-b361-19ff02524e8d"},"outputs":[{"name":"stdout","output_type":"stream","text":["üìä RESULTS SUMMARY\n","==================================================\n","\n","üìä ADA:\n","  üìù Questions: 10\n","  üìÑ Documents: 187,031\n","  üìà F1@5: 0.000 ‚Üí 0.000 (+0.0%)\n","  üìà MRR: 0.000 ‚Üí 0.000\n","  üìä Avg Score: 0.819 ‚Üí 0.819\n","  üß† CrossEncoder Score: 0.487\n","  üìä Documents Reranked: 500\n","  ü§ñ RAG Metrics Available: ‚úÖ\n","    üìã Faithfulness: 0.900\n","    üéØ BERT F1: 0.492\n","\n","üìä E5-LARGE:\n","  üìù Questions: 10\n","  üìÑ Documents: 187,031\n","  üìà F1@5: 0.000 ‚Üí 0.000 (+0.0%)\n","  üìà MRR: 0.000 ‚Üí 0.000\n","  üìä Avg Score: 0.843 ‚Üí 0.843\n","  üß† CrossEncoder Score: 0.371\n","  üìä Documents Reranked: 500\n","  ü§ñ RAG Metrics Available: ‚úÖ\n","    üìã Faithfulness: 0.940\n","    üéØ BERT F1: 0.490\n","\n","üìä MPNET:\n","  üìù Questions: 10\n","  üìÑ Documents: 187,031\n","  üìà F1@5: 0.000 ‚Üí 0.000 (+0.0%)\n","  üìà MRR: 0.000 ‚Üí 0.000\n","  üìä Avg Score: 0.324 ‚Üí 0.324\n","  üß† CrossEncoder Score: 0.454\n","  üìä Documents Reranked: 500\n","  ü§ñ RAG Metrics Available: ‚úÖ\n","    üìã Faithfulness: 0.960\n","    üéØ BERT F1: 0.467\n","\n","üìä MINILM:\n","  üìù Questions: 10\n","  üìÑ Documents: 187,031\n","  üìà F1@5: 0.000 ‚Üí 0.000 (+0.0%)\n","  üìà MRR: 0.000 ‚Üí 0.000\n","  üìä Avg Score: 0.541 ‚Üí 0.541\n","  üß† CrossEncoder Score: 0.451\n","  üìä Documents Reranked: 500\n","  ü§ñ RAG Metrics Available: ‚úÖ\n","    üìã Faithfulness: 0.920\n","    üéØ BERT F1: 0.480\n","\n","üèÜ OVERALL:\n","  ü•á Best F1@5:  (0.000)\n","  üìä Best Score: e5-large (0.843)\n","\n","üî¨ VERIFICATION:\n","  ‚úÖ Real data: True\n","  üìä Framework: Complete_RAGAS_with_OpenAI_API\n","  üîÑ Method: crossencoder_reranking\n","\n","üéâ EVALUATION COMPLETE!\n"]}],"source":["# Display results summary\n","if saved_files and 'json' in saved_files:\n","    import json\n","\n","    with open(saved_files['json'], 'r') as f:\n","        final_results = json.load(f)\n","\n","    print(\"üìä RESULTS SUMMARY\")\n","    print(\"=\"*50)\n","\n","    if 'results' in final_results:\n","        results_data = final_results['results']\n","\n","        for model_name, model_data in results_data.items():\n","            before_metrics = model_data.get('avg_before_metrics', {})\n","            after_metrics = model_data.get('avg_after_metrics', {})\n","\n","            print(f\"\\nüìä {model_name.upper()}:\")\n","            print(f\"  üìù Questions: {model_data.get('num_questions_evaluated', 0)}\")\n","            print(f\"  üìÑ Documents: {model_data.get('total_documents', 0):,}\")\n","\n","            if before_metrics and after_metrics:\n","                # Performance metrics\n","                f1_before = before_metrics.get('f1@5', 0)\n","                f1_after = after_metrics.get('f1@5', 0)\n","                improvement = ((f1_after - f1_before) / f1_before * 100) if f1_before > 0 else 0\n","\n","                print(f\"  üìà F1@5: {f1_before:.3f} ‚Üí {f1_after:.3f} ({improvement:+.1f}%)\")\n","                print(f\"  üìà MRR: {before_metrics.get('mrr', 0):.3f} ‚Üí {after_metrics.get('mrr', 0):.3f}\")\n","\n","                # Score metrics\n","                score_before = before_metrics.get('model_avg_score', 0)\n","                score_after = after_metrics.get('model_avg_score', 0)\n","\n","                print(f\"  üìä Avg Score: {score_before:.3f} ‚Üí {score_after:.3f}\")\n","\n","                if 'model_avg_crossencoder_score' in after_metrics:\n","                    ce_score = after_metrics.get('model_avg_crossencoder_score', 0)\n","                    print(f\"  üß† CrossEncoder Score: {ce_score:.3f}\")\n","                    print(f\"  üìä Documents Reranked: {after_metrics.get('model_total_documents_reranked', 0)}\")\n","\n","            # RAG metrics\n","            rag_metrics = model_data.get('rag_metrics', {})\n","            if rag_metrics.get('rag_available'):\n","                print(f\"  ü§ñ RAG Metrics Available: ‚úÖ\")\n","                if 'avg_faithfulness' in rag_metrics:\n","                    print(f\"    üìã Faithfulness: {rag_metrics['avg_faithfulness']:.3f}\")\n","                if 'avg_bert_f1' in rag_metrics:\n","                    print(f\"    üéØ BERT F1: {rag_metrics['avg_bert_f1']:.3f}\")\n","            else:\n","                print(f\"  ü§ñ RAG Metrics: ‚ùå\")\n","\n","        # Overall comparison\n","        print(f\"\\nüèÜ OVERALL:\")\n","        best_f1 = (\"\", 0)\n","        best_score = (\"\", 0)\n","\n","        for model_name, model_data in results_data.items():\n","            after_metrics = model_data.get('avg_after_metrics', {})\n","            f1 = after_metrics.get('f1@5', 0)\n","            score = after_metrics.get('model_avg_score', 0)\n","\n","            if f1 > best_f1[1]:\n","                best_f1 = (model_name, f1)\n","            if score > best_score[1]:\n","                best_score = (model_name, score)\n","\n","        print(f\"  ü•á Best F1@5: {best_f1[0]} ({best_f1[1]:.3f})\")\n","        print(f\"  üìä Best Score: {best_score[0]} ({best_score[1]:.3f})\")\n","\n","        # Methodology info\n","        data_verification = final_results.get('evaluation_info', {}).get('data_verification', {})\n","        print(f\"\\nüî¨ VERIFICATION:\")\n","        print(f\"  ‚úÖ Real data: {data_verification.get('is_real_data', False)}\")\n","        print(f\"  üìä Framework: {data_verification.get('rag_framework', 'N/A')}\")\n","        print(f\"  üîÑ Method: {data_verification.get('reranking_method', 'N/A')}\")\n","\n","print(\"\\nüéâ EVALUATION COMPLETE!\")"]},{"cell_type":"markdown","metadata":{"id":"JMU1nw-hG9b3"},"source":["## üßπ 8. Cleanup"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"QLjtydu4G9b3","outputId":"88d66dd5-5652-4eec-af1f-2787cb964fad"},"outputs":[{"name":"stdout","output_type":"stream","text":["üßπ Cleanup completed\n","üéØ Results ready for Streamlit import\n"]}],"source":["# Cleanup\n","data_pipeline.cleanup()\n","import gc\n","gc.collect()\n","\n","print(\"üßπ Cleanup completed\")\n","print(\"üéØ Results ready for Streamlit import\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-lE-TA4NWwHt"},"outputs":[],"source":["# Play an audio beep. Any audio URL will do.\n","from google.colab import output\n","output.eval_js('new Audio(\"https://upload.wikimedia.org/wikipedia/commons/0/05/Beep-09.ogg\").play()')"]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","machine_shape":"hm","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.12"}},"nbformat":4,"nbformat_minor":0}