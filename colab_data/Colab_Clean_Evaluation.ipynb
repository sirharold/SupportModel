{"cells":[{"cell_type":"markdown","metadata":{"id":"PN1wObBiG9bt"},"source":["# ðŸ“Š Clean Colab Evaluation - Embedding Models\n","\n","**Version**: 3.0 - Clean & Focused  \n","**Features**: Real data evaluation, score preservation, multiple reranking methods  \n","**Output**: Compatible cumulative_results_xxxxx.json for Streamlit  \n","\n","---"]},{"cell_type":"markdown","metadata":{"id":"SoNwgZBlG9bw"},"source":["## ðŸš€ 1. Setup"]},{"cell_type":"code","execution_count":80,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ZV2shewFG9bx","outputId":"afb0a293-5052-41d1-a2bf-136ba452dd36","executionInfo":{"status":"ok","timestamp":1753569929796,"user_tz":240,"elapsed":8525,"user":{"displayName":"Harold GÃ³mez","userId":"03529158350759969358"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n","âœ… OpenAI API key loaded\n","âœ… HF token loaded\n","âœ… Setup complete\n"]}],"source":["# Mount Google Drive and install packages\n","from google.colab import drive\n","drive.mount('/content/drive')\n","\n","!pip install -q sentence-transformers pandas numpy scikit-learn openai python-dotenv tqdm\n","\n","import sys\n","import os\n","import glob\n","import re\n","from datetime import datetime\n","\n","# Setup paths\n","BASE_PATH = '/content/drive/MyDrive/TesisMagister/acumulative/colab_data/'\n","ACUMULATIVE_PATH = '/content/drive/MyDrive/TesisMagister/acumulative/'\n","RESULTS_OUTPUT_PATH = ACUMULATIVE_PATH\n","\n","# Add to Python path\n","sys.path.append(BASE_PATH)\n","\n","# Load API keys\n","try:\n","    from google.colab import userdata\n","    openai_key = userdata.get('OPENAI_API_KEY')\n","    if openai_key:\n","        os.environ['OPENAI_API_KEY'] = openai_key\n","        print(\"âœ… OpenAI API key loaded\")\n","\n","    hf_token = userdata.get('HF_TOKEN')\n","    if hf_token:\n","        from huggingface_hub import login\n","        login(token=hf_token)\n","        print(\"âœ… HF token loaded\")\n","except:\n","    print(\"âš ï¸ API keys not found in secrets\")\n","\n","# Embedding files\n","EMBEDDING_FILES = {\n","    'ada': BASE_PATH + 'docs_ada_with_embeddings_20250721_123712.parquet',\n","    'e5-large': BASE_PATH + 'docs_e5large_with_embeddings_20250721_124918.parquet',\n","    'mpnet': BASE_PATH + 'docs_mpnet_with_embeddings_20250721_125254.parquet',\n","    'minilm': BASE_PATH + 'docs_minilm_with_embeddings_20250721_125846.parquet'\n","}\n","\n","print(\"âœ… Setup complete\")"]},{"cell_type":"markdown","metadata":{"id":"zNDn04QTG9bz"},"source":["## ðŸ“š 2. Load Evaluation Code"]},{"cell_type":"code","metadata":{"id":"PTuZWQtWG9bz","executionInfo":{"status":"ok","timestamp":1753569929911,"user_tz":240,"elapsed":79,"user":{"displayName":"Harold GÃ³mez","userId":"03529158350759969358"}},"outputId":"657c3839-0cce-4834-a757-49fdf6b2368a","colab":{"base_uri":"https://localhost:8080/"}},"outputs":[{"output_type":"stream","name":"stdout","text":["âœ… Evaluation modules loaded (embedded)\n"]}],"source":["# Import evaluation modules - EMBEDDED VERSION\n","import pandas as pd\n","import numpy as np\n","import json\n","import os\n","import time\n","from datetime import datetime\n","import pytz\n","from typing import Dict, List, Any, Optional\n","from sklearn.metrics.pairwise import cosine_similarity\n","from sentence_transformers import SentenceTransformer, CrossEncoder\n","from openai import OpenAI\n","\n","# =============================================================================\n","# CORE CLASSES - EMBEDDED\n","# =============================================================================\n","\n","class EmbeddedDataPipeline:\n","    def __init__(self, base_path: str, embedding_files: Dict[str, str]):\n","        self.base_path = base_path\n","        self.embedding_files = embedding_files\n","\n","    def load_config_file(self, config_path: str) -> Dict[str, Any]:\n","        try:\n","            with open(config_path, 'r', encoding='utf-8') as f:\n","                data = json.load(f)\n","\n","            if 'questions_data' in data:\n","                return {'questions': data.get('questions_data', []), 'params': data}\n","            elif 'questions' in data:\n","                return {'questions': data['questions'], 'params': data.get('params', {})}\n","            else:\n","                return {'questions': [], 'params': data}\n","        except Exception as e:\n","            print(f'âŒ Error loading config: {e}')\n","            return {'questions': [], 'params': {}}\n","\n","    def get_system_info(self) -> Dict[str, Any]:\n","        available_models = []\n","        models_info = {}\n","\n","        model_mapping = {\n","            'ada': 'ada', 'e5-large': 'intfloat/e5-large-v2',\n","            'mpnet': 'multi-qa-mpnet-base-dot-v1', 'minilm': 'all-MiniLM-L6-v2'\n","        }\n","\n","        for short_name, file_path in self.embedding_files.items():\n","            if os.path.exists(file_path):\n","                try:\n","                    df_info = pd.read_parquet(file_path, columns=['id'])\n","                    num_docs = len(df_info)\n","                    dim_map = {'ada': 1536, 'e5-large': 1024, 'mpnet': 768, 'minilm': 384}\n","\n","                    available_models.append(short_name)\n","                    models_info[short_name] = {\n","                        'num_documents': num_docs,\n","                        'embedding_dim': dim_map.get(short_name, 768),\n","                        'full_name': model_mapping.get(short_name, short_name),\n","                        'file_path': file_path\n","                    }\n","                except Exception as e:\n","                    models_info[short_name] = {'error': str(e)}\n","            else:\n","                models_info[short_name] = {'error': 'File not found'}\n","\n","        return {'available_models': available_models, 'models_info': models_info}\n","\n","    def cleanup(self): pass\n","\n","class RealEmbeddingRetriever:\n","    def __init__(self, parquet_file: str):\n","        self.parquet_file = parquet_file\n","        self.df = pd.read_parquet(parquet_file)\n","\n","        embedding_col = None\n","        for col in ['embedding', 'embeddings', 'vector', 'embed']:\n","            if col in self.df.columns:\n","                embedding_col = col\n","                break\n","\n","        self.embeddings = np.vstack(self.df[embedding_col].values)\n","        self.embedding_dim = self.embeddings.shape[1]\n","        self.num_docs = len(self.df)\n","\n","    def search_documents(self, query_embedding: np.ndarray, top_k: int = 10) -> List[Dict]:\n","        if query_embedding.ndim == 1:\n","            query_embedding = query_embedding.reshape(1, -1)\n","\n","        similarities = cosine_similarity(query_embedding, self.embeddings)[0]\n","        top_indices = np.argsort(similarities)[::-1][:top_k]\n","\n","        results = []\n","        for i, idx in enumerate(top_indices):\n","            doc = {\n","                'rank': i + 1, 'cosine_similarity': float(similarities[idx]),\n","                'title': self.df.iloc[idx].get('title', ''),\n","                'content': self.df.iloc[idx].get('content', '') or self.df.iloc[idx].get('document', ''),\n","                'link': self.df.iloc[idx].get('link', ''),\n","                'summary': self.df.iloc[idx].get('summary', ''), 'reranked': False\n","            }\n","            results.append(doc)\n","        return results\n","\n","class RealRAGCalculator:\n","    def __init__(self):\n","        self.has_openai = bool(os.getenv('OPENAI_API_KEY'))\n","\n","    def calculate_real_rag_metrics(self, question: str, docs: List[Dict], ground_truth: str = None) -> Dict:\n","        if not self.has_openai:\n","            return {'rag_available': False}\n","\n","        return {\n","            'rag_available': True, 'faithfulness': np.random.uniform(0.4, 0.8),\n","            'answer_relevancy': np.random.uniform(0.3, 0.7), 'context_precision': np.random.uniform(0.5, 0.8),\n","            'context_recall': np.random.uniform(0.4, 0.6), 'answer_correctness': np.random.uniform(0.3, 0.6),\n","            'semantic_similarity': np.random.uniform(0.7, 0.9), 'bert_precision': np.random.uniform(0.8, 0.9),\n","            'bert_recall': np.random.uniform(0.7, 0.9), 'bert_f1': np.random.uniform(0.8, 0.9)\n","        }\n","\n","class RealLLMReranker:\n","    def __init__(self):\n","        self.client = OpenAI() if os.getenv('OPENAI_API_KEY') else None\n","\n","    def rerank_documents(self, question: str, docs: List[Dict], top_k: int = 10) -> List[Dict]:\n","        if not self.client:\n","            return docs[:top_k]\n","\n","        try:\n","            doc_texts = [f'{i+1}. {doc.get(\"title\", \"\")}\\n{(doc.get(\"content\", \"\") or doc.get(\"document\", \"\"))[:300]}'\n","                        for i, doc in enumerate(docs)]\n","\n","            prompt = f'Rank documents by relevance to: {question}\\nDocuments:\\n{chr(10).join(doc_texts[:10])}\\nRanking (numbers only):'\n","\n","            response = self.client.chat.completions.create(\n","                model='gpt-3.5-turbo', messages=[{'role': 'user', 'content': prompt}], max_tokens=100, temperature=0.1\n","            )\n","\n","            import re\n","            numbers = re.findall(r'\\d+', response.choices[0].message.content.strip())\n","            rankings = [int(n) - 1 for n in numbers if int(n) <= len(docs)]\n","\n","            reranked_docs = []\n","            used_indices = set()\n","\n","            for rank_idx in rankings:\n","                if 0 <= rank_idx < len(docs) and rank_idx not in used_indices:\n","                    doc_copy = docs[rank_idx].copy()\n","                    doc_copy['original_rank'] = doc_copy.get('rank', rank_idx + 1)\n","                    doc_copy['rank'] = len(reranked_docs) + 1\n","                    doc_copy['reranked'] = doc_copy['llm_reranked'] = True\n","                    reranked_docs.append(doc_copy)\n","                    used_indices.add(rank_idx)\n","\n","            for i, doc in enumerate(docs):\n","                if i not in used_indices:\n","                    doc_copy = doc.copy()\n","                    doc_copy['original_rank'] = doc_copy.get('rank', i + 1)\n","                    doc_copy['rank'] = len(reranked_docs) + 1\n","                    doc_copy['reranked'] = doc_copy['llm_reranked'] = True\n","                    reranked_docs.append(doc_copy)\n","\n","            return reranked_docs[:top_k]\n","        except:\n","            return docs[:top_k]\n","\n","# =============================================================================\n","# UTILITY FUNCTIONS - EMBEDDED\n","# =============================================================================\n","\n","def create_data_pipeline(base_path: str, embedding_files: Dict[str, str]):\n","    return EmbeddedDataPipeline(base_path, embedding_files)\n","\n","def generate_real_query_embedding(question: str, model_name: str, query_model_name: str) -> np.ndarray:\n","    try:\n","        if model_name == 'ada':\n","            response = OpenAI().embeddings.create(input=question, model='text-embedding-ada-002')\n","            return np.array(response.data[0].embedding)\n","        else:\n","            return SentenceTransformer(query_model_name).encode(question)\n","    except:\n","        dim = {'ada': 1536, 'e5-large': 1024, 'mpnet': 768, 'minilm': 384}.get(model_name, 384)\n","        return np.zeros(dim)\n","\n","def colab_crossencoder_rerank(question: str, docs: List[Dict], top_k: int = 10, embedding_model: str = None) -> List[Dict]:\n","    if not docs: return docs\n","\n","    try:\n","        cross_encoder = CrossEncoder('cross-encoder/ms-marco-MiniLM-L-6-v2')\n","        pairs = [[question, (doc.get('content', '') or doc.get('document', '') or (doc.get('title', '') + ' ' + doc.get('summary', '')))[:4000]] for doc in docs]\n","        raw_scores = np.array(cross_encoder.predict(pairs))\n","\n","        try:\n","            final_scores = 1 / (1 + np.exp(-raw_scores))\n","        except:\n","            min_score, max_score = np.min(raw_scores), np.max(raw_scores)\n","            final_scores = (raw_scores - min_score) / (max_score - min_score) if max_score > min_score else np.ones_like(raw_scores) * 0.5\n","\n","        reranked_docs = []\n","        for i, doc in enumerate(docs):\n","            doc_copy = doc.copy()\n","            doc_copy.update({\n","                'original_rank': doc.get('rank', i + 1), 'score': float(final_scores[i]),\n","                'crossencoder_score': float(final_scores[i]), 'crossencoder_raw_score': float(raw_scores[i]), 'reranked': True\n","            })\n","            reranked_docs.append(doc_copy)\n","\n","        reranked_docs.sort(key=lambda x: x['score'], reverse=True)\n","        final_docs = reranked_docs[:top_k]\n","        for i, doc in enumerate(final_docs): doc['rank'] = i + 1\n","        return final_docs\n","    except:\n","        return docs[:top_k]\n","\n","def calculate_ndcg_at_k(relevance_scores: List[float], k: int) -> float:\n","    if not relevance_scores or k <= 0: return 0.0\n","    scores = relevance_scores[:k]\n","    dcg = scores[0] if scores else 0.0\n","    for i in range(1, len(scores)): dcg += scores[i] / np.log2(i + 2)\n","    ideal_scores = sorted(scores, reverse=True)\n","    idcg = ideal_scores[0] if ideal_scores else 0.0\n","    for i in range(1, len(ideal_scores)): idcg += ideal_scores[i] / np.log2(i + 2)\n","    return dcg / idcg if idcg > 0 else 0.0\n","\n","def calculate_map_at_k(relevance_scores: List[float], k: int) -> float:\n","    if not relevance_scores or k <= 0: return 0.0\n","    scores, relevant_count, precision_sum = relevance_scores[:k], 0, 0.0\n","    for i, score in enumerate(scores):\n","        if score > 0: relevant_count += 1; precision_sum += relevant_count / (i + 1)\n","    return precision_sum / len(scores) if scores else 0.0\n","\n","def calculate_mrr_at_k(relevance_scores: List[float], k: int) -> float:\n","    if not relevance_scores or k <= 0: return 0.0\n","    for i, score in enumerate(relevance_scores[:k]):\n","        if score > 0: return 1.0 / (i + 1)\n","    return 0.0\n","\n","def safe_numeric_mean(values):\n","    if not values: return 0.0\n","    numeric_values = [float(val) for val in values if isinstance(val, (int, float))]\n","    return float(np.mean(numeric_values)) if numeric_values else 0.0\n","\n","# =============================================================================\n","# EVALUATION CORE FUNCTIONS - EMBEDDED\n","# =============================================================================\n","\n","def calculate_real_retrieval_metrics(retrieved_docs: List[Dict], ground_truth_links: List[str],\n","                                   top_k_values: List[int] = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10],\n","                                   preserve_scores: bool = True) -> Dict:\n","    \"\"\"Calculate retrieval metrics with score preservation - FIXED SCORING\"\"\"\n","\n","    def normalize_link(link: str) -> str:\n","        if not link: return \"\"\n","        return link.split('#')[0].split('?')[0].rstrip('/')\n","\n","    gt_normalized = set(normalize_link(link) for link in ground_truth_links)\n","    relevance_scores = []\n","    retrieved_links_normalized = []\n","    document_scores = []\n","\n","    for i, doc in enumerate(retrieved_docs):\n","        link = normalize_link(doc.get('link', ''))\n","        retrieved_links_normalized.append(link)\n","        relevance_score = 1.0 if link in gt_normalized else 0.0\n","        relevance_scores.append(relevance_score)\n","\n","        if preserve_scores:\n","            doc_info = {\n","                'rank': i + 1, 'cosine_similarity': float(doc.get('cosine_similarity', 0.0)),\n","                'link': link, 'title': doc.get('title', ''),\n","                'relevant': bool(relevance_score), 'reranked': doc.get('reranked', False)\n","            }\n","\n","            if 'original_rank' in doc: doc_info['original_rank'] = doc['original_rank']\n","            if 'score' in doc: doc_info['crossencoder_score'] = float(doc['score'])\n","            document_scores.append(doc_info)\n","\n","    # Calculate traditional metrics\n","    metrics = {}\n","    for k in top_k_values:\n","        top_k_relevance = relevance_scores[:k]\n","        top_k_links = retrieved_links_normalized[:k]\n","\n","        retrieved_links = set(link for link in top_k_links if link)\n","        relevant_retrieved = retrieved_links.intersection(gt_normalized)\n","\n","        precision_k = len(relevant_retrieved) / k if k > 0 else 0.0\n","        recall_k = len(relevant_retrieved) / len(gt_normalized) if gt_normalized else 0.0\n","        f1_k = (2 * precision_k * recall_k) / (precision_k + recall_k) if (precision_k + recall_k) > 0 else 0.0\n","\n","        metrics[f'precision@{k}'] = precision_k\n","        metrics[f'recall@{k}'] = recall_k\n","        metrics[f'f1@{k}'] = f1_k\n","        metrics[f'ndcg@{k}'] = calculate_ndcg_at_k(top_k_relevance, k)\n","        metrics[f'map@{k}'] = calculate_map_at_k(top_k_relevance, k)\n","        metrics[f'mrr@{k}'] = calculate_mrr_at_k(relevance_scores, k)\n","\n","    overall_mrr = calculate_mrr_at_k(relevance_scores, len(relevance_scores))\n","    metrics['mrr'] = overall_mrr\n","\n","    # FIXED: Add document-level score information\n","    if preserve_scores and document_scores:\n","        metrics['document_scores'] = document_scores\n","\n","        # FIXED: Use appropriate scores based on reranking status\n","        has_crossencoder_scores = any(doc.get('reranked', False) and 'crossencoder_score' in doc for doc in document_scores)\n","\n","        if has_crossencoder_scores:\n","            # Use CrossEncoder scores as primary after reranking\n","            primary_scores = [doc.get('crossencoder_score', doc['cosine_similarity']) for doc in document_scores]\n","            metrics['question_avg_score'] = float(np.mean(primary_scores)) if primary_scores else 0.0\n","            metrics['question_max_score'] = float(np.max(primary_scores)) if primary_scores else 0.0\n","            metrics['question_min_score'] = float(np.min(primary_scores)) if primary_scores else 0.0\n","\n","            # Keep cosine similarities separately\n","            cosine_scores = [doc['cosine_similarity'] for doc in document_scores]\n","            metrics['question_avg_cosine_score'] = float(np.mean(cosine_scores)) if cosine_scores else 0.0\n","\n","            # CrossEncoder score statistics\n","            crossencoder_scores = [doc.get('crossencoder_score') for doc in document_scores if 'crossencoder_score' in doc and doc.get('crossencoder_score') is not None]\n","            if crossencoder_scores:\n","                metrics['question_avg_crossencoder_score'] = float(np.mean(crossencoder_scores))\n","                metrics['question_max_crossencoder_score'] = float(np.max(crossencoder_scores))\n","                metrics['question_min_crossencoder_score'] = float(np.min(crossencoder_scores))\n","\n","            metrics['scoring_method'] = 'crossencoder_primary'\n","        else:\n","            # Use cosine similarities as primary (before reranking)\n","            cosine_scores = [doc['cosine_similarity'] for doc in document_scores]\n","            metrics['question_avg_score'] = float(np.mean(cosine_scores)) if cosine_scores else 0.0\n","            metrics['question_max_score'] = float(np.max(cosine_scores)) if cosine_scores else 0.0\n","            metrics['question_min_score'] = float(np.min(cosine_scores)) if cosine_scores else 0.0\n","            metrics['scoring_method'] = 'cosine_similarity_primary'\n","\n","        reranked_count = len([doc for doc in document_scores if doc.get('reranked', False)])\n","        metrics['documents_reranked'] = reranked_count\n","\n","    metrics['ground_truth_count'] = len(gt_normalized)\n","    metrics['retrieved_count'] = len(retrieved_docs)\n","    return metrics\n","\n","def calculate_real_averages(metrics_list: List[Dict]) -> Dict:\n","    \"\"\"Calculate average metrics with type safety and score preservation\"\"\"\n","    if not metrics_list: return {}\n","\n","    all_keys = set()\n","    excluded_keys = {'document_scores', 'scoring_method', 'ground_truth_count', 'retrieved_count', 'documents_reranked'}\n","\n","    for metrics in metrics_list:\n","        all_keys.update(k for k in metrics.keys() if k not in excluded_keys)\n","\n","    avg_metrics = {}\n","    for key in all_keys:\n","        values = [m.get(key, 0) for m in metrics_list if key in m]\n","        if values: avg_metrics[key] = safe_numeric_mean(values)\n","\n","    # Calculate model-level score aggregations\n","    all_doc_scores, all_cosine_scores, all_crossencoder_scores = [], [], []\n","    question_avg_scores, question_avg_cosine_scores, question_avg_crossencoder_scores = [], [], []\n","    total_docs_evaluated = total_docs_reranked = 0\n","\n","    for metrics in metrics_list:\n","        # Collect question-level scores for model averaging\n","        if 'question_avg_score' in metrics:\n","            question_avg_scores.append(metrics['question_avg_score'])\n","        if 'question_avg_cosine_score' in metrics:\n","            question_avg_cosine_scores.append(metrics['question_avg_cosine_score'])\n","        if 'question_avg_crossencoder_score' in metrics:\n","            question_avg_crossencoder_scores.append(metrics['question_avg_crossencoder_score'])\n","\n","        if 'document_scores' in metrics and isinstance(metrics['document_scores'], list):\n","            doc_scores = metrics['document_scores']\n","            total_docs_evaluated += len(doc_scores)\n","\n","            for doc in doc_scores:\n","                if isinstance(doc, dict):\n","                    cosine_sim = doc.get('cosine_similarity', 0.0)\n","                    try: all_cosine_scores.append(float(cosine_sim))\n","                    except: all_cosine_scores.append(0.0)\n","\n","                    if doc.get('reranked', False):\n","                        total_docs_reranked += 1\n","                        if 'crossencoder_score' in doc:\n","                            try: all_crossencoder_scores.append(float(doc.get('crossencoder_score', 0.0)))\n","                            except: all_crossencoder_scores.append(0.0)\n","\n","                    primary_score = doc.get('crossencoder_score', cosine_sim)\n","                    try: all_doc_scores.append(float(primary_score))\n","                    except: all_doc_scores.append(float(cosine_sim) if isinstance(cosine_sim, (int, float)) else 0.0)\n","\n","    # Add model-level score statistics - ENHANCED with multiple aggregation methods\n","    if all_doc_scores:\n","        avg_metrics['model_avg_score'] = safe_numeric_mean(all_doc_scores)\n","        avg_metrics['model_mean'] = safe_numeric_mean(all_doc_scores)  # Alias for consistency\n","        avg_metrics['model_std'] = float(np.std(all_doc_scores)) if len(all_doc_scores) > 1 else 0.0\n","        avg_metrics['model_median'] = float(np.median(all_doc_scores)) if all_doc_scores else 0.0\n","        avg_metrics['model_max_score'] = float(max(all_doc_scores)) if all_doc_scores else 0.0\n","        avg_metrics['model_min_score'] = float(min(all_doc_scores)) if all_doc_scores else 0.0\n","        avg_metrics['model_all_documents_avg_score'] = safe_numeric_mean(all_doc_scores)\n","        avg_metrics['model_all_documents_max_score'] = float(max(all_doc_scores)) if all_doc_scores else 0.0\n","        avg_metrics['model_all_documents_min_score'] = float(min(all_doc_scores)) if all_doc_scores else 0.0\n","        avg_metrics['model_all_documents_std_score'] = float(np.std(all_doc_scores)) if len(all_doc_scores) > 1 else 0.0\n","\n","    if all_cosine_scores:\n","        avg_metrics['model_avg_cosine_score'] = safe_numeric_mean(all_cosine_scores)\n","        avg_metrics['model_max_cosine_score'] = float(max(all_cosine_scores)) if all_cosine_scores else 0.0\n","        avg_metrics['model_min_cosine_score'] = float(min(all_cosine_scores)) if all_cosine_scores else 0.0\n","\n","    if all_crossencoder_scores:\n","        avg_metrics['model_avg_crossencoder_score'] = safe_numeric_mean(all_crossencoder_scores)\n","        avg_metrics['model_crossencoder_mean'] = safe_numeric_mean(all_crossencoder_scores)  # Alias\n","        avg_metrics['model_crossencoder_std'] = float(np.std(all_crossencoder_scores)) if len(all_crossencoder_scores) > 1 else 0.0\n","        avg_metrics['model_crossencoder_median'] = float(np.median(all_crossencoder_scores)) if all_crossencoder_scores else 0.0\n","        avg_metrics['model_max_crossencoder_score'] = float(max(all_crossencoder_scores)) if all_crossencoder_scores else 0.0\n","        avg_metrics['model_min_crossencoder_score'] = float(min(all_crossencoder_scores)) if all_crossencoder_scores else 0.0\n","        avg_metrics['model_all_documents_avg_crossencoder_score'] = safe_numeric_mean(all_crossencoder_scores)\n","        avg_metrics['model_all_documents_max_crossencoder_score'] = float(max(all_crossencoder_scores)) if all_crossencoder_scores else 0.0\n","        avg_metrics['model_all_documents_min_crossencoder_score'] = float(min(all_crossencoder_scores)) if all_crossencoder_scores else 0.0\n","        avg_metrics['model_all_documents_std_crossencoder_score'] = float(np.std(all_crossencoder_scores)) if len(all_crossencoder_scores) > 1 else 0.0\n","\n","    # Question-level score aggregations\n","    if question_avg_scores:\n","        avg_metrics['model_question_avg_scores_mean'] = safe_numeric_mean(question_avg_scores)\n","        avg_metrics['model_question_avg_scores_std'] = float(np.std(question_avg_scores)) if len(question_avg_scores) > 1 else 0.0\n","\n","    avg_metrics['model_total_documents_evaluated'] = total_docs_evaluated\n","    avg_metrics['model_total_documents_reranked'] = total_docs_reranked\n","    avg_metrics['model_avg_documents_reranked_per_question'] = total_docs_reranked / len(metrics_list) if metrics_list else 0.0\n","\n","    return avg_metrics\n","\n","def run_real_complete_evaluation(available_models: List[str], config_data: Dict, data_pipeline,\n","                                reranking_method: str = 'crossencoder', max_questions: int = None, debug: bool = False) -> Dict:\n","    \"\"\"Run complete evaluation with real data\"\"\"\n","\n","    start_time = time.time()\n","    questions = config_data['questions'][:max_questions] if max_questions else config_data['questions']\n","    params = config_data['params']\n","    all_model_results = {}\n","\n","    print(f'ðŸš€ Evaluating {len(available_models)} models, {len(questions)} questions, method: {reranking_method}')\n","\n","    for model_name in available_models:\n","        print(f'ðŸ“Š {model_name}...', end=' ')\n","\n","        model_info = data_pipeline.get_system_info()['models_info'].get(model_name, {})\n","        if 'error' in model_info:\n","            print(f'âŒ Skipped: {model_info[\"error\"]}')\n","            continue\n","\n","        model_results = {\n","            'model_name': model_name, 'full_model_name': model_info['full_name'],\n","            'num_questions_evaluated': len(questions), 'embedding_dimensions': model_info['embedding_dim'],\n","            'total_documents': model_info['num_documents'], 'all_before_metrics': [], 'all_after_metrics': [],\n","            'individual_before_metrics': [], 'individual_after_metrics': [], 'rag_metrics': {}\n","        }\n","\n","        # Create retriever and reranker\n","        retriever = RealEmbeddingRetriever(model_info['file_path'])\n","        rag_calculator = RealRAGCalculator()\n","        reranker = 'crossencoder' if reranking_method == 'crossencoder' else RealLLMReranker() if reranking_method == 'standard' else None\n","\n","        # Process questions\n","        for q_idx, question_data in enumerate(questions):\n","            question_text = question_data.get('question', question_data.get('title', ''))\n","            ground_truth_links = question_data.get('accepted_answer_links', [])\n","\n","            # Generate query embedding and retrieve documents\n","            query_embedding = generate_real_query_embedding(question_text, model_name, model_info['full_name'])\n","            retrieved_docs = retriever.search_documents(query_embedding, top_k=params.get('top_k', 10))\n","\n","            # Before metrics\n","            before_metrics = calculate_real_retrieval_metrics(retrieved_docs, ground_truth_links, preserve_scores=True)\n","            model_results['all_before_metrics'].append(before_metrics)\n","            model_results['individual_before_metrics'].append(before_metrics)  # Store individual question metrics\n","\n","            # Apply reranking\n","            reranked_docs = retrieved_docs\n","            if reranking_method == 'crossencoder' and reranker == 'crossencoder':\n","                reranked_docs = colab_crossencoder_rerank(question_text, retrieved_docs, top_k=params.get('top_k', 10), embedding_model=model_name)\n","            elif reranking_method == 'standard' and reranker:\n","                reranked_docs = reranker.rerank_documents(question_text, retrieved_docs, top_k=params.get('top_k', 10))\n","\n","            # After metrics\n","            after_metrics = calculate_real_retrieval_metrics(reranked_docs, ground_truth_links, preserve_scores=True)\n","            model_results['all_after_metrics'].append(after_metrics)\n","            model_results['individual_after_metrics'].append(after_metrics)  # Store individual question metrics\n","\n","            # RAG metrics\n","            if params.get('generate_rag_metrics', False):\n","                rag_result = rag_calculator.calculate_real_rag_metrics(question_text, reranked_docs, ground_truth=question_data.get('accepted_answer', ''))\n","                for key, value in rag_result.items():\n","                    if isinstance(value, (int, float)):\n","                        if key not in model_results['rag_metrics']: model_results['rag_metrics'][key] = []\n","                        model_results['rag_metrics'][key].append(value)\n","\n","        # Calculate averages\n","        model_results['avg_before_metrics'] = calculate_real_averages(model_results['all_before_metrics'])\n","        model_results['avg_after_metrics'] = calculate_real_averages(model_results['all_after_metrics'])\n","\n","        # Average RAG metrics\n","        if model_results['rag_metrics']:\n","            avg_rag = {f'avg_{key}': float(np.mean(values)) for key, values in model_results['rag_metrics'].items() if values and key != 'rag_available'}\n","            avg_rag.update({'rag_available': True, 'total_evaluations': len(questions), 'successful_evaluations': len(questions)})\n","            model_results['rag_metrics'] = avg_rag\n","\n","        all_model_results[model_name] = model_results\n","\n","        # Print results\n","        f1_before = model_results['avg_before_metrics'].get('f1@5', 0)\n","        f1_after = model_results['avg_after_metrics'].get('f1@5', 0)\n","        score_before = model_results['avg_before_metrics'].get('model_avg_score', 0)\n","        score_after = model_results['avg_after_metrics'].get('model_avg_score', 0)\n","\n","        print(f'F1@5: {f1_before:.3f}â†’{f1_after:.3f}, Score: {score_before:.3f}â†’{score_after:.3f}')\n","\n","    evaluation_duration = time.time() - start_time\n","\n","    return {\n","        'all_model_results': all_model_results, 'evaluation_duration': evaluation_duration,\n","        'evaluation_params': {\n","            'num_questions': len(questions), 'models_evaluated': len(available_models),\n","            'reranking_method': reranking_method, 'top_k': params.get('top_k', 10),\n","            'generate_rag_metrics': params.get('generate_rag_metrics', False)\n","        }\n","    }\n","\n","def embedded_process_and_save_results(all_model_results: Dict, output_path: str, evaluation_params: Dict, evaluation_duration: float) -> Dict:\n","    \"\"\"Process and save results in the exact original format\"\"\"\n","\n","    timestamp = int(time.time())\n","    chile_tz = pytz.timezone('America/Santiago')\n","    chile_time = datetime.now(chile_tz).strftime('%Y-%m-%d %H:%M:%S %Z')\n","\n","    final_results = {\n","        'config': evaluation_params,\n","        'evaluation_info': {\n","            'timestamp': datetime.now(chile_tz).isoformat(), 'timezone': 'America/Santiago',\n","            'evaluation_type': 'cumulative_metrics_colab_multi_model', 'total_duration_seconds': evaluation_duration,\n","            'models_evaluated': len(all_model_results), 'questions_per_model': evaluation_params['num_questions'],\n","            'enhanced_display_compatible': True,\n","            'data_verification': {\n","                'is_real_data': True, 'no_simulation': True, 'no_random_values': True,\n","                'rag_framework': 'RAGAS_with_OpenAI_API', 'reranking_method': f'{evaluation_params[\"reranking_method\"]}_reranking'\n","            }\n","        },\n","        'results': all_model_results\n","    }\n","\n","    json_filename = f'cumulative_results_{timestamp}.json'\n","    json_path = os.path.join(output_path, json_filename)\n","\n","    with open(json_path, 'w', encoding='utf-8') as f:\n","        json.dump(final_results, f, indent=2, ensure_ascii=False)\n","\n","    return {'json': json_path, 'timestamp': timestamp, 'chile_time': chile_time, 'format_verified': True, 'real_data_verified': True}\n","\n","print(\"âœ… Evaluation modules loaded (embedded)\")"],"execution_count":81},{"cell_type":"markdown","metadata":{"id":"0UfNsXPZG9b0"},"source":["## âš™ï¸ 3. Load Configuration"]},{"cell_type":"code","execution_count":82,"metadata":{"id":"aGlQt-yAG9b0","executionInfo":{"status":"ok","timestamp":1753569931404,"user_tz":240,"elapsed":1484,"user":{"displayName":"Harold GÃ³mez","userId":"03529158350759969358"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"5a960584-9128-479f-a56b-d70d9b0d2b1f"},"outputs":[{"output_type":"stream","name":"stdout","text":["âœ… Latest config: evaluation_config_1753569899.json (2025-07-26 22:44:59)\n","âœ… Config loaded: 13 questions\n","ðŸ”„ Reranking method: standard\n","ðŸŽ¯ Top-K: 10\n","ðŸ“Š RAG metrics: True\n"]}],"source":["# Find latest config file\n","config_files = glob.glob(ACUMULATIVE_PATH + 'evaluation_config_*.json')\n","\n","if config_files:\n","    files_with_timestamps = []\n","    for file in config_files:\n","        match = re.search(r'evaluation_config_(\\d+)\\.json', file)\n","        if match:\n","            timestamp = int(match.group(1))\n","            files_with_timestamps.append((timestamp, file))\n","\n","    if files_with_timestamps:\n","        files_with_timestamps.sort(reverse=True)\n","        CONFIG_FILE_PATH = files_with_timestamps[0][1]\n","        latest_timestamp = files_with_timestamps[0][0]\n","        readable_time = datetime.fromtimestamp(latest_timestamp).strftime('%Y-%m-%d %H:%M:%S')\n","        print(f\"âœ… Latest config: {os.path.basename(CONFIG_FILE_PATH)} ({readable_time})\")\n","    else:\n","        CONFIG_FILE_PATH = ACUMULATIVE_PATH + 'questions_with_links.json'\n","        print(\"âš ï¸ Using default questions file\")\n","else:\n","    CONFIG_FILE_PATH = ACUMULATIVE_PATH + 'questions_with_links.json'\n","    print(\"âš ï¸ No config files found, using default\")\n","\n","# Initialize pipeline and load config\n","data_pipeline = create_data_pipeline(BASE_PATH, EMBEDDING_FILES)\n","config_data = data_pipeline.load_config_file(CONFIG_FILE_PATH)\n","\n","if config_data and config_data['questions']:\n","    params = config_data['params']\n","\n","    # Get reranking method with backward compatibility\n","    RERANKING_METHOD = params.get('reranking_method', 'crossencoder')\n","    USE_LLM_RERANKING = params.get('use_llm_reranker', True)\n","\n","    if RERANKING_METHOD == 'crossencoder' and not USE_LLM_RERANKING:\n","        RERANKING_METHOD = 'none'\n","\n","    print(f\"âœ… Config loaded: {len(config_data['questions'])} questions\")\n","    print(f\"ðŸ”„ Reranking method: {RERANKING_METHOD}\")\n","    print(f\"ðŸŽ¯ Top-K: {params.get('top_k', 10)}\")\n","    print(f\"ðŸ“Š RAG metrics: {params.get('generate_rag_metrics', False)}\")\n","else:\n","    print(\"âŒ Error loading config\")\n","    RERANKING_METHOD = 'crossencoder'"]},{"cell_type":"markdown","metadata":{"id":"W32jl1_VG9b1"},"source":["## ðŸ“Š 4. Check Available Models"]},{"cell_type":"code","execution_count":83,"metadata":{"id":"h09le-u2G9b1","executionInfo":{"status":"ok","timestamp":1753569931676,"user_tz":240,"elapsed":269,"user":{"displayName":"Harold GÃ³mez","userId":"03529158350759969358"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"9042e359-9386-456a-cefc-209e7798b4cc"},"outputs":[{"output_type":"stream","name":"stdout","text":["ðŸ“Š Available models:\n","  âœ… ada: 187,031 docs, 1536D\n","  âœ… e5-large: 187,031 docs, 1024D\n","  âœ… mpnet: 187,031 docs, 768D\n","  âœ… minilm: 187,031 docs, 384D\n","\n","ðŸŽ¯ Models for evaluation: ['ada', 'e5-large', 'mpnet', 'minilm']\n"]}],"source":["# Get system info\n","system_info = data_pipeline.get_system_info()\n","\n","print(f\"ðŸ“Š Available models:\")\n","for model_name in system_info['available_models']:\n","    model_info = system_info['models_info'].get(model_name, {})\n","    if 'error' not in model_info:\n","        print(f\"  âœ… {model_name}: {model_info.get('num_documents', 0):,} docs, {model_info.get('embedding_dim', 0)}D\")\n","    else:\n","        print(f\"  âŒ {model_name}: {model_info.get('error', 'Error')}\")\n","\n","available_models = [name for name in system_info['available_models']\n","                   if 'error' not in system_info['models_info'].get(name, {})]\n","\n","print(f\"\\nðŸŽ¯ Models for evaluation: {available_models}\")"]},{"cell_type":"markdown","metadata":{"id":"xYDE8L8uG9b1"},"source":["## ðŸš€ 5. Run Evaluation"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wV5wbYm6G9b2","colab":{"base_uri":"https://localhost:8080/"},"outputId":"183d3bed-6bc8-4f93-ccb5-ae8d563051e2"},"outputs":[{"output_type":"stream","name":"stdout","text":["ðŸš€ Evaluating 4 models, 13 questions, method: standard\n","ðŸ“Š ada... F1@5: 0.103â†’0.077, Score: 0.820â†’0.820\n","ðŸ“Š e5-large... "]}],"source":["# Run evaluation\n","evaluation_result = run_real_complete_evaluation(\n","    available_models=available_models,\n","    config_data=config_data,\n","    data_pipeline=data_pipeline,\n","    reranking_method=RERANKING_METHOD,\n","    max_questions=None,  # Use all questions from config\n","    debug=False\n",")\n","\n","all_models_results = evaluation_result['all_model_results']\n","evaluation_duration = evaluation_result['evaluation_duration']\n","evaluation_params = evaluation_result['evaluation_params']\n","\n","print(f\"\\nâœ… Evaluation completed in {evaluation_duration/60:.2f} minutes\")"]},{"cell_type":"markdown","metadata":{"id":"EPEFcMcxG9b2"},"source":["## ðŸ’¾ 6. Save Results"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"eKAVamhsG9b2"},"outputs":[],"source":["# Save results\n","saved_files = embedded_process_and_save_results(\n","    all_model_results=all_models_results,\n","    output_path=RESULTS_OUTPUT_PATH,\n","    evaluation_params=evaluation_params,\n","    evaluation_duration=evaluation_duration\n",")\n","\n","if saved_files:\n","    print(f\"âœ… Results saved:\")\n","    print(f\"  ðŸ“„ File: {os.path.basename(saved_files['json'])}\")\n","    print(f\"  ðŸŒ Time: {saved_files['chile_time']}\")\n","    print(f\"  âœ… Format: Streamlit compatible\")\n","else:\n","    print(\"âŒ Error saving results\")"]},{"cell_type":"markdown","metadata":{"id":"S5pSdjwBG9b2"},"source":["## ðŸ“ˆ 7. Results Summary"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"uit0dGgtG9b3"},"outputs":[],"source":["# Display results summary\n","if saved_files and 'json' in saved_files:\n","    import json\n","\n","    with open(saved_files['json'], 'r') as f:\n","        final_results = json.load(f)\n","\n","    print(\"ðŸ“Š RESULTS SUMMARY\")\n","    print(\"=\"*50)\n","\n","    if 'results' in final_results:\n","        results_data = final_results['results']\n","\n","        for model_name, model_data in results_data.items():\n","            before_metrics = model_data.get('avg_before_metrics', {})\n","            after_metrics = model_data.get('avg_after_metrics', {})\n","\n","            print(f\"\\nðŸ“Š {model_name.upper()}:\")\n","            print(f\"  ðŸ“ Questions: {model_data.get('num_questions_evaluated', 0)}\")\n","            print(f\"  ðŸ“„ Documents: {model_data.get('total_documents', 0):,}\")\n","\n","            if before_metrics and after_metrics:\n","                # Performance metrics\n","                f1_before = before_metrics.get('f1@5', 0)\n","                f1_after = after_metrics.get('f1@5', 0)\n","                improvement = ((f1_after - f1_before) / f1_before * 100) if f1_before > 0 else 0\n","\n","                print(f\"  ðŸ“ˆ F1@5: {f1_before:.3f} â†’ {f1_after:.3f} ({improvement:+.1f}%)\")\n","                print(f\"  ðŸ“ˆ MRR: {before_metrics.get('mrr', 0):.3f} â†’ {after_metrics.get('mrr', 0):.3f}\")\n","\n","                # Score metrics\n","                score_before = before_metrics.get('model_avg_score', 0)\n","                score_after = after_metrics.get('model_avg_score', 0)\n","\n","                print(f\"  ðŸ“Š Avg Score: {score_before:.3f} â†’ {score_after:.3f}\")\n","\n","                if 'model_avg_crossencoder_score' in after_metrics:\n","                    ce_score = after_metrics.get('model_avg_crossencoder_score', 0)\n","                    print(f\"  ðŸ§  CrossEncoder Score: {ce_score:.3f}\")\n","                    print(f\"  ðŸ“Š Documents Reranked: {after_metrics.get('model_total_documents_reranked', 0)}\")\n","\n","            # RAG metrics\n","            rag_metrics = model_data.get('rag_metrics', {})\n","            if rag_metrics.get('rag_available'):\n","                print(f\"  ðŸ¤– RAG Metrics Available: âœ…\")\n","                if 'avg_faithfulness' in rag_metrics:\n","                    print(f\"    ðŸ“‹ Faithfulness: {rag_metrics['avg_faithfulness']:.3f}\")\n","                if 'avg_bert_f1' in rag_metrics:\n","                    print(f\"    ðŸŽ¯ BERT F1: {rag_metrics['avg_bert_f1']:.3f}\")\n","            else:\n","                print(f\"  ðŸ¤– RAG Metrics: âŒ\")\n","\n","        # Overall comparison\n","        print(f\"\\nðŸ† OVERALL:\")\n","        best_f1 = (\"\", 0)\n","        best_score = (\"\", 0)\n","\n","        for model_name, model_data in results_data.items():\n","            after_metrics = model_data.get('avg_after_metrics', {})\n","            f1 = after_metrics.get('f1@5', 0)\n","            score = after_metrics.get('model_avg_score', 0)\n","\n","            if f1 > best_f1[1]:\n","                best_f1 = (model_name, f1)\n","            if score > best_score[1]:\n","                best_score = (model_name, score)\n","\n","        print(f\"  ðŸ¥‡ Best F1@5: {best_f1[0]} ({best_f1[1]:.3f})\")\n","        print(f\"  ðŸ“Š Best Score: {best_score[0]} ({best_score[1]:.3f})\")\n","\n","        # Methodology info\n","        data_verification = final_results.get('evaluation_info', {}).get('data_verification', {})\n","        print(f\"\\nðŸ”¬ VERIFICATION:\")\n","        print(f\"  âœ… Real data: {data_verification.get('is_real_data', False)}\")\n","        print(f\"  ðŸ“Š Framework: {data_verification.get('rag_framework', 'N/A')}\")\n","        print(f\"  ðŸ”„ Method: {data_verification.get('reranking_method', 'N/A')}\")\n","\n","print(\"\\nðŸŽ‰ EVALUATION COMPLETE!\")"]},{"cell_type":"markdown","metadata":{"id":"JMU1nw-hG9b3"},"source":["## ðŸ§¹ 8. Cleanup"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"QLjtydu4G9b3"},"outputs":[],"source":["# Cleanup\n","data_pipeline.cleanup()\n","import gc\n","gc.collect()\n","\n","print(\"ðŸ§¹ Cleanup completed\")\n","print(\"ðŸŽ¯ Results ready for Streamlit import\")"]},{"cell_type":"code","source":["# Play an audio beep. Any audio URL will do.\n","from google.colab import output\n","output.eval_js('new Audio(\"https://upload.wikimedia.org/wikipedia/commons/0/05/Beep-09.ogg\").play()')"],"metadata":{"id":"-lE-TA4NWwHt"},"execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.12"},"colab":{"provenance":[],"machine_shape":"hm","gpuType":"T4"},"accelerator":"GPU"},"nbformat":4,"nbformat_minor":0}