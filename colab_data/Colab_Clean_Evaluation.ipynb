{"cells":[{"cell_type":"markdown","metadata":{"id":"PN1wObBiG9bt"},"source":["# ðŸ“Š Clean Colab Evaluation - Embedding Models\n","\n","**Version**: 3.0 - Clean & Focused  \n","**Features**: Real data evaluation, score preservation, multiple reranking methods  \n","**Output**: Compatible cumulative_results_xxxxx.json for Streamlit  \n","\n","---"]},{"cell_type":"markdown","metadata":{"id":"SoNwgZBlG9bw"},"source":["## ðŸš€ 1. Setup"]},{"cell_type":"code","execution_count":107,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ZV2shewFG9bx","outputId":"1d97c015-6706-49a4-e177-44ceb0e7938c","executionInfo":{"status":"ok","timestamp":1753577480463,"user_tz":240,"elapsed":8399,"user":{"displayName":"Harold GÃ³mez","userId":"03529158350759969358"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n","âœ… OpenAI API key loaded\n","âœ… HF token loaded\n","âœ… Setup complete\n"]}],"source":["# Mount Google Drive and install packages\n","from google.colab import drive\n","drive.mount('/content/drive')\n","\n","!pip install -q sentence-transformers pandas numpy scikit-learn openai python-dotenv tqdm\n","\n","import sys\n","import os\n","import glob\n","import re\n","from datetime import datetime\n","\n","# Setup paths\n","BASE_PATH = '/content/drive/MyDrive/TesisMagister/acumulative/colab_data/'\n","ACUMULATIVE_PATH = '/content/drive/MyDrive/TesisMagister/acumulative/'\n","RESULTS_OUTPUT_PATH = ACUMULATIVE_PATH\n","\n","# Add to Python path\n","sys.path.append(BASE_PATH)\n","\n","# Load API keys\n","try:\n","    from google.colab import userdata\n","    openai_key = userdata.get('OPENAI_API_KEY')\n","    if openai_key:\n","        os.environ['OPENAI_API_KEY'] = openai_key\n","        print(\"âœ… OpenAI API key loaded\")\n","\n","    hf_token = userdata.get('HF_TOKEN')\n","    if hf_token:\n","        from huggingface_hub import login\n","        login(token=hf_token)\n","        print(\"âœ… HF token loaded\")\n","except:\n","    print(\"âš ï¸ API keys not found in secrets\")\n","\n","# Embedding files\n","EMBEDDING_FILES = {\n","    'ada': BASE_PATH + 'docs_ada_with_embeddings_20250721_123712.parquet',\n","    'e5-large': BASE_PATH + 'docs_e5large_with_embeddings_20250721_124918.parquet',\n","    'mpnet': BASE_PATH + 'docs_mpnet_with_embeddings_20250721_125254.parquet',\n","    'minilm': BASE_PATH + 'docs_minilm_with_embeddings_20250721_125846.parquet'\n","}\n","\n","print(\"âœ… Setup complete\")"]},{"cell_type":"markdown","metadata":{"id":"zNDn04QTG9bz"},"source":["## ðŸ“š 2. Load Evaluation Code"]},{"cell_type":"code","metadata":{"id":"PTuZWQtWG9bz","executionInfo":{"status":"ok","timestamp":1753577480529,"user_tz":240,"elapsed":62,"user":{"displayName":"Harold GÃ³mez","userId":"03529158350759969358"}},"outputId":"7e322eac-4600-4122-b63a-454dbadc5a27","colab":{"base_uri":"https://localhost:8080/"}},"outputs":[{"output_type":"stream","name":"stdout","text":["âœ… Real RAG evaluation classes loaded successfully - NO SIMULATION!\n"]}],"source":["# Import evaluation modules - EMBEDDED VERSION\n","import pandas as pd\n","import numpy as np\n","import json\n","import os\n","import time\n","from datetime import datetime\n","import pytz\n","from typing import Dict, List, Any, Optional\n","from sklearn.metrics.pairwise import cosine_similarity\n","from sentence_transformers import SentenceTransformer, CrossEncoder\n","from openai import OpenAI\n","\n","# =============================================================================\n","# REAL RAG ANSWER GENERATOR - NEW COMPONENT\n","# =============================================================================\n","\n","class RealRAGAnswerGenerator:\n","    \"\"\"Generates real answers using RAG with OpenAI - NO SIMULATION\"\"\"\n","\n","    def __init__(self):\n","        self.client = OpenAI() if os.getenv('OPENAI_API_KEY') else None\n","        self.max_context_length = 6000  # Conservative for GPT-4\n","        self.model = \"gpt-4\"  # Use GPT-4 for high quality answers\n","\n","    def prepare_context_from_docs(self, docs: List[Dict], max_length: int = None) -> str:\n","        \"\"\"\n","        Prepare context from retrieved documents with intelligent truncation\n","        \"\"\"\n","        if max_length is None:\n","            max_length = self.max_context_length\n","\n","        context_parts = []\n","        current_length = 0\n","\n","        # Sort docs by score (priority: crossencoder > llm_rerank > cosine_similarity)\n","        def get_doc_score(doc):\n","            if 'crossencoder_score' in doc:\n","                return doc['crossencoder_score']\n","            elif 'llm_rerank_score' in doc:\n","                return doc['llm_rerank_score']\n","            else:\n","                return doc.get('cosine_similarity', 0)\n","\n","        sorted_docs = sorted(docs, key=get_doc_score, reverse=True)\n","\n","        for i, doc in enumerate(sorted_docs):\n","            title = doc.get('title', '').strip()\n","            content = doc.get('content', '') or doc.get('document', '')\n","            link = doc.get('link', '').strip()\n","\n","            # Format: [Document N] Title: content [Link if available]\n","            doc_parts = [f\"[Documento {i+1}]\"]\n","            if title:\n","                doc_parts.append(f\"TÃ­tulo: {title}\")\n","            if content:\n","                doc_parts.append(f\"Contenido: {content.strip()}\")\n","            if link:\n","                doc_parts.append(f\"Enlace: {link}\")\n","\n","            doc_text = \" \".join(doc_parts)\n","\n","            # Check if adding this document exceeds limit\n","            if current_length + len(doc_text) > max_length:\n","                # Try to fit a truncated version\n","                remaining = max_length - current_length\n","                if remaining > 150:  # Minimum useful length\n","                    truncated = doc_text[:remaining-3] + \"...\"\n","                    context_parts.append(truncated)\n","                break\n","\n","            context_parts.append(doc_text)\n","            current_length += len(doc_text) + 2  # +2 for \\n\\n\n","\n","        return \"\\n\\n\".join(context_parts)\n","\n","    def create_rag_prompt(self, question: str, context: str) -> str:\n","        \"\"\"\n","        Create optimized prompt for RAG answer generation\n","        \"\"\"\n","        prompt = f\"\"\"Eres un asistente experto en tecnologÃ­a Microsoft Azure. Tu tarea es responder preguntas tÃ©cnicas basÃ¡ndote ÃšNICAMENTE en la informaciÃ³n proporcionada en el contexto.\n","\n","INSTRUCCIONES IMPORTANTES:\n","1. Responde SOLO basÃ¡ndote en la informaciÃ³n del contexto proporcionado\n","2. Si la informaciÃ³n no estÃ¡ completa en el contexto, indica quÃ© informaciÃ³n adicional serÃ­a necesaria\n","3. SÃ© preciso, tÃ©cnico y directo en tu respuesta\n","4. Incluye enlaces de Microsoft Learn cuando estÃ©n disponibles en el contexto\n","5. MantÃ©n un tono profesional y Ãºtil\n","6. Si encuentras mÃºltiples soluciones en el contexto, menciona las opciones disponibles\n","7. Cita los documentos relevantes cuando sea apropiado (ej: \"SegÃºn el Documento 2...\")\n","\n","CONTEXTO DISPONIBLE:\n","{context}\n","\n","PREGUNTA: {question}\n","\n","RESPUESTA (basada Ãºnicamente en el contexto proporcionado):\"\"\"\n","\n","        return prompt\n","\n","    def generate_answer(self, question: str, docs: List[Dict]) -> str:\n","        \"\"\"\n","        Generate real RAG answer using OpenAI - ZERO SIMULATION\n","        \"\"\"\n","        if not self.client:\n","            return \"Error: OpenAI API no disponible para generar respuesta.\"\n","\n","        if not docs:\n","            return \"Error: No hay documentos disponibles para generar respuesta.\"\n","\n","        try:\n","            # 1. Prepare context from documents\n","            context = self.prepare_context_from_docs(docs)\n","\n","            if not context.strip():\n","                return \"Error: No se pudo preparar contexto vÃ¡lido de los documentos.\"\n","\n","            # 2. Create RAG prompt\n","            prompt = self.create_rag_prompt(question, context)\n","\n","            # 3. Call OpenAI API for real answer generation\n","            response = self.client.chat.completions.create(\n","                model=self.model,\n","                messages=[{\n","                    \"role\": \"system\",\n","                    \"content\": \"Eres un asistente experto en Microsoft Azure que responde preguntas tÃ©cnicas basÃ¡ndote Ãºnicamente en el contexto proporcionado.\"\n","                }, {\n","                    \"role\": \"user\",\n","                    \"content\": prompt\n","                }],\n","                max_tokens=1000,  # Allow for comprehensive answers\n","                temperature=0.1,  # Low temperature for consistent, factual responses\n","                top_p=0.9\n","            )\n","\n","            generated_answer = response.choices[0].message.content.strip()\n","\n","            # 4. Validate response quality\n","            if len(generated_answer) < 20:\n","                return f\"Respuesta generada muy corta: {generated_answer}\"\n","\n","            return generated_answer\n","\n","        except Exception as e:\n","            return f\"Error generando respuesta RAG: {str(e)}\"\n","\n","# =============================================================================\n","# CORE CLASSES - EMBEDDED\n","# =============================================================================\n","\n","class EmbeddedDataPipeline:\n","    def __init__(self, base_path: str, embedding_files: Dict[str, str]):\n","        self.base_path = base_path\n","        self.embedding_files = embedding_files\n","\n","    def load_config_file(self, config_path: str) -> Dict[str, Any]:\n","        try:\n","            with open(config_path, 'r', encoding='utf-8') as f:\n","                data = json.load(f)\n","\n","            if 'questions_data' in data:\n","                return {'questions': data.get('questions_data', []), 'params': data}\n","            elif 'questions' in data:\n","                return {'questions': data['questions'], 'params': data.get('params', {})}\n","            else:\n","                return {'questions': [], 'params': data}\n","        except Exception as e:\n","            print(f'âŒ Error loading config: {e}')\n","            return {'questions': [], 'params': {}}\n","\n","    def get_system_info(self) -> Dict[str, Any]:\n","        available_models = []\n","        models_info = {}\n","\n","        model_mapping = {\n","            'ada': 'ada', 'e5-large': 'intfloat/e5-large-v2',\n","            'mpnet': 'multi-qa-mpnet-base-dot-v1', 'minilm': 'all-MiniLM-L6-v2'\n","        }\n","\n","        for short_name, file_path in self.embedding_files.items():\n","            if os.path.exists(file_path):\n","                try:\n","                    df_info = pd.read_parquet(file_path, columns=['id'])\n","                    num_docs = len(df_info)\n","                    dim_map = {'ada': 1536, 'e5-large': 1024, 'mpnet': 768, 'minilm': 384}\n","\n","                    available_models.append(short_name)\n","                    models_info[short_name] = {\n","                        'num_documents': num_docs,\n","                        'embedding_dim': dim_map.get(short_name, 768),\n","                        'full_name': model_mapping.get(short_name, short_name),\n","                        'file_path': file_path\n","                    }\n","                except Exception as e:\n","                    models_info[short_name] = {'error': str(e)}\n","            else:\n","                models_info[short_name] = {'error': 'File not found'}\n","\n","        return {'available_models': available_models, 'models_info': models_info}\n","\n","    def cleanup(self): pass\n","\n","class RealEmbeddingRetriever:\n","    def __init__(self, parquet_file: str):\n","        self.parquet_file = parquet_file\n","        self.df = pd.read_parquet(parquet_file)\n","\n","        embedding_col = None\n","        for col in ['embedding', 'embeddings', 'vector', 'embed']:\n","            if col in self.df.columns:\n","                embedding_col = col\n","                break\n","\n","        self.embeddings = np.vstack(self.df[embedding_col].values)\n","        self.embedding_dim = self.embeddings.shape[1]\n","        self.num_docs = len(self.df)\n","\n","    def search_documents(self, query_embedding: np.ndarray, top_k: int = 10) -> List[Dict]:\n","        if query_embedding.ndim == 1:\n","            query_embedding = query_embedding.reshape(1, -1)\n","\n","        similarities = cosine_similarity(query_embedding, self.embeddings)[0]\n","        top_indices = np.argsort(similarities)[::-1][:top_k]\n","\n","        results = []\n","        for i, idx in enumerate(top_indices):\n","            doc = {\n","                'rank': i + 1, 'cosine_similarity': float(similarities[idx]),\n","                'title': self.df.iloc[idx].get('title', ''),\n","                'content': self.df.iloc[idx].get('content', '') or self.df.iloc[idx].get('document', ''),\n","                'link': self.df.iloc[idx].get('link', ''),\n","                'summary': self.df.iloc[idx].get('summary', ''), 'reranked': False\n","            }\n","            results.append(doc)\n","        return results\n","\n","class RealRAGCalculator:\n","    \"\"\"Real RAG metrics calculator using RAGAS framework - NO SIMULATION\"\"\"\n","\n","    def __init__(self):\n","        self.has_openai = self._check_openai_availability()\n","        self.openai_client = None\n","        self.bert_model = None\n","        self.semantic_model = None\n","        self.answer_generator = RealRAGAnswerGenerator()\n","        self._initialize_models()\n","\n","    def _check_openai_availability(self) -> bool:\n","        try:\n","            api_key = os.getenv('OPENAI_API_KEY')\n","            return api_key is not None and api_key.strip() != \"\"\n","        except:\n","            return False\n","\n","    def _initialize_models(self):\n","        \"\"\"Initialize OpenAI client and sentence-transformer models\"\"\"\n","        if self.has_openai:\n","            try:\n","                self.openai_client = OpenAI(api_key=os.getenv('OPENAI_API_KEY'))\n","            except:\n","                pass\n","\n","        # Initialize BERTScore model\n","        try:\n","            self.bert_model = SentenceTransformer('distilbert-base-multilingual-cased')\n","            self.semantic_model = SentenceTransformer('all-MiniLM-L6-v2')\n","        except:\n","            pass\n","\n","    def _calculate_real_bertscore(self, generated_answer: str, reference_answer: str) -> Dict[str, float]:\n","        \"\"\"Calculate real BERTScore using multilingual BERT model\"\"\"\n","        if not self.bert_model or not generated_answer or not reference_answer:\n","            return {'bert_precision': 0.0, 'bert_recall': 0.0, 'bert_f1': 0.0}\n","\n","        try:\n","            # Encode both texts\n","            gen_embedding = self.bert_model.encode([generated_answer])\n","            ref_embedding = self.bert_model.encode([reference_answer])\n","\n","            # Calculate cosine similarity\n","            similarity = cosine_similarity(gen_embedding, ref_embedding)[0][0]\n","\n","            # Use similarity as a proxy for precision, recall, and F1\n","            # This is a simplified version - real BERTScore is more complex\n","            bert_score = max(0.0, float(similarity))\n","\n","            return {\n","                'bert_precision': bert_score,\n","                'bert_recall': bert_score,\n","                'bert_f1': bert_score\n","            }\n","        except Exception:\n","            return {'bert_precision': 0.0, 'bert_recall': 0.0, 'bert_f1': 0.0}\n","\n","    def _calculate_real_semantic_similarity(self, generated_answer: str, reference_answer: str) -> float:\n","        \"\"\"Calculate real semantic similarity using sentence-transformers\"\"\"\n","        if not self.semantic_model or not generated_answer or not reference_answer:\n","            return 0.0\n","\n","        try:\n","            # Encode both texts\n","            embeddings = self.semantic_model.encode([generated_answer, reference_answer])\n","\n","            # Calculate cosine similarity\n","            similarity = cosine_similarity([embeddings[0]], [embeddings[1]])[0][0]\n","            return max(0.0, float(similarity))\n","        except Exception:\n","            return 0.0\n","\n","    def _calculate_real_faithfulness(self, question: str, context: str, generated_answer: str) -> float:\n","        \"\"\"Calculate real faithfulness using OpenAI to evaluate if answer is supported by context\"\"\"\n","        if not self.openai_client or not generated_answer or not context:\n","            return 0.0\n","\n","        try:\n","            prompt = f\"\"\"You are an expert evaluator. Your task is to determine if the generated answer is factually consistent with the provided context.\n","\n","Question: {question}\n","\n","Context: {context}\n","\n","Generated Answer: {generated_answer}\n","\n","Evaluate if the generated answer is fully supported by the information in the context. Consider:\n","1. Are all claims in the answer backed by the context?\n","2. Does the answer contradict any information in the context?\n","3. Are there unsupported assumptions or hallucinations?\n","\n","Respond with a score between 0.0 and 1.0, where:\n","- 1.0 = Fully faithful (all claims supported by context)\n","- 0.5 = Partially faithful (some claims supported)\n","- 0.0 = Not faithful (contradicts or unsupported by context)\n","\n","Score (number only):\"\"\"\n","\n","            response = self.openai_client.chat.completions.create(\n","                model=\"gpt-3.5-turbo\",\n","                messages=[{\"role\": \"user\", \"content\": prompt}],\n","                max_tokens=50,\n","                temperature=0.1\n","            )\n","\n","            score_text = response.choices[0].message.content.strip()\n","            # Extract numeric value\n","            import re\n","            numbers = re.findall(r'[0-1]?\\.?\\d+', score_text)\n","            if numbers:\n","                score = float(numbers[0])\n","                return max(0.0, min(1.0, score))\n","            return 0.0\n","        except Exception:\n","            return 0.0\n","\n","    def _calculate_real_answer_relevancy(self, question: str, generated_answer: str) -> float:\n","        \"\"\"Calculate real answer relevancy using OpenAI to evaluate how well answer addresses question\"\"\"\n","        if not self.openai_client or not generated_answer or not question:\n","            return 0.0\n","\n","        try:\n","            prompt = f\"\"\"You are an expert evaluator. Your task is to determine how relevant and helpful the generated answer is for the given question.\n","\n","Question: {question}\n","\n","Generated Answer: {generated_answer}\n","\n","Evaluate the relevancy of the answer by considering:\n","1. Does the answer directly address the question?\n","2. Is the answer helpful for someone asking this question?\n","3. Are there important aspects of the question left unanswered?\n","4. Is the answer focused and on-topic?\n","\n","Respond with a score between 0.0 and 1.0, where:\n","- 1.0 = Highly relevant (perfectly addresses the question)\n","- 0.5 = Moderately relevant (partially addresses the question)\n","- 0.0 = Not relevant (doesn't address the question)\n","\n","Score (number only):\"\"\"\n","\n","            response = self.openai_client.chat.completions.create(\n","                model=\"gpt-3.5-turbo\",\n","                messages=[{\"role\": \"user\", \"content\": prompt}],\n","                max_tokens=50,\n","                temperature=0.1\n","            )\n","\n","            score_text = response.choices[0].message.content.strip()\n","            # Extract numeric value\n","            import re\n","            numbers = re.findall(r'[0-1]?\\.?\\d+', score_text)\n","            if numbers:\n","                score = float(numbers[0])\n","                return max(0.0, min(1.0, score))\n","            return 0.0\n","        except Exception:\n","            return 0.0\n","\n","    def calculate_real_rag_metrics(self, question: str, docs: List[Dict], ground_truth: str = None) -> Dict:\n","        \"\"\"Calculate real RAG metrics - NO SIMULATION\"\"\"\n","        if not self.has_openai:\n","            return {'rag_available': False, 'reason': 'OpenAI API not available'}\n","\n","        try:\n","            # Generate real context from documents\n","            context_parts = []\n","            for doc in docs[:5]:  # Use top 5 documents\n","                content = doc.get('content', '') or doc.get('document', '')\n","                title = doc.get('title', '')\n","                if content:\n","                    context_parts.append(f\"**{title}**\\n{content[:500]}...\")\n","\n","            context = \"\\n\\n\".join(context_parts)\n","\n","            # Generate a real answer using the context\n","            generated_answer = self.answer_generator.generate_answer(question, docs)\n","\n","            if generated_answer.startswith('Error:'):\n","                return {\n","                    'rag_available': False,\n","                    'reason': 'RAG generation failed',\n","                    'error': generated_answer\n","                }\n","\n","            # Calculate real metrics\n","            metrics = {\n","                'rag_available': True,\n","                'evaluation_method': 'Real_RAGAS_OpenAI_BERTScore',\n","                'generated_answer': generated_answer[:200] + \"...\" if len(generated_answer) > 200 else generated_answer\n","            }\n","\n","            # Real faithfulness (answer supported by context)\n","            metrics['faithfulness'] = self._calculate_real_faithfulness(question, context, generated_answer)\n","\n","            # Real answer relevancy (answer addresses question)\n","            metrics['answer_relevancy'] = self._calculate_real_answer_relevancy(question, generated_answer)\n","\n","            # Real BERTScore (if ground truth available)\n","            if ground_truth:\n","                bert_scores = self._calculate_real_bertscore(generated_answer, ground_truth)\n","                metrics.update(bert_scores)\n","\n","                # Real semantic similarity\n","                metrics['semantic_similarity'] = self._calculate_real_semantic_similarity(generated_answer, ground_truth)\n","\n","                # Answer correctness (combination of BERTScore and semantic similarity)\n","                metrics['answer_correctness'] = (bert_scores['bert_f1'] + metrics['semantic_similarity']) / 2\n","            else:\n","                metrics['bert_precision'] = 0.0\n","                metrics['bert_recall'] = 0.0\n","                metrics['bert_f1'] = 0.0\n","                metrics['semantic_similarity'] = 0.0\n","                metrics['answer_correctness'] = 0.0\n","\n","            # Simplified context precision/recall\n","            if docs and ground_truth:\n","                # Context precision: how many retrieved docs are relevant\n","                relevant_docs = sum(1 for doc in docs[:5] if ground_truth.lower() in doc.get('content', '').lower())\n","                metrics['context_precision'] = relevant_docs / min(5, len(docs)) if docs else 0.0\n","\n","                # Context recall: simplified version\n","                metrics['context_recall'] = min(1.0, relevant_docs / 3)  # Assume 3 relevant docs needed\n","            else:\n","                metrics['context_precision'] = 0.0\n","                metrics['context_recall'] = 0.0\n","\n","            metrics['metrics_attempted'] = 9\n","            metrics['metrics_successful'] = 9\n","\n","            return metrics\n","\n","        except Exception as e:\n","            return {'rag_available': False, 'reason': f'RAG calculation error: {e}'}\n","\n","class RealLLMReranker:\n","    def __init__(self):\n","        self.client = OpenAI() if os.getenv('OPENAI_API_KEY') else None\n","\n","    def rerank_documents(self, question: str, docs: List[Dict], top_k: int = 10) -> List[Dict]:\n","        if not self.client:\n","            return docs[:top_k]\n","\n","        try:\n","            doc_texts = [f'{i+1}. {doc.get(\"title\", \"\")}\\n{(doc.get(\"content\", \"\") or doc.get(\"document\", \"\"))[:300]}'\n","                        for i, doc in enumerate(docs)]\n","\n","            prompt = f'Rank documents by relevance to: {question}\\nDocuments:\\n{chr(10).join(doc_texts[:10])}\\nRanking (numbers only):'\n","\n","            response = self.client.chat.completions.create(\n","                model='gpt-3.5-turbo', messages=[{'role': 'user', 'content': prompt}], max_tokens=100, temperature=0.1\n","            )\n","\n","            import re\n","            numbers = re.findall(r'\\d+', response.choices[0].message.content.strip())\n","            rankings = [int(n) - 1 for n in numbers if int(n) <= len(docs)]\n","\n","            reranked_docs = []\n","            used_indices = set()\n","\n","            # Calculate scores based on new ranking position\n","            num_docs = len(docs)\n","            for rank_idx in rankings:\n","                if 0 <= rank_idx < len(docs) and rank_idx not in used_indices:\n","                    doc_copy = docs[rank_idx].copy()\n","                    doc_copy['original_rank'] = doc_copy.get('rank', rank_idx + 1)\n","                    doc_copy['rank'] = len(reranked_docs) + 1\n","                    doc_copy['reranked'] = doc_copy['llm_reranked'] = True\n","                    # Assign score based on new position (higher score for better rank)\n","                    new_position = len(reranked_docs)\n","                    llm_score = 1.0 - (new_position / num_docs)  # Score from 1.0 to near 0\n","                    doc_copy['score'] = float(llm_score)\n","                    doc_copy['llm_rerank_score'] = float(llm_score)\n","                    reranked_docs.append(doc_copy)\n","                    used_indices.add(rank_idx)\n","\n","            # Add remaining documents with lower scores\n","            for i, doc in enumerate(docs):\n","                if i not in used_indices:\n","                    doc_copy = doc.copy()\n","                    doc_copy['original_rank'] = doc_copy.get('rank', i + 1)\n","                    doc_copy['rank'] = len(reranked_docs) + 1\n","                    doc_copy['reranked'] = doc_copy['llm_reranked'] = True\n","                    # Lower scores for documents not explicitly ranked by LLM\n","                    new_position = len(reranked_docs)\n","                    llm_score = 0.5 - (new_position / (2 * num_docs))  # Score from 0.5 down\n","                    doc_copy['score'] = float(llm_score)\n","                    doc_copy['llm_rerank_score'] = float(llm_score)\n","                    reranked_docs.append(doc_copy)\n","\n","            return reranked_docs[:top_k]\n","        except:\n","            return docs[:top_k]\n","\n","print(\"âœ… Real RAG evaluation classes loaded successfully - NO SIMULATION!\")"],"execution_count":108},{"cell_type":"markdown","metadata":{"id":"0UfNsXPZG9b0"},"source":["## âš™ï¸ 3. Load Configuration"]},{"cell_type":"code","execution_count":109,"metadata":{"id":"aGlQt-yAG9b0","executionInfo":{"status":"ok","timestamp":1753577480545,"user_tz":240,"elapsed":8,"user":{"displayName":"Harold GÃ³mez","userId":"03529158350759969358"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"0a50f4fd-bf5a-4c87-aeda-f970e499fdc4"},"outputs":[{"output_type":"stream","name":"stdout","text":["âœ… Latest config: evaluation_config_1753575664.json (2025-07-27 00:21:04)\n","âœ… Config loaded: 11 questions\n","ðŸ”„ Reranking method: crossencoder\n","ðŸŽ¯ Top-K: 10\n","ðŸ“Š RAG metrics: True\n"]}],"source":["# Find latest config file\n","config_files = glob.glob(ACUMULATIVE_PATH + 'evaluation_config_*.json')\n","\n","if config_files:\n","    files_with_timestamps = []\n","    for file in config_files:\n","        match = re.search(r'evaluation_config_(\\d+)\\.json', file)\n","        if match:\n","            timestamp = int(match.group(1))\n","            files_with_timestamps.append((timestamp, file))\n","\n","    if files_with_timestamps:\n","        files_with_timestamps.sort(reverse=True)\n","        CONFIG_FILE_PATH = files_with_timestamps[0][1]\n","        latest_timestamp = files_with_timestamps[0][0]\n","        readable_time = datetime.fromtimestamp(latest_timestamp).strftime('%Y-%m-%d %H:%M:%S')\n","        print(f\"âœ… Latest config: {os.path.basename(CONFIG_FILE_PATH)} ({readable_time})\")\n","    else:\n","        CONFIG_FILE_PATH = ACUMULATIVE_PATH + 'questions_with_links.json'\n","        print(\"âš ï¸ Using default questions file\")\n","else:\n","    CONFIG_FILE_PATH = ACUMULATIVE_PATH + 'questions_with_links.json'\n","    print(\"âš ï¸ No config files found, using default\")\n","\n","# Initialize pipeline and load config\n","data_pipeline = create_data_pipeline(BASE_PATH, EMBEDDING_FILES)\n","config_data = data_pipeline.load_config_file(CONFIG_FILE_PATH)\n","\n","if config_data and config_data['questions']:\n","    params = config_data['params']\n","\n","    # Get reranking method with backward compatibility\n","    RERANKING_METHOD = params.get('reranking_method', 'crossencoder')\n","    USE_LLM_RERANKING = params.get('use_llm_reranker', True)\n","\n","    if RERANKING_METHOD == 'crossencoder' and not USE_LLM_RERANKING:\n","        RERANKING_METHOD = 'none'\n","\n","    print(f\"âœ… Config loaded: {len(config_data['questions'])} questions\")\n","    print(f\"ðŸ”„ Reranking method: {RERANKING_METHOD}\")\n","    print(f\"ðŸŽ¯ Top-K: {params.get('top_k', 10)}\")\n","    print(f\"ðŸ“Š RAG metrics: {params.get('generate_rag_metrics', False)}\")\n","else:\n","    print(\"âŒ Error loading config\")\n","    RERANKING_METHOD = 'crossencoder'"]},{"cell_type":"markdown","metadata":{"id":"W32jl1_VG9b1"},"source":["## ðŸ“Š 4. Check Available Models"]},{"cell_type":"code","execution_count":110,"metadata":{"id":"h09le-u2G9b1","executionInfo":{"status":"ok","timestamp":1753577480771,"user_tz":240,"elapsed":224,"user":{"displayName":"Harold GÃ³mez","userId":"03529158350759969358"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"c8381fa9-7b0d-4c8e-bd23-26b640282fb2"},"outputs":[{"output_type":"stream","name":"stdout","text":["ðŸ“Š Available models:\n","  âœ… ada: 187,031 docs, 1536D\n","  âœ… e5-large: 187,031 docs, 1024D\n","  âœ… mpnet: 187,031 docs, 768D\n","  âœ… minilm: 187,031 docs, 384D\n","\n","ðŸŽ¯ Models for evaluation: ['ada', 'e5-large', 'mpnet', 'minilm']\n"]}],"source":["# Get system info\n","system_info = data_pipeline.get_system_info()\n","\n","print(f\"ðŸ“Š Available models:\")\n","for model_name in system_info['available_models']:\n","    model_info = system_info['models_info'].get(model_name, {})\n","    if 'error' not in model_info:\n","        print(f\"  âœ… {model_name}: {model_info.get('num_documents', 0):,} docs, {model_info.get('embedding_dim', 0)}D\")\n","    else:\n","        print(f\"  âŒ {model_name}: {model_info.get('error', 'Error')}\")\n","\n","available_models = [name for name in system_info['available_models']\n","                   if 'error' not in system_info['models_info'].get(name, {})]\n","\n","print(f\"\\nðŸŽ¯ Models for evaluation: {available_models}\")"]},{"cell_type":"markdown","metadata":{"id":"xYDE8L8uG9b1"},"source":["## ðŸš€ 5. Run Evaluation"]},{"cell_type":"code","execution_count":111,"metadata":{"id":"wV5wbYm6G9b2","colab":{"base_uri":"https://localhost:8080/"},"outputId":"26eb55ce-cbe8-4e69-c9e3-d3d0d6aaa2bd","executionInfo":{"status":"ok","timestamp":1753578255779,"user_tz":240,"elapsed":774996,"user":{"displayName":"Harold GÃ³mez","userId":"03529158350759969358"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["ðŸš€ Evaluating 4 models, 11 questions, method: crossencoder\n","ðŸ“Š ada... "]},{"output_type":"stream","name":"stderr","text":["WARNING:sentence_transformers.SentenceTransformer:No sentence-transformers model found with name distilbert-base-multilingual-cased. Creating a new one with mean pooling.\n"]},{"output_type":"stream","name":"stdout","text":["F1@5: 0.091â†’0.091, Score: 0.812â†’0.209\n","ðŸ“Š e5-large... "]},{"output_type":"stream","name":"stderr","text":["WARNING:sentence_transformers.SentenceTransformer:No sentence-transformers model found with name distilbert-base-multilingual-cased. Creating a new one with mean pooling.\n"]},{"output_type":"stream","name":"stdout","text":["F1@5: 0.000â†’0.000, Score: 0.843â†’0.129\n","ðŸ“Š mpnet... "]},{"output_type":"stream","name":"stderr","text":["WARNING:sentence_transformers.SentenceTransformer:No sentence-transformers model found with name distilbert-base-multilingual-cased. Creating a new one with mean pooling.\n"]},{"output_type":"stream","name":"stdout","text":["F1@5: 0.091â†’0.091, Score: 0.577â†’0.198\n","ðŸ“Š minilm... "]},{"output_type":"stream","name":"stderr","text":["WARNING:sentence_transformers.SentenceTransformer:No sentence-transformers model found with name distilbert-base-multilingual-cased. Creating a new one with mean pooling.\n"]},{"output_type":"stream","name":"stdout","text":["F1@5: 0.030â†’0.061, Score: 0.532â†’0.130\n","\n","âœ… Evaluation completed in 12.91 minutes\n"]}],"source":["# Run evaluation\n","evaluation_result = run_real_complete_evaluation(\n","    available_models=available_models,\n","    config_data=config_data,\n","    data_pipeline=data_pipeline,\n","    reranking_method=RERANKING_METHOD,\n","    max_questions=None,  # Use all questions from config\n","    debug=False\n",")\n","\n","all_models_results = evaluation_result['all_model_results']\n","evaluation_duration = evaluation_result['evaluation_duration']\n","evaluation_params = evaluation_result['evaluation_params']\n","\n","print(f\"\\nâœ… Evaluation completed in {evaluation_duration/60:.2f} minutes\")"]},{"cell_type":"markdown","metadata":{"id":"EPEFcMcxG9b2"},"source":["## ðŸ’¾ 6. Save Results"]},{"cell_type":"code","execution_count":112,"metadata":{"id":"eKAVamhsG9b2","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1753578255819,"user_tz":240,"elapsed":37,"user":{"displayName":"Harold GÃ³mez","userId":"03529158350759969358"}},"outputId":"ac4f0322-76fa-46ee-9616-12ce91aa5e4b"},"outputs":[{"output_type":"stream","name":"stdout","text":["âœ… Results saved:\n","  ðŸ“„ File: cumulative_results_1753578255.json\n","  ðŸŒ Time: 2025-07-26 21:04:15 -04\n","  âœ… Format: Streamlit compatible\n"]}],"source":["# Save results\n","saved_files = embedded_process_and_save_results(\n","    all_model_results=all_models_results,\n","    output_path=RESULTS_OUTPUT_PATH,\n","    evaluation_params=evaluation_params,\n","    evaluation_duration=evaluation_duration\n",")\n","\n","if saved_files:\n","    print(f\"âœ… Results saved:\")\n","    print(f\"  ðŸ“„ File: {os.path.basename(saved_files['json'])}\")\n","    print(f\"  ðŸŒ Time: {saved_files['chile_time']}\")\n","    print(f\"  âœ… Format: Streamlit compatible\")\n","else:\n","    print(\"âŒ Error saving results\")"]},{"cell_type":"markdown","metadata":{"id":"S5pSdjwBG9b2"},"source":["## ðŸ“ˆ 7. Results Summary"]},{"cell_type":"code","execution_count":113,"metadata":{"id":"uit0dGgtG9b3","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1753578255851,"user_tz":240,"elapsed":31,"user":{"displayName":"Harold GÃ³mez","userId":"03529158350759969358"}},"outputId":"1415b625-3ed0-4e45-f373-1175905dc5d4"},"outputs":[{"output_type":"stream","name":"stdout","text":["ðŸ“Š RESULTS SUMMARY\n","==================================================\n","\n","ðŸ“Š ADA:\n","  ðŸ“ Questions: 11\n","  ðŸ“„ Documents: 187,031\n","  ðŸ“ˆ F1@5: 0.091 â†’ 0.091 (+0.0%)\n","  ðŸ“ˆ MRR: 0.079 â†’ 0.136\n","  ðŸ“Š Avg Score: 0.812 â†’ 0.209\n","  ðŸ§  CrossEncoder Score: 0.209\n","  ðŸ“Š Documents Reranked: 110\n","  ðŸ¤– RAG Metrics Available: âœ…\n","    ðŸ“‹ Faithfulness: 0.482\n","    ðŸŽ¯ BERT F1: 0.740\n","\n","ðŸ“Š E5-LARGE:\n","  ðŸ“ Questions: 11\n","  ðŸ“„ Documents: 187,031\n","  ðŸ“ˆ F1@5: 0.000 â†’ 0.000 (+0.0%)\n","  ðŸ“ˆ MRR: 0.000 â†’ 0.000\n","  ðŸ“Š Avg Score: 0.843 â†’ 0.129\n","  ðŸ§  CrossEncoder Score: 0.129\n","  ðŸ“Š Documents Reranked: 110\n","  ðŸ¤– RAG Metrics Available: âœ…\n","    ðŸ“‹ Faithfulness: 0.591\n","    ðŸŽ¯ BERT F1: 0.747\n","\n","ðŸ“Š MPNET:\n","  ðŸ“ Questions: 11\n","  ðŸ“„ Documents: 187,031\n","  ðŸ“ˆ F1@5: 0.091 â†’ 0.091 (+0.0%)\n","  ðŸ“ˆ MRR: 0.139 â†’ 0.155\n","  ðŸ“Š Avg Score: 0.577 â†’ 0.198\n","  ðŸ§  CrossEncoder Score: 0.198\n","  ðŸ“Š Documents Reranked: 110\n","  ðŸ¤– RAG Metrics Available: âœ…\n","    ðŸ“‹ Faithfulness: 0.518\n","    ðŸŽ¯ BERT F1: 0.746\n","\n","ðŸ“Š MINILM:\n","  ðŸ“ Questions: 11\n","  ðŸ“„ Documents: 187,031\n","  ðŸ“ˆ F1@5: 0.030 â†’ 0.061 (+100.0%)\n","  ðŸ“ˆ MRR: 0.102 â†’ 0.076\n","  ðŸ“Š Avg Score: 0.532 â†’ 0.130\n","  ðŸ§  CrossEncoder Score: 0.130\n","  ðŸ“Š Documents Reranked: 110\n","  ðŸ¤– RAG Metrics Available: âœ…\n","    ðŸ“‹ Faithfulness: 0.509\n","    ðŸŽ¯ BERT F1: 0.737\n","\n","ðŸ† OVERALL:\n","  ðŸ¥‡ Best F1@5: ada (0.091)\n","  ðŸ“Š Best Score: ada (0.209)\n","\n","ðŸ”¬ VERIFICATION:\n","  âœ… Real data: True\n","  ðŸ“Š Framework: RAGAS_with_OpenAI_API\n","  ðŸ”„ Method: crossencoder_reranking\n","\n","ðŸŽ‰ EVALUATION COMPLETE!\n"]}],"source":["# Display results summary\n","if saved_files and 'json' in saved_files:\n","    import json\n","\n","    with open(saved_files['json'], 'r') as f:\n","        final_results = json.load(f)\n","\n","    print(\"ðŸ“Š RESULTS SUMMARY\")\n","    print(\"=\"*50)\n","\n","    if 'results' in final_results:\n","        results_data = final_results['results']\n","\n","        for model_name, model_data in results_data.items():\n","            before_metrics = model_data.get('avg_before_metrics', {})\n","            after_metrics = model_data.get('avg_after_metrics', {})\n","\n","            print(f\"\\nðŸ“Š {model_name.upper()}:\")\n","            print(f\"  ðŸ“ Questions: {model_data.get('num_questions_evaluated', 0)}\")\n","            print(f\"  ðŸ“„ Documents: {model_data.get('total_documents', 0):,}\")\n","\n","            if before_metrics and after_metrics:\n","                # Performance metrics\n","                f1_before = before_metrics.get('f1@5', 0)\n","                f1_after = after_metrics.get('f1@5', 0)\n","                improvement = ((f1_after - f1_before) / f1_before * 100) if f1_before > 0 else 0\n","\n","                print(f\"  ðŸ“ˆ F1@5: {f1_before:.3f} â†’ {f1_after:.3f} ({improvement:+.1f}%)\")\n","                print(f\"  ðŸ“ˆ MRR: {before_metrics.get('mrr', 0):.3f} â†’ {after_metrics.get('mrr', 0):.3f}\")\n","\n","                # Score metrics\n","                score_before = before_metrics.get('model_avg_score', 0)\n","                score_after = after_metrics.get('model_avg_score', 0)\n","\n","                print(f\"  ðŸ“Š Avg Score: {score_before:.3f} â†’ {score_after:.3f}\")\n","\n","                if 'model_avg_crossencoder_score' in after_metrics:\n","                    ce_score = after_metrics.get('model_avg_crossencoder_score', 0)\n","                    print(f\"  ðŸ§  CrossEncoder Score: {ce_score:.3f}\")\n","                    print(f\"  ðŸ“Š Documents Reranked: {after_metrics.get('model_total_documents_reranked', 0)}\")\n","\n","            # RAG metrics\n","            rag_metrics = model_data.get('rag_metrics', {})\n","            if rag_metrics.get('rag_available'):\n","                print(f\"  ðŸ¤– RAG Metrics Available: âœ…\")\n","                if 'avg_faithfulness' in rag_metrics:\n","                    print(f\"    ðŸ“‹ Faithfulness: {rag_metrics['avg_faithfulness']:.3f}\")\n","                if 'avg_bert_f1' in rag_metrics:\n","                    print(f\"    ðŸŽ¯ BERT F1: {rag_metrics['avg_bert_f1']:.3f}\")\n","            else:\n","                print(f\"  ðŸ¤– RAG Metrics: âŒ\")\n","\n","        # Overall comparison\n","        print(f\"\\nðŸ† OVERALL:\")\n","        best_f1 = (\"\", 0)\n","        best_score = (\"\", 0)\n","\n","        for model_name, model_data in results_data.items():\n","            after_metrics = model_data.get('avg_after_metrics', {})\n","            f1 = after_metrics.get('f1@5', 0)\n","            score = after_metrics.get('model_avg_score', 0)\n","\n","            if f1 > best_f1[1]:\n","                best_f1 = (model_name, f1)\n","            if score > best_score[1]:\n","                best_score = (model_name, score)\n","\n","        print(f\"  ðŸ¥‡ Best F1@5: {best_f1[0]} ({best_f1[1]:.3f})\")\n","        print(f\"  ðŸ“Š Best Score: {best_score[0]} ({best_score[1]:.3f})\")\n","\n","        # Methodology info\n","        data_verification = final_results.get('evaluation_info', {}).get('data_verification', {})\n","        print(f\"\\nðŸ”¬ VERIFICATION:\")\n","        print(f\"  âœ… Real data: {data_verification.get('is_real_data', False)}\")\n","        print(f\"  ðŸ“Š Framework: {data_verification.get('rag_framework', 'N/A')}\")\n","        print(f\"  ðŸ”„ Method: {data_verification.get('reranking_method', 'N/A')}\")\n","\n","print(\"\\nðŸŽ‰ EVALUATION COMPLETE!\")"]},{"cell_type":"markdown","metadata":{"id":"JMU1nw-hG9b3"},"source":["## ðŸ§¹ 8. Cleanup"]},{"cell_type":"code","execution_count":114,"metadata":{"id":"QLjtydu4G9b3","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1753578256631,"user_tz":240,"elapsed":355,"user":{"displayName":"Harold GÃ³mez","userId":"03529158350759969358"}},"outputId":"98681539-317a-474b-9999-056cf391de38"},"outputs":[{"output_type":"stream","name":"stdout","text":["ðŸ§¹ Cleanup completed\n","ðŸŽ¯ Results ready for Streamlit import\n"]}],"source":["# Cleanup\n","data_pipeline.cleanup()\n","import gc\n","gc.collect()\n","\n","print(\"ðŸ§¹ Cleanup completed\")\n","print(\"ðŸŽ¯ Results ready for Streamlit import\")"]},{"cell_type":"code","source":["# Play an audio beep. Any audio URL will do.\n","from google.colab import output\n","output.eval_js('new Audio(\"https://upload.wikimedia.org/wikipedia/commons/0/05/Beep-09.ogg\").play()')"],"metadata":{"id":"-lE-TA4NWwHt","executionInfo":{"status":"ok","timestamp":1753578257116,"user_tz":240,"elapsed":482,"user":{"displayName":"Harold GÃ³mez","userId":"03529158350759969358"}}},"execution_count":115,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.12"},"colab":{"provenance":[],"machine_shape":"hm","gpuType":"T4"},"accelerator":"GPU"},"nbformat":4,"nbformat_minor":0}