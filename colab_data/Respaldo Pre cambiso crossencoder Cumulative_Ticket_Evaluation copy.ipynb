{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3lJUissrFiJ9"
   },
   "source": [
    "# 📊 Cumulative Process of Tickets - RAG Evaluation\n",
    "\n",
    "**Version**: 1.0  \n",
    "**Features**: Real data evaluation, complete RAG metrics, multi-model comparison  \n",
    "**Output**: Compatible cumulative_results_xxxxx.json for Streamlit visualization  \n",
    "\n",
    "**Complete Evaluation Pipeline:**\n",
    "- Real embedding generation using actual models (Ada, E5-Large, MPNet, MiniLM)\n",
    "- CrossEncoder reranking with Min-Max normalization\n",
    "- Complete RAGAS metrics (faithfulness, answer_relevancy, answer_correctness, context_precision, context_recall, semantic_similarity)\n",
    "- Complete BERTScore metrics (bert_precision, bert_recall, bert_f1)\n",
    "- Proper URL normalization for ground truth matching\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Tqg8lFZHFiKB"
   },
   "source": [
    "## 🚀 1. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3926,
     "status": "ok",
     "timestamp": 1760198764590,
     "user": {
      "displayName": "Harold Gómez",
      "userId": "03529158350759969358"
     },
     "user_tz": 180
    },
    "id": "Cz2kvqsExgTC",
    "outputId": "f31ad06e-ca0c-443d-f89f-645554b940e4"
   },
   "outputs": [],
   "source": [
    "!pip install torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 10570,
     "status": "ok",
     "timestamp": 1760198775163,
     "user": {
      "displayName": "Harold Gómez",
      "userId": "03529158350759969358"
     },
     "user_tz": 180
    },
    "id": "H5gCuIOIFiKB",
    "outputId": "ab2694a2-4bd6-4a24-b3b8-2f1f247513ea"
   },
   "outputs": [],
   "source": [
    "# Mount Google Drive and install packages\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "!pip install -q sentence-transformers pandas numpy scikit-learn openai python-dotenv tqdm torch bert-score\n",
    "\n",
    "import sys\n",
    "import os\n",
    "import glob\n",
    "import re\n",
    "from datetime import datetime\n",
    "\n",
    "# Setup paths\n",
    "BASE_PATH = '/content/drive/MyDrive/TesisMagister/acumulative/colab_data/'\n",
    "ACUMULATIVE_PATH = '/content/drive/MyDrive/TesisMagister/acumulative/'\n",
    "RESULTS_OUTPUT_PATH = ACUMULATIVE_PATH\n",
    "\n",
    "# Add to Python path\n",
    "sys.path.append(BASE_PATH)\n",
    "\n",
    "# Load API keys\n",
    "try:\n",
    "    from google.colab import userdata\n",
    "    openai_key = userdata.get('OPENAI_API_KEY')\n",
    "    if openai_key:\n",
    "        os.environ['OPENAI_API_KEY'] = openai_key\n",
    "        print(\"✅ OpenAI API key loaded\")\n",
    "\n",
    "    hf_token = userdata.get('HF_TOKEN')\n",
    "    if hf_token:\n",
    "        from huggingface_hub import login\n",
    "        login(token=hf_token)\n",
    "        print(\"✅ HF token loaded\")\n",
    "except:\n",
    "    print(\"⚠️ API keys not found in secrets\")\n",
    "\n",
    "# Embedding files\n",
    "EMBEDDING_FILES = {\n",
    "    'ada': BASE_PATH + 'docs_ada_with_embeddings_20250721_123712.parquet',\n",
    "    'e5-large': BASE_PATH + 'docs_e5large_with_embeddings_20250721_124918.parquet',\n",
    "    'mpnet': BASE_PATH + 'docs_mpnet_with_embeddings_20250721_125254.parquet',\n",
    "    'minilm': BASE_PATH + 'docs_minilm_with_embeddings_20250721_125846.parquet'\n",
    "}\n",
    "\n",
    "print(\"✅ Setup complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2440,
     "status": "ok",
     "timestamp": 1760198777612,
     "user": {
      "displayName": "Harold Gómez",
      "userId": "03529158350759969358"
     },
     "user_tz": 180
    },
    "id": "M_gHwXNrxFTf",
    "outputId": "a9af2d35-ed84-481b-8565-50b19a20df17"
   },
   "outputs": [],
   "source": [
    "# GPU Memory Configuration (AÑADIR COMO NUEVA CELDA)\n",
    "import torch\n",
    "import gc\n",
    "import os\n",
    "\n",
    "# Configurar PyTorch para mejor manejo de memoria\n",
    "os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'expandable_segments:True'\n",
    "\n",
    "# Limitar uso de GPU al 80%\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.set_per_process_memory_fraction(0.8)\n",
    "    print(f\"GPU disponible: {torch.cuda.get_device_name()}\")\n",
    "    print(f\"Memoria total: {torch.cuda.get_device_properties(0).total_memory/1024**3:.2f} GB\")\n",
    "\n",
    "def clear_gpu_memory():\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "    print(\"🧹 GPU memory cleared\")\n",
    "\n",
    "def check_gpu_memory():\n",
    "    if torch.cuda.is_available():\n",
    "        allocated = torch.cuda.memory_allocated()/1024**3\n",
    "        reserved = torch.cuda.memory_reserved()/1024**3\n",
    "        print(f\"🔍 GPU Memory - Allocated: {allocated:.2f}GB, Reserved: {reserved:.2f}GB\")\n",
    "\n",
    "print(\"✅ GPU configuration loaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "o1RV84OgFiKD"
   },
   "source": [
    "## 📚 2. Load Evaluation Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 8544,
     "status": "ok",
     "timestamp": 1760198786208,
     "user": {
      "displayName": "Harold Gómez",
      "userId": "03529158350759969358"
     },
     "user_tz": 180
    },
    "id": "zXeuvvmvFiKD",
    "outputId": "d10de54c-1648-4edb-de1f-f9a1bfa66d6b"
   },
   "outputs": [],
   "source": [
    "# Complete evaluation code\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import time\n",
    "import pytz\n",
    "import hashlib\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sentence_transformers import SentenceTransformer, CrossEncoder\n",
    "from openai import OpenAI\n",
    "from urllib.parse import urlparse, urlunparse\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# ===========================\n",
    "# OPENAI CACHE SYSTEM\n",
    "# ===========================\n",
    "\n",
    "class OpenAICache:\n",
    "    \"\"\"Manages caching of OpenAI API responses to minimize costs\"\"\"\n",
    "\n",
    "    def __init__(self, cache_path: str):\n",
    "        self.cache_path = cache_path\n",
    "        self.cache = {\"metadata\": {}, \"cached_responses\": {}}\n",
    "        self.hits = 0\n",
    "        self.misses = 0\n",
    "        self.modified = False\n",
    "        self.load_cache()\n",
    "\n",
    "    def _create_hash(self, question: str, context_links: list = None, prompt_type: str = \"answer\") -> str:\n",
    "        \"\"\"Create unique hash for question + context combination\"\"\"\n",
    "        # Normalize context links\n",
    "        context_str = \"\"\n",
    "        if context_links:\n",
    "            sorted_links = sorted([normalize_url(link) for link in context_links if link])\n",
    "            context_str = \"|\".join(sorted_links)\n",
    "\n",
    "        # Create hash string\n",
    "        hash_input = f\"{prompt_type}:{question}:{context_str}\"\n",
    "        return hashlib.md5(hash_input.encode('utf-8')).hexdigest()\n",
    "\n",
    "    def load_cache(self):\n",
    "        \"\"\"Load existing cache from file\"\"\"\n",
    "        try:\n",
    "            if os.path.exists(self.cache_path):\n",
    "                with open(self.cache_path, 'r', encoding='utf-8') as f:\n",
    "                    self.cache = json.load(f)\n",
    "\n",
    "                total = len(self.cache.get('cached_responses', {}))\n",
    "                print(f\"✅ Loaded OpenAI cache: {total} cached entries\")\n",
    "            else:\n",
    "                print(\"📝 No existing cache found, creating new cache\")\n",
    "                self.cache = {\n",
    "                    \"metadata\": {\n",
    "                        \"cache_version\": \"1.0\",\n",
    "                        \"created_at\": datetime.now().isoformat()\n",
    "                    },\n",
    "                    \"cached_responses\": {}\n",
    "                }\n",
    "        except Exception as e:\n",
    "            print(f\"⚠️ Error loading cache: {e}, starting fresh\")\n",
    "            self.cache = {\"metadata\": {}, \"cached_responses\": {}}\n",
    "\n",
    "    def save_cache(self):\n",
    "        \"\"\"Save cache to file\"\"\"\n",
    "        if not self.modified:\n",
    "            return\n",
    "\n",
    "        try:\n",
    "            # Update metadata\n",
    "            self.cache[\"metadata\"][\"total_entries\"] = len(self.cache[\"cached_responses\"])\n",
    "            self.cache[\"metadata\"][\"last_updated\"] = datetime.now().isoformat()\n",
    "            self.cache[\"metadata\"][\"stats\"] = {\n",
    "                \"hits\": self.hits,\n",
    "                \"misses\": self.misses,\n",
    "                \"hit_rate\": self.hits / (self.hits + self.misses) if (self.hits + self.misses) > 0 else 0\n",
    "            }\n",
    "\n",
    "            with open(self.cache_path, 'w', encoding='utf-8') as f:\n",
    "                json.dump(self.cache, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "            print(f\"💾 Cache saved: {len(self.cache['cached_responses'])} entries\")\n",
    "            self.modified = False\n",
    "        except Exception as e:\n",
    "            print(f\"❌ Error saving cache: {e}\")\n",
    "\n",
    "    def get(self, question: str, context_links: list = None, prompt_type: str = \"answer\"):\n",
    "        \"\"\"Get cached response if exists\"\"\"\n",
    "        cache_key = self._create_hash(question, context_links, prompt_type)\n",
    "\n",
    "        if cache_key in self.cache[\"cached_responses\"]:\n",
    "            self.hits += 1\n",
    "            return self.cache[\"cached_responses\"][cache_key]\n",
    "\n",
    "        self.misses += 1\n",
    "        return None\n",
    "\n",
    "    def set(self, question: str, response_data: dict, context_links: list = None, prompt_type: str = \"answer\"):\n",
    "        \"\"\"Store response in cache\"\"\"\n",
    "        cache_key = self._create_hash(question, context_links, prompt_type)\n",
    "\n",
    "        self.cache[\"cached_responses\"][cache_key] = {\n",
    "            **response_data,\n",
    "            \"timestamp\": datetime.now().isoformat()\n",
    "        }\n",
    "        self.modified = True\n",
    "\n",
    "    def get_stats(self):\n",
    "        \"\"\"Get cache statistics\"\"\"\n",
    "        total = self.hits + self.misses\n",
    "        hit_rate = (self.hits / total * 100) if total > 0 else 0\n",
    "\n",
    "        return {\n",
    "            \"total_queries\": total,\n",
    "            \"cache_hits\": self.hits,\n",
    "            \"cache_misses\": self.misses,\n",
    "            \"hit_rate\": hit_rate,\n",
    "            \"estimated_cost_saved\": self.hits * 0.05  # Approx $0.05 per cached query\n",
    "        }\n",
    "\n",
    "# Global cache instance (will be initialized in evaluation)\n",
    "openai_cache = None\n",
    "\n",
    "# Global semantic similarity model (will be initialized in evaluation to avoid repeated loading)\n",
    "semantic_similarity_model = None\n",
    "\n",
    "def normalize_url(url: str) -> str:\n",
    "    \"\"\"\n",
    "    Normalizes a URL by removing query parameters and fragments (anchors).\n",
    "\n",
    "    Examples:\n",
    "        https://learn.microsoft.com/en-us/azure/storage/blobs/storage-blob-overview?view=azure-cli-latest#overview\n",
    "        -> https://learn.microsoft.com/en-us/azure/storage/blobs/storage-blob-overview\n",
    "\n",
    "        https://learn.microsoft.com/azure/virtual-machines/windows/quick-create-portal?tabs=windows10#create-vm\n",
    "        -> https://learn.microsoft.com/azure/virtual-machines/windows/quick-create-portal\n",
    "\n",
    "    Args:\n",
    "        url: The URL to normalize\n",
    "\n",
    "    Returns:\n",
    "        The normalized URL without query parameters and fragments\n",
    "    \"\"\"\n",
    "    if not url or not url.strip():\n",
    "        return \"\"\n",
    "\n",
    "    try:\n",
    "        # Parse the URL\n",
    "        parsed = urlparse(url.strip())\n",
    "\n",
    "        # Reconstruct without query parameters and fragments\n",
    "        normalized = urlunparse((\n",
    "            parsed.scheme,    # https\n",
    "            parsed.netloc,    # learn.microsoft.com\n",
    "            parsed.path,      # /en-us/azure/storage/blobs/storage-blob-overview\n",
    "            '',               # params (empty)\n",
    "            '',               # query (empty) - removes ?view=azure-cli-latest\n",
    "            ''                # fragment (empty) - removes #overview\n",
    "        ))\n",
    "\n",
    "        return normalized\n",
    "    except Exception as e:\n",
    "        # If parsing fails, return the original URL stripped\n",
    "        return url.strip()\n",
    "\n",
    "class RealEmbeddingGenerator:\n",
    "    \"\"\"Generates real embeddings using actual models\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.models = {}\n",
    "        self._load_models()\n",
    "\n",
    "    def _load_models(self):\n",
    "        \"\"\"Load sentence transformer models\"\"\"\n",
    "        model_configs = {\n",
    "            'e5-large': 'intfloat/e5-large-v2',\n",
    "            'mpnet': 'sentence-transformers/all-mpnet-base-v2',\n",
    "            'minilm': 'sentence-transformers/all-MiniLM-L6-v2'\n",
    "        }\n",
    "\n",
    "        for name, model_path in model_configs.items():\n",
    "            try:\n",
    "                self.models[name] = SentenceTransformer(model_path)\n",
    "                print(f\"✅ Loaded {name} model\")\n",
    "            except Exception as e:\n",
    "                print(f\"❌ Error loading {name}: {e}\")\n",
    "                self.models[name] = None\n",
    "\n",
    "    def validate_models(self):\n",
    "        \"\"\"Validate that all required models are loaded before evaluation\"\"\"\n",
    "        required_models = ['e5-large', 'mpnet', 'minilm']\n",
    "        failed_models = []\n",
    "\n",
    "        for model_name in required_models:\n",
    "            if model_name not in self.models or self.models[model_name] is None:\n",
    "                failed_models.append(model_name)\n",
    "\n",
    "        if failed_models:\n",
    "            raise RuntimeError(\n",
    "                f\"Required models failed to load: {', '.join(failed_models)}. \"\n",
    "                f\"Cannot proceed with evaluation. Please check model availability and try again.\"\n",
    "            )\n",
    "\n",
    "        print(\"✅ All sentence-transformer models validated and ready\")\n",
    "\n",
    "    def generate_query_embedding(self, question: str, model_name: str) -> np.ndarray:\n",
    "        \"\"\"Generate real query embedding for the given question\"\"\"\n",
    "\n",
    "        if model_name == 'ada':\n",
    "            # Use REAL OpenAI API for Ada embeddings\n",
    "            try:\n",
    "                client = OpenAI(api_key=os.getenv('OPENAI_API_KEY'))\n",
    "                response = client.embeddings.create(\n",
    "                    input=question,\n",
    "                    model=\"text-embedding-ada-002\"\n",
    "                )\n",
    "                ada_embedding = np.array(response.data[0].embedding)\n",
    "                return ada_embedding.astype(np.float32)\n",
    "            except Exception as e:\n",
    "                # CRITICAL: Fail explicitly instead of using random data\n",
    "                raise RuntimeError(f\"Failed to generate Ada embedding for question. Error: {e}\")\n",
    "\n",
    "        elif model_name in self.models and self.models[model_name]:\n",
    "            try:\n",
    "                # For sentence-transformer models, encode directly\n",
    "                if model_name == 'mpnet':\n",
    "                    # For MPNet, add query prefix as recommended\n",
    "                    prefixed_question = f\"query: {question}\"\n",
    "                    embedding = self.models[model_name].encode(prefixed_question)\n",
    "                else:\n",
    "                    embedding = self.models[model_name].encode(question)\n",
    "\n",
    "                return embedding.astype(np.float32)\n",
    "            except Exception as e:\n",
    "                # CRITICAL: Fail explicitly instead of using random data\n",
    "                raise RuntimeError(f\"Failed to generate {model_name} embedding for question. Error: {e}\")\n",
    "\n",
    "        else:\n",
    "            # CRITICAL: Fail explicitly for unknown or unloaded models\n",
    "            raise ValueError(f\"Model '{model_name}' is unknown or failed to load. Cannot generate embedding.\")\n",
    "\n",
    "class EmbeddedRetriever:\n",
    "    \"\"\"Handles document embedding retrieval and search\"\"\"\n",
    "\n",
    "    def __init__(self, file_path: str, model_name: str):\n",
    "        self.model_name = model_name\n",
    "        self.file_path = file_path\n",
    "        self.df = None\n",
    "        self.embeddings = None\n",
    "        self.embedding_dim = None\n",
    "        self.load_data()\n",
    "\n",
    "    def load_data(self):\n",
    "        \"\"\"Load embedding data from parquet file\"\"\"\n",
    "        try:\n",
    "            self.df = pd.read_parquet(self.file_path)\n",
    "\n",
    "            # Get embeddings\n",
    "            if 'embedding' in self.df.columns:\n",
    "                embeddings_list = self.df['embedding'].tolist()\n",
    "                self.embeddings = np.array(embeddings_list)\n",
    "                self.embedding_dim = self.embeddings.shape[1] if len(self.embeddings) > 0 else 0\n",
    "                print(f\"✅ Loaded {len(self.df)} documents for {self.model_name} ({self.embedding_dim}D)\")\n",
    "            else:\n",
    "                raise ValueError(\"No 'embedding' column found\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"❌ Error loading {self.model_name}: {e}\")\n",
    "            self.df = pd.DataFrame()\n",
    "            self.embeddings = np.array([])\n",
    "            self.embedding_dim = 0\n",
    "\n",
    "    def search(self, query_embedding: np.ndarray, top_k: int = 10):\n",
    "        \"\"\"Search for similar documents\"\"\"\n",
    "        if len(self.embeddings) == 0:\n",
    "            return []\n",
    "\n",
    "        try:\n",
    "            # Calculate cosine similarities\n",
    "            similarities = cosine_similarity(query_embedding.reshape(1, -1), self.embeddings)[0]\n",
    "\n",
    "            # Get top-k indices\n",
    "            top_indices = np.argsort(similarities)[::-1][:top_k]\n",
    "\n",
    "            results = []\n",
    "            for idx in top_indices:\n",
    "                if idx < len(self.df):\n",
    "                    doc = self.df.iloc[idx]\n",
    "                    results.append({\n",
    "                        'rank': len(results) + 1,\n",
    "                        'cosine_similarity': float(similarities[idx]),\n",
    "                        'link': doc.get('link', ''),\n",
    "                        'title': doc.get('title', ''),\n",
    "                        'content': doc.get('content', '')\n",
    "                    })\n",
    "\n",
    "            # DETERMINISTIC: Sort by similarity desc, then by link asc (tie-breaking)\n",
    "            results = sorted(\n",
    "                results,\n",
    "                key=lambda x: (-x['cosine_similarity'], x['link'])\n",
    "            )\n",
    "\n",
    "            # Re-assign ranks after deterministic sort\n",
    "            for i, doc in enumerate(results):\n",
    "                doc['rank'] = i + 1\n",
    "\n",
    "            return results\n",
    "        except Exception as e:\n",
    "            print(f\"⚠️ Search error for {self.model_name}: {e}\")\n",
    "            return []\n",
    "\n",
    "class EmbeddedDataPipeline:\n",
    "    \"\"\"Main pipeline for embedded document retrieval and evaluation\"\"\"\n",
    "\n",
    "    def __init__(self, base_path: str, embedding_files: dict):\n",
    "        self.base_path = base_path\n",
    "        self.embedding_files = embedding_files\n",
    "        self.retrievers = {}\n",
    "        self.real_embedding_generator = RealEmbeddingGenerator()\n",
    "        self.cross_encoder = None\n",
    "        self._load_retrievers()\n",
    "        self._load_cross_encoder()\n",
    "\n",
    "    def _load_retrievers(self):\n",
    "        \"\"\"Load all embedding retrievers\"\"\"\n",
    "        for model_name, file_path in self.embedding_files.items():\n",
    "            if os.path.exists(file_path):\n",
    "                self.retrievers[model_name] = EmbeddedRetriever(file_path, model_name)\n",
    "            else:\n",
    "                print(f\"❌ File not found for {model_name}: {file_path}\")\n",
    "\n",
    "    def _load_cross_encoder(self):\n",
    "        \"\"\"Load CrossEncoder for reranking\"\"\"\n",
    "        try:\n",
    "            self.cross_encoder = CrossEncoder('cross-encoder/ms-marco-MiniLM-L-6-v2')\n",
    "            print(\"✅ CrossEncoder loaded\")\n",
    "        except Exception as e:\n",
    "            print(f\"❌ Error loading CrossEncoder: {e}\")\n",
    "\n",
    "    def load_config_file(self, config_path: str):\n",
    "        \"\"\"Load configuration file\"\"\"\n",
    "        try:\n",
    "            with open(config_path, 'r', encoding='utf-8') as f:\n",
    "                return json.load(f)\n",
    "        except Exception as e:\n",
    "            print(f\"❌ Error loading config: {e}\")\n",
    "            return None\n",
    "\n",
    "    def get_system_info(self):\n",
    "        \"\"\"Get system information\"\"\"\n",
    "        available_models = list(self.retrievers.keys())\n",
    "        models_info = {}\n",
    "\n",
    "        for model_name, retriever in self.retrievers.items():\n",
    "            if retriever.df is not None and len(retriever.df) > 0:\n",
    "                models_info[model_name] = {\n",
    "                    'num_documents': len(retriever.df),\n",
    "                    'embedding_dim': retriever.embedding_dim\n",
    "                }\n",
    "            else:\n",
    "                models_info[model_name] = {'error': 'Failed to load'}\n",
    "\n",
    "        return {\n",
    "            'available_models': available_models,\n",
    "            'models_info': models_info\n",
    "        }\n",
    "\n",
    "    def cleanup(self):\n",
    "        \"\"\"Clean up resources\"\"\"\n",
    "        pass\n",
    "\n",
    "def calculate_real_retrieval_metrics(ground_truth_links: list, retrieved_docs: list, top_k_values: list = None):\n",
    "    \"\"\"Calculate retrieval metrics using real cosine similarities and document links\"\"\"\n",
    "\n",
    "    if top_k_values is None:\n",
    "        top_k_values = list(range(1, 51))  # Support up to 50 documents\n",
    "\n",
    "    # Normalize ground truth links using the same function as in collection creation\n",
    "    normalized_gt = [normalize_url(link) for link in ground_truth_links if link]\n",
    "\n",
    "    # Create relevance array based on actual link matching\n",
    "    relevance_scores = []\n",
    "    doc_scores = []\n",
    "\n",
    "    for doc in retrieved_docs:\n",
    "        doc_link = normalize_url(doc.get('link', ''))\n",
    "        is_relevant = 1 if doc_link in normalized_gt else 0\n",
    "        relevance_scores.append(is_relevant)\n",
    "\n",
    "        # Store document info with real cosine similarity\n",
    "        doc_scores.append({\n",
    "            'rank': doc.get('rank', 0),\n",
    "            'cosine_similarity': doc.get('cosine_similarity', 0.0),  # Real similarity\n",
    "            'link': doc.get('link', ''),\n",
    "            'title': doc.get('title', ''),\n",
    "            'is_relevant': bool(is_relevant)\n",
    "        })\n",
    "\n",
    "    metrics = {}\n",
    "\n",
    "    # Calculate metrics for each k\n",
    "    for k in top_k_values:\n",
    "        if k <= len(relevance_scores):\n",
    "            rel_k = relevance_scores[:k]\n",
    "\n",
    "            # Precision@k\n",
    "            precision_k = sum(rel_k) / k if k > 0 else 0\n",
    "            metrics[f'precision@{k}'] = precision_k\n",
    "\n",
    "            # Recall@k\n",
    "            total_relevant = len(normalized_gt)\n",
    "            recall_k = sum(rel_k) / total_relevant if total_relevant > 0 else 0\n",
    "            metrics[f'recall@{k}'] = recall_k\n",
    "\n",
    "            # F1@k\n",
    "            if precision_k + recall_k > 0:\n",
    "                f1_k = 2 * (precision_k * recall_k) / (precision_k + recall_k)\n",
    "            else:\n",
    "                f1_k = 0\n",
    "            metrics[f'f1@{k}'] = f1_k\n",
    "\n",
    "            # NDCG@k\n",
    "            dcg = sum(rel_k[i] / np.log2(i + 2) for i in range(len(rel_k)))\n",
    "            ideal_rel = sorted(rel_k, reverse=True)\n",
    "            idcg = sum(ideal_rel[i] / np.log2(i + 2) for i in range(len(ideal_rel))) if ideal_rel else 0\n",
    "            ndcg_k = dcg / idcg if idcg > 0 else 0\n",
    "            metrics[f'ndcg@{k}'] = ndcg_k\n",
    "\n",
    "            # MAP@k (Mean Average Precision)\n",
    "            ap = 0\n",
    "            num_relevant = 0\n",
    "            for i in range(k):\n",
    "                if rel_k[i] == 1:\n",
    "                    num_relevant += 1\n",
    "                    precision_at_i = num_relevant / (i + 1)\n",
    "                    ap += precision_at_i\n",
    "            map_k = ap / total_relevant if total_relevant > 0 else 0\n",
    "            metrics[f'map@{k}'] = map_k\n",
    "\n",
    "            # MRR@k (Mean Reciprocal Rank)\n",
    "            mrr_k = 0\n",
    "            for i in range(k):\n",
    "                if rel_k[i] == 1:\n",
    "                    mrr_k = 1 / (i + 1)\n",
    "                    break\n",
    "            metrics[f'mrr@{k}'] = mrr_k\n",
    "\n",
    "    # Overall MRR (not limited to specific k)\n",
    "    mrr_overall = 0\n",
    "    for i in range(len(relevance_scores)):\n",
    "        if relevance_scores[i] == 1:\n",
    "            mrr_overall = 1 / (i + 1)\n",
    "            break\n",
    "    metrics['mrr'] = mrr_overall\n",
    "\n",
    "    # Add document scores for analysis\n",
    "    metrics['document_scores'] = doc_scores\n",
    "\n",
    "    return metrics\n",
    "\n",
    "def calculate_rag_metrics_real(question: str, context_docs: list, generated_answer: str, ground_truth: str):\n",
    "    \"\"\"Calculate comprehensive RAG metrics using real OpenAI API and BERTScore (with caching)\"\"\"\n",
    "\n",
    "    global openai_cache, semantic_similarity_model\n",
    "\n",
    "    # Get context links for cache key\n",
    "    context_links = [doc.get('link', '') for doc in context_docs[:3]]\n",
    "\n",
    "    # Create unique cache key including generated_answer and ground_truth\n",
    "    cache_input = f\"{question}|{generated_answer}|{ground_truth}\"\n",
    "\n",
    "    # Try to get from cache first\n",
    "    if openai_cache:\n",
    "        cached = openai_cache.get(cache_input, context_links, prompt_type=\"rag_metrics\")\n",
    "        if cached:\n",
    "            # Return cached metrics (excluding timestamp and context_links)\n",
    "            return {k: v for k, v in cached.items() if k not in ['timestamp', 'context_links']}\n",
    "\n",
    "    # Not in cache, calculate metrics\n",
    "    try:\n",
    "        client = OpenAI(api_key=os.getenv('OPENAI_API_KEY'))\n",
    "\n",
    "        # Prepare context\n",
    "        context_text = \"\\n\".join([doc.get('content', '')[:3000] for doc in context_docs[:3]])\n",
    "\n",
    "        # OPTIMIZED: Single API call for all RAGAS metrics (6 calls → 1 call, 83% cost reduction)\n",
    "        combined_prompt = f\"\"\"Evaluate this RAG system output across 5 dimensions. Respond ONLY with a JSON object.\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Context: {context_text}\n",
    "\n",
    "Generated Answer: {generated_answer}\n",
    "\n",
    "Ground Truth Answer: {ground_truth if ground_truth else \"Not provided\"}\n",
    "\n",
    "Rate each dimension on a 1-5 scale and respond in this EXACT JSON format:\n",
    "{{\n",
    "  \"faithfulness\": <1-5>,\n",
    "  \"answer_relevancy\": <1-5>,\n",
    "  \"answer_correctness\": <1-5>,\n",
    "  \"context_precision\": <1-5>,\n",
    "  \"context_recall\": <1-5>\n",
    "}}\n",
    "\n",
    "Dimension definitions:\n",
    "- faithfulness: Does the answer contradict the context? (1=contradicts, 5=fully supported)\n",
    "- answer_relevancy: Is the answer relevant to the question? (1=irrelevant, 5=perfect)\n",
    "- answer_correctness: How correct is the answer vs ground truth? (1=wrong, 5=correct, 3=no ground truth)\n",
    "- context_precision: How relevant is the context for answering? (1=irrelevant, 5=precise)\n",
    "- context_recall: Does context have all info needed for ground truth? (1=missing most, 5=complete, 3=no ground truth)\n",
    "\n",
    "Respond with ONLY the JSON object, no other text.\"\"\"\n",
    "\n",
    "        # Initialize scores\n",
    "        faithfulness_score = None\n",
    "        relevancy_score = None\n",
    "        correctness_score = None\n",
    "        context_precision_score = None\n",
    "        context_recall_score = None\n",
    "\n",
    "        try:\n",
    "            ragas_response = client.chat.completions.create(\n",
    "                model=\"gpt-3.5-turbo-0125\",\n",
    "                messages=[{\"role\": \"user\", \"content\": combined_prompt}],\n",
    "                max_tokens=150,\n",
    "                temperature=0,\n",
    "                response_format={\"type\": \"json_object\"}  # Force JSON response\n",
    "            )\n",
    "\n",
    "            # Parse JSON response\n",
    "            metrics_json = json.loads(ragas_response.choices[0].message.content)\n",
    "\n",
    "            # Extract and normalize scores (1-5 scale to 0-1)\n",
    "            faithfulness_score = float(metrics_json.get(\"faithfulness\", 0)) / 5.0 if metrics_json.get(\"faithfulness\") else None\n",
    "            relevancy_score = float(metrics_json.get(\"answer_relevancy\", 0)) / 5.0 if metrics_json.get(\"answer_relevancy\") else None\n",
    "            correctness_score = float(metrics_json.get(\"answer_correctness\", 0)) / 5.0 if metrics_json.get(\"answer_correctness\") else None\n",
    "            context_precision_score = float(metrics_json.get(\"context_precision\", 0)) / 5.0 if metrics_json.get(\"context_precision\") else None\n",
    "            context_recall_score = float(metrics_json.get(\"context_recall\", 0)) / 5.0 if metrics_json.get(\"context_recall\") else None\n",
    "\n",
    "        except json.JSONDecodeError as e:\n",
    "            print(f\"⚠️ Failed to parse RAGAS JSON response: {e}\")\n",
    "            # Scores remain None\n",
    "        except Exception as e:\n",
    "            print(f\"⚠️ Failed to calculate RAGAS metrics: {e}\")\n",
    "            # Scores remain None\n",
    "\n",
    "        # 6. BERTScore metrics (precision, recall, f1) and Semantic Similarity\n",
    "        bert_precision = None\n",
    "        bert_recall = None\n",
    "        bert_f1 = None\n",
    "        semantic_similarity = None\n",
    "\n",
    "        try:\n",
    "            if ground_truth and generated_answer:\n",
    "                # Calculate real BERTScore using bert-score library\n",
    "                from bert_score import score as bert_score_fn\n",
    "\n",
    "                P, R, F1 = bert_score_fn(\n",
    "                    [generated_answer],\n",
    "                    [ground_truth],\n",
    "                    lang='en',\n",
    "                    model_type='microsoft/deberta-base-mnli',\n",
    "                    device='cuda' if torch.cuda.is_available() else 'cpu',\n",
    "                    batch_size=1,\n",
    "                    verbose=False\n",
    "                )\n",
    "\n",
    "                # Free up GPU memory to prevent error CUDA out of memory\n",
    "                torch.cuda.empty_cache()\n",
    "\n",
    "                bert_precision = float(P[0])\n",
    "                bert_recall = float(R[0])\n",
    "                bert_f1s = F1.tolist()\n",
    "\n",
    "                # Eliminar variables para liberar memoria\n",
    "                del P, R, F1\n",
    "\n",
    "                # Calculate semantic similarity separately using sentence transformers\n",
    "                # Use global model to avoid loading it 2067 times (GPU memory leak fix)\n",
    "                if semantic_similarity_model is not None:\n",
    "                    gt_embedding = semantic_similarity_model.encode(ground_truth)\n",
    "                    answer_embedding = semantic_similarity_model.encode(generated_answer)\n",
    "\n",
    "                    similarity = cosine_similarity(\n",
    "                        gt_embedding.reshape(1, -1),\n",
    "                        answer_embedding.reshape(1, -1)\n",
    "                    )[0][0]\n",
    "                    semantic_similarity = float(similarity)\n",
    "                else:\n",
    "                    # Fallback if model not initialized (shouldn't happen)\n",
    "                    print(\"⚠️ Warning: semantic_similarity_model not initialized\")\n",
    "                    semantic_similarity = None\n",
    "\n",
    "        except Exception as e:\n",
    "            # Fallback in case of errors\n",
    "            print(f\"⚠️ Failed to calculate BERTScore/semantic similarity: {e}\")\n",
    "            bert_precision = None\n",
    "            bert_recall = None\n",
    "            bert_f1 = None\n",
    "            semantic_similarity = None\n",
    "\n",
    "        # Prepare result dict\n",
    "        result = {\n",
    "            # RAGAS metrics\n",
    "            'faithfulness': faithfulness_score,\n",
    "            'answer_relevancy': relevancy_score,  # Note: using 'answer_relevancy' (with y) as expected by Streamlit\n",
    "            'answer_correctness': correctness_score,\n",
    "            'context_precision': context_precision_score,\n",
    "            'context_recall': context_recall_score,\n",
    "            'semantic_similarity': semantic_similarity,\n",
    "\n",
    "            # BERTScore metrics\n",
    "            'bert_precision': bert_precision,\n",
    "            'bert_recall': bert_recall,\n",
    "            'bert_f1': bert_f1,\n",
    "\n",
    "            # Additional fields\n",
    "            'evaluation_method': 'Complete_RAGAS_OpenAI_BERTScore'\n",
    "        }\n",
    "\n",
    "        # Save to cache\n",
    "        if openai_cache:\n",
    "            openai_cache.set(\n",
    "                cache_input,\n",
    "                {**result, \"context_links\": context_links},\n",
    "                context_links,\n",
    "                prompt_type=\"rag_metrics\"\n",
    "            )\n",
    "\n",
    "        return result\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"⚠️ CRITICAL error in RAG metrics calculation: {e}\")\n",
    "        return {\n",
    "            # RAGAS metrics - all None on critical error\n",
    "            'faithfulness': None,\n",
    "            'answer_relevancy': None,\n",
    "            'answer_correctness': None,\n",
    "            'context_precision': None,\n",
    "            'context_recall': None,\n",
    "            'semantic_similarity': None,\n",
    "\n",
    "            # BERTScore metrics - all None on critical error\n",
    "            'bert_precision': None,\n",
    "            'bert_recall': None,\n",
    "            'bert_f1': None,\n",
    "\n",
    "            # Additional fields\n",
    "            'evaluation_method': 'Critical_Error_Fallback'\n",
    "        }\n",
    "\n",
    "def generate_rag_answer(question: str, context_docs: list):\n",
    "    \"\"\"Generate answer using OpenAI GPT and context documents (with caching)\"\"\"\n",
    "\n",
    "    global openai_cache\n",
    "\n",
    "    # Get context links for cache key\n",
    "    context_links = [doc.get('link', '') for doc in context_docs[:3]]\n",
    "\n",
    "    # Try to get from cache first\n",
    "    if openai_cache:\n",
    "        cached = openai_cache.get(question, context_links, prompt_type=\"rag_answer\")\n",
    "        if cached:\n",
    "            return cached.get('generated_answer', '')\n",
    "\n",
    "    # Not in cache, call OpenAI\n",
    "    try:\n",
    "        client = OpenAI(api_key=os.getenv('OPENAI_API_KEY'))\n",
    "\n",
    "        # Prepare context from top documents\n",
    "        context_text = \"\\n\\n\".join([\n",
    "            f\"Document {i+1}: {doc.get('content', '')[:800]}\"\n",
    "            for i, doc in enumerate(context_docs[:3])\n",
    "        ])\n",
    "\n",
    "        prompt = f\"\"\"\n",
    "        Based on the following context documents, answer the question accurately and concisely.\n",
    "\n",
    "        Context:\n",
    "        {context_text}\n",
    "\n",
    "        Question: {question}\n",
    "\n",
    "        Answer:\n",
    "        \"\"\"\n",
    "\n",
    "        response = client.chat.completions.create(\n",
    "            model=\"gpt-3.5-turbo-0125\",\n",
    "            messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "            max_tokens=200,\n",
    "            temperature=0  # DETERMINISTIC: Changed from 0.1 to 0 for reproducibility\n",
    "        )\n",
    "\n",
    "        generated_answer = response.choices[0].message.content.strip()\n",
    "\n",
    "        # Save to cache\n",
    "        if openai_cache:\n",
    "            openai_cache.set(\n",
    "                question,\n",
    "                {\"generated_answer\": generated_answer, \"context_links\": context_links},\n",
    "                context_links,\n",
    "                prompt_type=\"rag_answer\"\n",
    "            )\n",
    "\n",
    "        return generated_answer\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"⚠️ Error generating RAG answer: {e}\")\n",
    "        return f\"Error generating answer: {str(e)}\"\n",
    "\n",
    "def get_optimal_batch_size(documents: list, max_content_length: int = 1200) -> int:\n",
    "    \"\"\"Calculate optimal batch size based on content length (MEJORA DE RENDIMIENTO)\"\"\"\n",
    "    if not documents:\n",
    "        return 4\n",
    "\n",
    "    # Estimar longitud promedio del contenido (sample primeros 5 documentos)\n",
    "    sample_size = min(5, len(documents))\n",
    "    avg_content_length = np.mean([\n",
    "        len(doc.get('content', '')[:max_content_length]) + len(doc.get('title', ''))\n",
    "        for doc in documents[:sample_size]\n",
    "    ])\n",
    "\n",
    "    # Batch size inversamente proporcional a longitud de contenido\n",
    "    if avg_content_length > 1200:\n",
    "        return 2  # Documentos largos → batch pequeño para evitar OOM\n",
    "    elif avg_content_length > 600:\n",
    "        return 4  # Documentos medianos → batch estándar\n",
    "    else:\n",
    "        return 8  # Documentos cortos → batch grande para velocidad\n",
    "\n",
    "def rerank_with_cross_encoder(question: str, documents: list, cross_encoder, top_k: int = 10):\n",
    "    \"\"\"\n",
    "    Rerank documents using CrossEncoder with MEJORAS CRÍTICAS:\n",
    "    - Usa título + contenido (no solo contenido)\n",
    "    - Batch size adaptativo según longitud de documentos\n",
    "    - Mejora esperada: +15-25% NDCG@10, +10-20% MRR\n",
    "    \"\"\"\n",
    "\n",
    "    if not cross_encoder or not documents:\n",
    "        return documents\n",
    "\n",
    "    try:\n",
    "        # MEJORA 1: Prepare query-document pairs WITH TITLE + CONTENT\n",
    "        pairs = []\n",
    "        for doc in documents:\n",
    "            title = doc.get('title', '').strip()\n",
    "            content = doc.get('content', '').strip()\n",
    "\n",
    "            # Combinar título y contenido limitado\n",
    "            if title:\n",
    "                # Título + primeros 1200 caracteres de contenido\n",
    "                combined_text = f\"{title}: {content[:1200]}\"\n",
    "            else:\n",
    "                # Solo contenido limitado si no hay título\n",
    "                combined_text = content[:1500]\n",
    "\n",
    "            pairs.append([question, combined_text])\n",
    "\n",
    "        # MEJORA 2: Batch size adaptativo según longitud de documentos\n",
    "        optimal_batch_size = get_optimal_batch_size(documents, max_content_length=1200)\n",
    "\n",
    "        # Get CrossEncoder scores con batch size optimizado\n",
    "        scores = cross_encoder.predict(pairs, batch_size=optimal_batch_size)\n",
    "\n",
    "        # Apply Min-Max normalization to convert logits to [0, 1] range\n",
    "        scores = np.array(scores)\n",
    "        if len(scores) > 1 and scores.max() != scores.min():\n",
    "            # Standard Min-Max normalization\n",
    "            normalized_scores = (scores - scores.min()) / (scores.max() - scores.min())\n",
    "        else:\n",
    "            # Fallback for edge cases (all scores identical)\n",
    "            normalized_scores = np.full_like(scores, 0.5)\n",
    "\n",
    "        # Add scores to documents and sort\n",
    "        for i, doc in enumerate(documents):\n",
    "            doc['crossencoder_score'] = float(normalized_scores[i])\n",
    "\n",
    "        # DETERMINISTIC: Sort by CrossEncoder score desc, then by link asc (tie-breaking)\n",
    "        reranked = sorted(\n",
    "            documents,\n",
    "            key=lambda x: (-x['crossencoder_score'], x.get('link', ''))\n",
    "        )\n",
    "\n",
    "        # Update ranks\n",
    "        for i, doc in enumerate(reranked):\n",
    "            doc['rank'] = i + 1\n",
    "\n",
    "        return reranked[:top_k]\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"⚠️ CrossEncoder reranking error: {e}\")\n",
    "        return documents\n",
    "\n",
    "def evaluate_single_model_complete(model_name: str, data_pipeline: EmbeddedDataPipeline,\n",
    "                                   questions_data: list, reranking_method: str = 'crossencoder',\n",
    "                                   top_k: int = 10, generate_rag: bool = True):\n",
    "    \"\"\"Complete evaluation for a single model with real embeddings and metrics\"\"\"\n",
    "\n",
    "    print(f\"\\n🔄 Evaluating {model_name}...\")\n",
    "\n",
    "    retriever = data_pipeline.retrievers.get(model_name)\n",
    "    if not retriever or len(retriever.df) == 0:\n",
    "        print(f\"❌ No valid retriever for {model_name}\")\n",
    "        return None\n",
    "\n",
    "    all_before_metrics = []\n",
    "    all_after_metrics = []\n",
    "    individual_rag_metrics = []\n",
    "\n",
    "    # Score accumulators\n",
    "    before_scores = []\n",
    "    after_scores = []\n",
    "    ce_scores = []\n",
    "    total_docs_reranked = 0\n",
    "\n",
    "    for i, question_data in enumerate(tqdm(questions_data, desc=f\"{model_name}\")):\n",
    "        question = question_data['question']\n",
    "        ground_truth_links = question_data.get('validated_links', [])\n",
    "        ground_truth_answer = question_data.get('accepted_answer', '')\n",
    "\n",
    "        # Memory cleanup and cache checkpoint\n",
    "        if i % 100 == 0 and i > 0:\n",
    "          clear_gpu_memory()\n",
    "          print(f\"🧹 Memoria limpiada en pregunta {i}\")\n",
    "\n",
    "        # Save cache checkpoint every 50 questions\n",
    "        if i % 50 == 0 and i > 0 and openai_cache:\n",
    "            openai_cache.save_cache()\n",
    "            stats = openai_cache.get_stats()\n",
    "            print(f\"💾 Cache checkpoint @ {i}: {stats['cache_hits']} hits, {stats['cache_misses']} misses, hit rate: {stats['hit_rate']:.1f}%\")\n",
    "\n",
    "        # Generate real query embedding\n",
    "        query_embedding = data_pipeline.real_embedding_generator.generate_query_embedding(\n",
    "            question, model_name\n",
    "        )\n",
    "\n",
    "        # Retrieve documents\n",
    "        retrieved_docs = retriever.search(query_embedding, top_k=top_k)\n",
    "\n",
    "        if not retrieved_docs:\n",
    "            continue\n",
    "\n",
    "        # Calculate BEFORE reranking metrics\n",
    "        before_metrics = calculate_real_retrieval_metrics(\n",
    "            ground_truth_links, retrieved_docs, list(range(1, top_k + 1))\n",
    "        )\n",
    "\n",
    "        # Calculate average cosine similarity (before)\n",
    "        before_avg_score = np.mean([doc['cosine_similarity'] for doc in retrieved_docs])\n",
    "        before_scores.append(before_avg_score)\n",
    "\n",
    "        all_before_metrics.append(before_metrics)\n",
    "\n",
    "        # AFTER reranking\n",
    "        if reranking_method == 'crossencoder' and data_pipeline.cross_encoder:\n",
    "            # Rerank with CrossEncoder\n",
    "            reranked_docs = rerank_with_cross_encoder(\n",
    "                question, retrieved_docs, data_pipeline.cross_encoder, top_k\n",
    "            )\n",
    "\n",
    "            # Calculate AFTER reranking metrics\n",
    "            after_metrics = calculate_real_retrieval_metrics(\n",
    "                ground_truth_links, reranked_docs, list(range(1, top_k + 1))\n",
    "            )\n",
    "\n",
    "            # Calculate CrossEncoder scores\n",
    "            ce_question_scores = [doc.get('crossencoder_score', 0) for doc in reranked_docs]\n",
    "            ce_avg_score = np.mean(ce_question_scores) if ce_question_scores else 0\n",
    "            ce_scores.append(ce_avg_score)\n",
    "\n",
    "            # After score (using original cosine similarities)\n",
    "            after_avg_score = np.mean([doc['cosine_similarity'] for doc in reranked_docs])\n",
    "            after_scores.append(after_avg_score)\n",
    "\n",
    "            total_docs_reranked += len(reranked_docs)\n",
    "\n",
    "            # Store CrossEncoder specific metrics\n",
    "            after_metrics['model_crossencoder_scores'] = ce_question_scores\n",
    "            after_metrics['model_avg_crossencoder_score'] = ce_avg_score\n",
    "            after_metrics['model_total_documents_reranked'] = len(reranked_docs)\n",
    "\n",
    "        else:\n",
    "            # No reranking\n",
    "            after_metrics = before_metrics.copy()\n",
    "            reranked_docs = retrieved_docs\n",
    "            after_scores.append(before_avg_score)\n",
    "\n",
    "        all_after_metrics.append(after_metrics)\n",
    "\n",
    "        # RAG Metrics (using reranked docs as context)\n",
    "        if generate_rag:\n",
    "            try:\n",
    "                # Generate answer\n",
    "                generated_answer = generate_rag_answer(question, reranked_docs[:3])\n",
    "\n",
    "                # Calculate RAG metrics\n",
    "                rag_metrics = calculate_rag_metrics_real(\n",
    "                    question, reranked_docs[:3], generated_answer, ground_truth_answer\n",
    "                )\n",
    "\n",
    "                rag_metrics['question_index'] = i\n",
    "                rag_metrics['generated_answer'] = generated_answer\n",
    "                individual_rag_metrics.append(rag_metrics)\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"⚠️ RAG metrics error for question {i}: {e}\")\n",
    "\n",
    "    # Calculate averages\n",
    "    def calculate_averages(metrics_list):\n",
    "        if not metrics_list:\n",
    "            return {}\n",
    "\n",
    "        all_keys = set()\n",
    "        for metrics in metrics_list:\n",
    "            all_keys.update(metrics.keys())\n",
    "\n",
    "        averages = {}\n",
    "        for key in all_keys:\n",
    "            if key != 'document_scores':  # Skip document scores in averages\n",
    "                values = [m.get(key, 0) for m in metrics_list if isinstance(m.get(key), (int, float))]\n",
    "                if values:\n",
    "                    averages[key] = np.mean(values)\n",
    "\n",
    "        return averages\n",
    "\n",
    "    avg_before_metrics = calculate_averages(all_before_metrics)\n",
    "    avg_after_metrics = calculate_averages(all_after_metrics)\n",
    "\n",
    "    # Add model-level score metrics\n",
    "    avg_before_metrics['model_avg_score'] = np.mean(before_scores) if before_scores else 0\n",
    "    avg_after_metrics['model_avg_score'] = np.mean(after_scores) if after_scores else 0\n",
    "\n",
    "    if reranking_method == 'crossencoder' and ce_scores:\n",
    "        avg_after_metrics['model_avg_crossencoder_score'] = np.mean(ce_scores)\n",
    "        avg_after_metrics['model_total_documents_reranked'] = total_docs_reranked\n",
    "\n",
    "    # RAG averages - Complete RAGAS + BERTScore metrics\n",
    "    # CRITICAL: Filter out None values before calculating averages\n",
    "    rag_averages = {}\n",
    "    if individual_rag_metrics:\n",
    "        # Helper function to safely calculate average excluding None values\n",
    "        def safe_mean(metric_name):\n",
    "            values = [r[metric_name] for r in individual_rag_metrics if r[metric_name] is not None]\n",
    "            return np.mean(values) if values else None\n",
    "\n",
    "        rag_averages = {\n",
    "            # RAGAS metrics averages (excluding None values)\n",
    "            'avg_faithfulness': safe_mean('faithfulness'),\n",
    "            'avg_answer_relevance': safe_mean('answer_relevancy'),  # Note: 'answer_relevancy' with y\n",
    "            'avg_answer_correctness': safe_mean('answer_correctness'),\n",
    "            'avg_context_precision': safe_mean('context_precision'),\n",
    "            'avg_context_recall': safe_mean('context_recall'),\n",
    "            'avg_semantic_similarity': safe_mean('semantic_similarity'),\n",
    "\n",
    "            # BERTScore metrics averages (excluding None values)\n",
    "            'avg_bert_precision': safe_mean('bert_precision'),\n",
    "            'avg_bert_recall': safe_mean('bert_recall'),\n",
    "            'avg_bert_f1': safe_mean('bert_f1'),\n",
    "\n",
    "            # Status and count\n",
    "            'rag_available': True,\n",
    "            'total_rag_evaluations': len(individual_rag_metrics),\n",
    "            'successful_calculations': {\n",
    "                'faithfulness': len([r for r in individual_rag_metrics if r['faithfulness'] is not None]),\n",
    "                'answer_relevancy': len([r for r in individual_rag_metrics if r['answer_relevancy'] is not None]),\n",
    "                'answer_correctness': len([r for r in individual_rag_metrics if r['answer_correctness'] is not None]),\n",
    "                'context_precision': len([r for r in individual_rag_metrics if r['context_precision'] is not None]),\n",
    "                'context_recall': len([r for r in individual_rag_metrics if r['context_recall'] is not None]),\n",
    "                'semantic_similarity': len([r for r in individual_rag_metrics if r['semantic_similarity'] is not None]),\n",
    "                'bert_precision': len([r for r in individual_rag_metrics if r['bert_precision'] is not None]),\n",
    "                'bert_recall': len([r for r in individual_rag_metrics if r['bert_recall'] is not None]),\n",
    "                'bert_f1': len([r for r in individual_rag_metrics if r['bert_f1'] is not None])\n",
    "            }\n",
    "        }\n",
    "    else:\n",
    "        rag_averages = {'rag_available': False}\n",
    "\n",
    "    return {\n",
    "        'model_name': model_name,\n",
    "        'full_model_name': model_name,\n",
    "        'num_questions_evaluated': len(questions_data),\n",
    "        'embedding_dimensions': retriever.embedding_dim,\n",
    "        'total_documents': len(retriever.df),\n",
    "        'avg_before_metrics': avg_before_metrics,\n",
    "        'avg_after_metrics': avg_after_metrics,\n",
    "        'all_before_metrics': all_before_metrics,\n",
    "        'all_after_metrics': all_after_metrics,\n",
    "        'rag_metrics': rag_averages,\n",
    "        'individual_rag_metrics': individual_rag_metrics\n",
    "    }\n",
    "\n",
    "def run_real_complete_evaluation(available_models: list, config_data: dict,\n",
    "                                 data_pipeline: EmbeddedDataPipeline,\n",
    "                                 reranking_method: str = 'crossencoder',\n",
    "                                 max_questions: int = None, debug: bool = False):\n",
    "    \"\"\"Run complete evaluation with real embeddings and metrics\"\"\"\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    # Buscar questions_data primero, luego questions (compatibilidad)\n",
    "    questions_data = config_data.get('questions_data', config_data.get('questions', []))\n",
    "    if max_questions:\n",
    "        questions_data = questions_data[:max_questions]\n",
    "\n",
    "    params = config_data.get('params', {})\n",
    "    # Buscar top_k primero en el nivel raíz, luego en params\n",
    "    top_k = config_data.get('top_k', params.get('top_k', 10))\n",
    "    # Buscar generate_rag_metrics en nivel raíz, luego en params\n",
    "    generate_rag = config_data.get('generate_rag_metrics', params.get('generate_rag_metrics', True))\n",
    "\n",
    "    # Validate that all models are loaded before starting evaluation\n",
    "    print(\"🔍 Validating models...\")\n",
    "    data_pipeline.real_embedding_generator.validate_models()\n",
    "\n",
    "    # Initialize OpenAI cache\n",
    "    global openai_cache\n",
    "    cache_path = os.path.join(data_pipeline.base_path, '..', 'openai_responses_cache.json')\n",
    "    openai_cache = OpenAICache(cache_path)\n",
    "    print(f\"💾 OpenAI cache initialized: {cache_path}\")\n",
    "\n",
    "    # Initialize semantic similarity model ONCE (GPU memory leak fix)\n",
    "    global semantic_similarity_model\n",
    "    print(\"🔄 Loading semantic similarity model...\")\n",
    "    semantic_similarity_model = SentenceTransformer('sentence-transformers/all-mpnet-base-v2')\n",
    "    print(\"✅ Semantic similarity model loaded (will be reused across all 2067 questions)\")\n",
    "\n",
    "    print(f\"🚀 Starting evaluation of {len(available_models)} models on {len(questions_data)} questions\")\n",
    "    print(f\"📊 Reranking method: {reranking_method}\")\n",
    "    print(f\"🎯 Top-K: {top_k}\")\n",
    "    print(f\"🤖 RAG metrics: {generate_rag}\")\n",
    "\n",
    "    all_model_results = {}\n",
    "\n",
    "    for model_name in available_models:\n",
    "        result = evaluate_single_model_complete(\n",
    "            model_name=model_name,\n",
    "            data_pipeline=data_pipeline,\n",
    "            questions_data=questions_data,\n",
    "            reranking_method=reranking_method,\n",
    "            top_k=top_k,\n",
    "            generate_rag=generate_rag\n",
    "        )\n",
    "\n",
    "        if result:\n",
    "            all_model_results[model_name] = result\n",
    "\n",
    "            # Brief summary\n",
    "            avg_f1 = result['avg_after_metrics'].get('f1@5', 0)\n",
    "            avg_score = result['avg_after_metrics'].get('model_avg_score', 0)\n",
    "            print(f\"  ✅ {model_name}: F1@5={avg_f1:.3f}, Score={avg_score:.3f}\")\n",
    "\n",
    "    end_time = time.time()\n",
    "    evaluation_duration = end_time - start_time\n",
    "\n",
    "    # Save final cache and show statistics\n",
    "    if openai_cache:\n",
    "        openai_cache.save_cache()\n",
    "        stats = openai_cache.get_stats()\n",
    "        print(f\"\\n💾 OpenAI Cache Statistics:\")\n",
    "        print(f\"  📊 Total queries: {stats['total_queries']}\")\n",
    "        print(f\"  ✅ Cache hits: {stats['cache_hits']} ({stats['hit_rate']:.1f}%)\")\n",
    "        print(f\"  ⚠️  Cache misses: {stats['cache_misses']}\")\n",
    "        print(f\"  💰 Estimated cost saved: ${stats['estimated_cost_saved']:.2f}\")\n",
    "\n",
    "    evaluation_params = {\n",
    "        'num_questions': len(questions_data),\n",
    "        'models_evaluated': len(all_model_results),\n",
    "        'reranking_method': reranking_method,\n",
    "        'top_k': top_k,\n",
    "        'generate_rag_metrics': generate_rag\n",
    "    }\n",
    "\n",
    "    return {\n",
    "        'all_model_results': all_model_results,\n",
    "        'evaluation_duration': evaluation_duration,\n",
    "        'evaluation_params': evaluation_params\n",
    "    }\n",
    "\n",
    "def embedded_process_and_save_results(all_model_results: dict, output_path: str,\n",
    "                                      evaluation_params: dict, evaluation_duration: float):\n",
    "    \"\"\"Process and save results in Streamlit-compatible format\"\"\"\n",
    "\n",
    "    # Chile timezone\n",
    "    chile_tz = pytz.timezone('America/Santiago')\n",
    "    now_utc = datetime.now(pytz.UTC)\n",
    "    now_chile = now_utc.astimezone(chile_tz)\n",
    "\n",
    "    # Generate filename with date format YYYYMMDD_HHMMSS\n",
    "    timestamp_str = now_chile.strftime('%Y%m%d_%H%M%S')\n",
    "    filename = f\"cumulative_results_{timestamp_str}.json\"\n",
    "    filepath = os.path.join(output_path, filename)\n",
    "\n",
    "    # Create comprehensive results structure\n",
    "    results_data = {\n",
    "        'config': evaluation_params,\n",
    "        'evaluation_info': {\n",
    "            'timestamp': now_chile.isoformat(),\n",
    "            'timezone': 'America/Santiago',\n",
    "            'evaluation_type': 'cumulative_metrics_colab_multi_model',\n",
    "            'total_duration_seconds': evaluation_duration,\n",
    "            'models_evaluated': len(all_model_results),\n",
    "            'questions_per_model': evaluation_params.get('num_questions', 0),\n",
    "            'enhanced_display_compatible': True,\n",
    "            'data_verification': {\n",
    "                'is_real_data': True,\n",
    "                'no_simulation': True,\n",
    "                'no_random_values': True,\n",
    "                'rag_framework': 'Complete_RAGAS_with_OpenAI_API',\n",
    "                'reranking_method': f\"{evaluation_params.get('reranking_method', 'none')}_reranking\"\n",
    "            }\n",
    "        },\n",
    "        'results': all_model_results\n",
    "    }\n",
    "\n",
    "    # Save to file\n",
    "    try:\n",
    "        with open(filepath, 'w', encoding='utf-8') as f:\n",
    "            json.dump(results_data, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "        print(f\"✅ Results saved: {filename}\")\n",
    "\n",
    "        return {\n",
    "            'json': filepath,\n",
    "            'filename': filename,\n",
    "            'chile_time': now_chile.strftime('%Y-%m-%d %H:%M:%S %Z')\n",
    "        }\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error saving results: {e}\")\n",
    "        return None\n",
    "\n",
    "print(\"✅ Complete evaluation code loaded\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "A0y7-dY8FiKG"
   },
   "source": [
    "## ⚙️ 3. Load Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 69042,
     "status": "ok",
     "timestamp": 1760198855252,
     "user": {
      "displayName": "Harold Gómez",
      "userId": "03529158350759969358"
     },
     "user_tz": 180
    },
    "id": "qPazEnE4FiKG",
    "outputId": "ad31df44-cc59-49ce-f69b-85bb9045ccc2"
   },
   "outputs": [],
   "source": [
    "# Find latest config file\n",
    "config_files = glob.glob(ACUMULATIVE_PATH + 'evaluation_config_*.json')\n",
    "\n",
    "if config_files:\n",
    "    files_with_timestamps = []\n",
    "    for file in config_files:\n",
    "        match = re.search(r'evaluation_config_(\\d+)\\.json', file)\n",
    "        if match:\n",
    "            timestamp = int(match.group(1))\n",
    "            files_with_timestamps.append((timestamp, file))\n",
    "\n",
    "    if files_with_timestamps:\n",
    "        files_with_timestamps.sort(reverse=True)\n",
    "        CONFIG_FILE_PATH = files_with_timestamps[0][1]\n",
    "        latest_timestamp = files_with_timestamps[0][0]\n",
    "        readable_time = datetime.fromtimestamp(latest_timestamp).strftime('%Y-%m-%d %H:%M:%S')\n",
    "        print(f\"✅ Latest config: {os.path.basename(CONFIG_FILE_PATH)} ({readable_time})\")\n",
    "    else:\n",
    "        CONFIG_FILE_PATH = ACUMULATIVE_PATH + 'questions_with_links.json'\n",
    "        print(\"⚠️ Using default questions file\")\n",
    "else:\n",
    "    CONFIG_FILE_PATH = ACUMULATIVE_PATH + 'questions_with_links.json'\n",
    "    print(\"⚠️ No config files found, using default\")\n",
    "\n",
    "# Initialize pipeline and load config\n",
    "data_pipeline = EmbeddedDataPipeline(BASE_PATH, EMBEDDING_FILES)\n",
    "config_data = data_pipeline.load_config_file(CONFIG_FILE_PATH)\n",
    "\n",
    "if config_data and config_data.get('questions_data'):\n",
    "    # Handle new config format with questions_data\n",
    "    questions_data = config_data['questions_data']\n",
    "    params = config_data.get('data_config', {})  # New format uses data_config instead of params\n",
    "\n",
    "    # Convert new format to expected format for compatibility\n",
    "    config_data['questions'] = questions_data\n",
    "    config_data['params'] = {\n",
    "        'top_k': params.get('top_k', 10),\n",
    "        'reranking_method': params.get('reranking_method', 'crossencoder'),\n",
    "        'generate_rag_metrics': True,  # Default for new format\n",
    "        'use_llm_reranker': params.get('reranking_method', 'crossencoder') == 'crossencoder',\n",
    "        'num_questions': params.get('num_questions', len(questions_data))\n",
    "    }\n",
    "    params = config_data['params']\n",
    "\n",
    "    print(f\"✅ New config format loaded: {len(questions_data)} questions\")\n",
    "\n",
    "elif config_data and config_data.get('questions'):\n",
    "    # Handle legacy format\n",
    "    params = config_data['params']\n",
    "    print(f\"✅ Legacy config format loaded: {len(config_data['questions'])} questions\")\n",
    "\n",
    "else:\n",
    "    print(\"❌ Error loading config - no questions found\")\n",
    "    if config_data:\n",
    "        print(f\"Available keys: {list(config_data.keys())}\")\n",
    "    RERANKING_METHOD = 'crossencoder'\n",
    "    params = {'top_k': 10, 'reranking_method': 'crossencoder', 'generate_rag_metrics': True}\n",
    "\n",
    "if config_data and config_data.get('questions'):\n",
    "    # Get reranking method with backward compatibility\n",
    "    RERANKING_METHOD = params.get('reranking_method', 'crossencoder')\n",
    "    USE_LLM_RERANKING = params.get('use_llm_reranker', True)\n",
    "\n",
    "    if RERANKING_METHOD == 'crossencoder' and not USE_LLM_RERANKING:\n",
    "        RERANKING_METHOD = 'none'\n",
    "\n",
    "    print(f\"🔄 Reranking method: {RERANKING_METHOD}\")\n",
    "    print(f\"🎯 Top-K: {params.get('top_k', 10)}\")\n",
    "    print(f\"📊 RAG metrics: {params.get('generate_rag_metrics', False)}\")\n",
    "else:\n",
    "    print(\"❌ Error loading config\")\n",
    "    RERANKING_METHOD = 'crossencoder'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VuVs9gqoFiKH"
   },
   "source": [
    "## 📊 4. Check Available Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 29,
     "status": "ok",
     "timestamp": 1760198855308,
     "user": {
      "displayName": "Harold Gómez",
      "userId": "03529158350759969358"
     },
     "user_tz": 180
    },
    "id": "LlpkPdSMFiKH",
    "outputId": "1bde6e6e-9589-49cc-8125-17685c0303f0"
   },
   "outputs": [],
   "source": [
    "# Get system info\n",
    "system_info = data_pipeline.get_system_info()\n",
    "\n",
    "print(f\"📊 Available models:\")\n",
    "for model_name in system_info['available_models']:\n",
    "    model_info = system_info['models_info'].get(model_name, {})\n",
    "    if 'error' not in model_info:\n",
    "        print(f\"  ✅ {model_name}: {model_info.get('num_documents', 0):,} docs, {model_info.get('embedding_dim', 0)}D\")\n",
    "    else:\n",
    "        print(f\"  ❌ {model_name}: {model_info.get('error', 'Error')}\")\n",
    "\n",
    "available_models = [name for name in system_info['available_models']\n",
    "                   if 'error' not in system_info['models_info'].get(name, {})]\n",
    "\n",
    "print(f\"\\n🎯 Models for evaluation: {available_models}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "q2GUkyxwFiKI"
   },
   "source": [
    "## 🚀 5. Run Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 21,
     "status": "ok",
     "timestamp": 1760198855651,
     "user": {
      "displayName": "Harold Gómez",
      "userId": "03529158350759969358"
     },
     "user_tz": 180
    },
    "id": "UynetqSmzBRU",
    "outputId": "c8a1e9ff-4ff2-494e-8364-50f73f183cd3"
   },
   "outputs": [],
   "source": [
    "# CONFIGURACIONES MEJORADAS\n",
    "BATCH_QUESTIONS = 15  # Reducido de 50 a 15\n",
    "\n",
    "# Limpiar memoria antes de empezar\n",
    "clear_gpu_memory()\n",
    "check_gpu_memory()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 25803144,
     "status": "ok",
     "timestamp": 1760224658804,
     "user": {
      "displayName": "Harold Gómez",
      "userId": "03529158350759969358"
     },
     "user_tz": 180
    },
    "id": "WZzfzB9GFiKI",
    "outputId": "fe9c4f25-b1c6-4509-b584-6a8eb7cdba43"
   },
   "outputs": [],
   "source": [
    "# Run evaluation\n",
    "evaluation_result = run_real_complete_evaluation(\n",
    "    available_models=available_models,\n",
    "    config_data=config_data,\n",
    "    data_pipeline=data_pipeline,\n",
    "    reranking_method=RERANKING_METHOD,\n",
    "    max_questions=None,  # Use all questions from config\n",
    "    debug=False\n",
    ")\n",
    "\n",
    "all_models_results = evaluation_result['all_model_results']\n",
    "evaluation_duration = evaluation_result['evaluation_duration']\n",
    "evaluation_params = evaluation_result['evaluation_params']\n",
    "\n",
    "print(f\"\\n✅ Evaluation completed in {evaluation_duration/60:.2f} minutes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ntJ0pgfkFiKI"
   },
   "source": [
    "## 💾 6. Save Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5548,
     "status": "ok",
     "timestamp": 1760224664357,
     "user": {
      "displayName": "Harold Gómez",
      "userId": "03529158350759969358"
     },
     "user_tz": 180
    },
    "id": "S9VKAW4_FiKI",
    "outputId": "5527c3a0-045a-4e52-a0f6-f1440c7f97c1"
   },
   "outputs": [],
   "source": [
    "# Save results\n",
    "saved_files = embedded_process_and_save_results(\n",
    "    all_model_results=all_models_results,\n",
    "    output_path=RESULTS_OUTPUT_PATH,\n",
    "    evaluation_params=evaluation_params,\n",
    "    evaluation_duration=evaluation_duration\n",
    ")\n",
    "\n",
    "if saved_files:\n",
    "    print(f\"✅ Results saved:\")\n",
    "    print(f\"  📄 File: {os.path.basename(saved_files['json'])}\")\n",
    "    print(f\"  🌍 Time: {saved_files['chile_time']}\")\n",
    "    print(f\"  ✅ Format: Streamlit compatible\")\n",
    "else:\n",
    "    print(\"❌ Error saving results\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "F0JTubLXFiKJ"
   },
   "source": [
    "## 📈 7. Results Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1707,
     "status": "ok",
     "timestamp": 1760224666079,
     "user": {
      "displayName": "Harold Gómez",
      "userId": "03529158350759969358"
     },
     "user_tz": 180
    },
    "id": "jq26SMZmFiKJ",
    "outputId": "f3e521d0-f937-4d38-9bcd-05ecf4397276"
   },
   "outputs": [],
   "source": [
    "# Display results summary\n",
    "if saved_files and 'json' in saved_files:\n",
    "    import json\n",
    "\n",
    "    with open(saved_files['json'], 'r') as f:\n",
    "        final_results = json.load(f)\n",
    "\n",
    "    print(\"📊 RESULTS SUMMARY\")\n",
    "    print(\"=\"*50)\n",
    "\n",
    "    if 'results' in final_results:\n",
    "        results_data = final_results['results']\n",
    "\n",
    "        for model_name, model_data in results_data.items():\n",
    "            before_metrics = model_data.get('avg_before_metrics', {})\n",
    "            after_metrics = model_data.get('avg_after_metrics', {})\n",
    "\n",
    "            print(f\"\\n📊 {model_name.upper()}:\")\n",
    "            print(f\"  📝 Questions: {model_data.get('num_questions_evaluated', 0)}\")\n",
    "            print(f\"  📄 Documents: {model_data.get('total_documents', 0):,}\")\n",
    "\n",
    "            if before_metrics and after_metrics:\n",
    "                # Performance metrics\n",
    "                f1_before = before_metrics.get('f1@5', 0)\n",
    "                f1_after = after_metrics.get('f1@5', 0)\n",
    "                improvement = ((f1_after - f1_before) / f1_before * 100) if f1_before > 0 else 0\n",
    "\n",
    "                print(f\"  📈 F1@5: {f1_before:.3f} → {f1_after:.3f} ({improvement:+.1f}%)\")\n",
    "                print(f\"  📈 MRR: {before_metrics.get('mrr', 0):.3f} → {after_metrics.get('mrr', 0):.3f}\")\n",
    "\n",
    "                # Score metrics\n",
    "                score_before = before_metrics.get('model_avg_score', 0)\n",
    "                score_after = after_metrics.get('model_avg_score', 0)\n",
    "\n",
    "                print(f\"  📊 Avg Score: {score_before:.3f} → {score_after:.3f}\")\n",
    "\n",
    "                if 'model_avg_crossencoder_score' in after_metrics:\n",
    "                    ce_score = after_metrics.get('model_avg_crossencoder_score', 0)\n",
    "                    print(f\"  🧠 CrossEncoder Score: {ce_score:.3f}\")\n",
    "                    print(f\"  📊 Documents Reranked: {after_metrics.get('model_total_documents_reranked', 0)}\")\n",
    "\n",
    "            # RAG metrics\n",
    "            rag_metrics = model_data.get('rag_metrics', {})\n",
    "            if rag_metrics.get('rag_available'):\n",
    "                print(f\"  🤖 RAG Metrics Available: ✅\")\n",
    "                if 'avg_faithfulness' in rag_metrics:\n",
    "                    print(f\"    📋 Faithfulness: {rag_metrics['avg_faithfulness']:.3f}\")\n",
    "                if 'avg_bert_f1' in rag_metrics:\n",
    "                    print(f\"    🎯 BERT F1: {rag_metrics['avg_bert_f1']:.3f}\")\n",
    "            else:\n",
    "                print(f\"  🤖 RAG Metrics: ❌\")\n",
    "\n",
    "        # Overall comparison\n",
    "        print(f\"\\n🏆 OVERALL:\")\n",
    "        best_f1 = (\"\", 0)\n",
    "        best_score = (\"\", 0)\n",
    "\n",
    "        for model_name, model_data in results_data.items():\n",
    "            after_metrics = model_data.get('avg_after_metrics', {})\n",
    "            f1 = after_metrics.get('f1@5', 0)\n",
    "            score = after_metrics.get('model_avg_score', 0)\n",
    "\n",
    "            if f1 > best_f1[1]:\n",
    "                best_f1 = (model_name, f1)\n",
    "            if score > best_score[1]:\n",
    "                best_score = (model_name, score)\n",
    "\n",
    "        print(f\"  🥇 Best F1@5: {best_f1[0]} ({best_f1[1]:.3f})\")\n",
    "        print(f\"  📊 Best Score: {best_score[0]} ({best_score[1]:.3f})\")\n",
    "\n",
    "        # Methodology info\n",
    "        data_verification = final_results.get('evaluation_info', {}).get('data_verification', {})\n",
    "        print(f\"\\n🔬 VERIFICATION:\")\n",
    "        print(f\"  ✅ Real data: {data_verification.get('is_real_data', False)}\")\n",
    "        print(f\"  📊 Framework: {data_verification.get('rag_framework', 'N/A')}\")\n",
    "        print(f\"  🔄 Method: {data_verification.get('reranking_method', 'N/A')}\")\n",
    "\n",
    "print(\"\\n🎉 EVALUATION COMPLETE!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 56656280,
     "status": "ok",
     "timestamp": 1760281322361,
     "user": {
      "displayName": "Harold Gómez",
      "userId": "03529158350759969358"
     },
     "user_tz": 180
    },
    "id": "3O4-kIG1FiKJ"
   },
   "outputs": [],
   "source": [
    "# Play an audio beep. Any audio URL will do.\n",
    "from google.colab import output\n",
    "output.eval_js('new Audio(\"https://upload.wikimedia.org/wikipedia/commons/0/05/Beep-09.ogg\").play()')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tnuaWZQcFiKJ"
   },
   "source": [
    "## 🧹 8. Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 245,
     "status": "ok",
     "timestamp": 1760281323004,
     "user": {
      "displayName": "Harold Gómez",
      "userId": "03529158350759969358"
     },
     "user_tz": 180
    },
    "id": "iq4wLOXcFiKJ",
    "outputId": "2d783c03-abb2-462f-82d8-92bf89e817dd"
   },
   "outputs": [],
   "source": [
    "# Cleanup\n",
    "data_pipeline.cleanup()\n",
    "import gc\n",
    "gc.collect()\n",
    "\n",
    "print(\"🧹 Cleanup completed\")\n",
    "print(\"🎯 Results ready for Streamlit import\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
