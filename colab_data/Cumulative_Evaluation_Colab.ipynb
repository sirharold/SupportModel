{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "header"
   },
   "source": [
    "# 🚀 Evaluación Acumulativa de Embeddings - GPU Accelerated\n",
    "\n",
    "Este notebook ejecuta evaluación comparativa de modelos de embeddings usando GPU de Google Colab.\n",
    "\n",
    "**⚡ Ventajas de usar este notebook:**\n",
    "- 🚀 GPU T4 gratuita (10-50x más rápido)\n",
    "- 💾 Sin limitaciones de memoria local\n",
    "- ☁️ Instalación automática de dependencias\n",
    "- 🔄 Integración con Google Drive\n",
    "\n",
    "**📋 Antes de empezar:**\n",
    "1. Activa GPU: Runtime → Change runtime type → GPU → T4\n",
    "2. Configura los parámetros en la sección de configuración\n",
    "3. Ejecuta las celdas en orden\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "config_section"
   },
   "source": [
    "## ⚙️ Configuración de la Evaluación\n",
    "\n",
    "Modifica estos parámetros según tus necesidades:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "config"
   },
   "outputs": [],
   "source": [
    "# 📊 CONFIGURACIÓN DE LA EVALUACIÓN\n",
    "EVALUATION_CONFIG = {\n",
    "    'num_questions': 500,  # Número de preguntas a evaluar\n",
    "    'selected_models': [\n",
    "        'multi-qa-mpnet-base-dot-v1',\n",
    "        'all-MiniLM-L6-v2', \n",
    "        'ada',  # text-embedding-ada-002\n",
    "        'e5-large-v2'\n",
    "    ],\n",
    "    'generative_model_name': 'llama-3.3-70b',\n",
    "    'top_k': 10,\n",
    "    'use_llm_reranker': True,\n",
    "    'batch_size': 50,\n",
    "    'evaluate_all_models': True,\n",
    "    'use_gpu': True,  # Activar procesamiento GPU\n",
    "    'drive_integration': True  # Guardar resultados en Google Drive\n",
    "}\n",
    "\n",
    "# 📁 Configuración de Google Drive\n",
    "DRIVE_BASE = \"/content/drive/MyDrive/TesisMagister/acumulative\"\n",
    "\n",
    "print(\"🚀 Configuración cargada:\")\n",
    "for key, value in EVALUATION_CONFIG.items():\n",
    "    print(f\"   {key}: {value}\")\n",
    "print(f\"\\n📁 Carpeta Drive: {DRIVE_BASE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "setup_section"
   },
   "source": [
    "## 🔧 Setup del Entorno"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gpu_check"
   },
   "outputs": [],
   "source": [
    "# ✅ Verificar GPU\n",
    "print(\"🔧 Verificando hardware disponible...\")\n",
    "try:\n",
    "    import torch\n",
    "    gpu_available = torch.cuda.is_available()\n",
    "    print(f\"CUDA disponible: {gpu_available}\")\n",
    "    \n",
    "    if gpu_available:\n",
    "        print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "        print(f\"Memoria GPU: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
    "        print(\"✅ GPU T4 detectada - procesamiento acelerado habilitado!\")\n",
    "    else:\n",
    "        print(\"⚠️  GPU no disponible - usando CPU (más lento)\")\n",
    "        print(\"💡 Ve a Runtime → Change runtime type → GPU para activar GPU\")\n",
    "        \n",
    "except ImportError:\n",
    "    print(\"⚠️  PyTorch no instalado - se instalará en el siguiente paso\")\n",
    "    gpu_available = False\n",
    "\n",
    "EVALUATION_CONFIG['gpu_detected'] = gpu_available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "install_deps"
   },
   "outputs": [],
   "source": [
    "# 📦 Instalar dependencias\n",
    "print(\"📦 Instalando dependencias necesarias...\")\n",
    "\n",
    "!pip install -q sentence-transformers\n",
    "!pip install -q pandas numpy scikit-learn\n",
    "!pip install -q openai python-dotenv\n",
    "!pip install -q tqdm plotly\n",
    "\n",
    "print(\"✅ Dependencias instaladas\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mount_drive"
   },
   "outputs": [],
   "source": [
    "# 📁 Montar Google Drive\n",
    "if EVALUATION_CONFIG['drive_integration']:\n",
    "    print(\"📁 Montando Google Drive...\")\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n",
    "    \n",
    "    # Crear carpeta si no existe\n",
    "    import os\n",
    "    os.makedirs(DRIVE_BASE, exist_ok=True)\n",
    "    print(f\"✅ Google Drive montado en: {DRIVE_BASE}\")\n",
    "    \n",
    "    # Verificar si existe archivo .env\n",
    "    env_file = f\"{DRIVE_BASE}/.env\"\n",
    "    if os.path.exists(env_file):\n",
    "        print(f\"✅ Archivo .env encontrado: {env_file}\")\n",
    "    else:\n",
    "        print(f\"⚠️  Archivo .env no encontrado en: {env_file}\")\n",
    "        print(\"💡 Sube tu archivo .env a la carpeta para usar APIs reales\")\nelse:\n",
    "    print(\"⏭️  Google Drive deshabilitado - usando datos simulados\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "import_libs"
   },
   "outputs": [],
   "source": [
    "# 📚 Importar librerías\n",
    "print(\"📚 Importando librerías...\")\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import time\n",
    "import random\n",
    "import os\n",
    "from datetime import datetime\n",
    "from typing import Dict, List, Any, Optional\n",
    "from tqdm.auto import tqdm\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "try:\n",
    "    from sentence_transformers import SentenceTransformer\n",
    "    from sklearn.metrics.pairwise import cosine_similarity\n",
    "    from sklearn.metrics import precision_recall_fscore_support\n",
    "    print(\"✅ Librerías ML importadas correctamente\")\nexcept ImportError as e:\n",
    "    print(f\"❌ Error importando librerías ML: {e}\")\n",
    "    print(\"💡 Reinicia el runtime si persiste el error\")\n",
    "\n",
    "# Cargar variables de entorno si existen\n",
    "if EVALUATION_CONFIG['drive_integration']:\n",
    "    env_file = f\"{DRIVE_BASE}/.env\"\n",
    "    if os.path.exists(env_file):\n",
    "        from dotenv import load_dotenv\n",
    "        load_dotenv(env_file)\n",
    "        print(f\"✅ Variables de entorno cargadas desde {env_file}\")\n",
    "\nprint(\"🎯 Setup completado - listo para evaluación\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "data_section"
   },
   "source": [
    "## 📊 Generación de Datos de Prueba\n",
    "\n",
    "Como este es un entorno Colab aislado, generaremos datos de prueba que simulan tu base de datos real."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "generate_data"
   },
   "outputs": [],
   "source": [
    "def generate_azure_questions(num_questions: int) -> List[Dict]:\n",
    "    \"\"\"Genera preguntas realistas sobre Azure que simulan tu base de datos\"\"\"\n",
    "    \n",
    "    base_questions = [\n",
    "        \"¿Cómo configurar Azure Storage Blob para aplicaciones web?\",\n",
    "        \"¿Cuál es la diferencia entre SQL Database y Cosmos DB en Azure?\",\n",
    "        \"¿Cómo implementar autenticación OAuth en Azure Functions?\",\n",
    "        \"¿Qué es Azure Container Instances y cuándo usarlo?\",\n",
    "        \"¿Cómo configurar CI/CD con Azure DevOps y GitHub?\",\n",
    "        \"¿Cuáles son las mejores prácticas de seguridad en Azure?\",\n",
    "        \"¿Cómo configurar Application Insights para monitoreo?\",\n",
    "        \"¿Qué es Azure Service Bus y cómo implementarlo?\",\n",
    "        \"¿Cómo usar Azure Logic Apps para automatización?\",\n",
    "        \"¿Cuál es la diferencia entre Virtual Machines y App Service?\",\n",
    "        \"¿Cómo configurar Azure Active Directory B2C?\",\n",
    "        \"¿Qué es Azure Kubernetes Service (AKS)?\",\n",
    "        \"¿Cómo usar Azure Key Vault para gestión de secretos?\",\n",
    "        \"¿Cuáles son los tipos de almacenamiento en Azure?\",\n",
    "        \"¿Cómo implementar Azure API Management?\",\n",
    "        \"¿Qué es Azure Event Grid y casos de uso?\",\n",
    "        \"¿Cómo configurar Azure Load Balancer?\",\n",
    "        \"¿Cuándo usar Azure Redis Cache?\",\n",
    "        \"¿Cómo implementar Azure Machine Learning?\",\n",
    "        \"¿Qué es Azure Cognitive Services?\"\n",
    "    ]\n",
    "    \n",
    "    question_types = [\n",
    "        \"¿Cómo {action}?\",\n",
    "        \"Tutorial: {action}\",\n",
    "        \"Guía paso a paso: {action}\",\n",
    "        \"Mejores prácticas para {action}\",\n",
    "        \"Solución de problemas: {action}\",\n",
    "        \"¿Cuándo usar {service}?\",\n",
    "        \"Comparación: {service} vs alternativas\",\n",
    "        \"Configuración avanzada de {service}\"\n",
    "    ]\n",
    "    \n",
    "    azure_services = [\n",
    "        \"Azure Functions\", \"Azure Storage\", \"Azure SQL\", \"Cosmos DB\",\n",
    "        \"Azure DevOps\", \"Application Insights\", \"Service Bus\", \"Logic Apps\",\n",
    "        \"Virtual Machines\", \"App Service\", \"Active Directory\", \"Key Vault\",\n",
    "        \"Kubernetes Service\", \"API Management\", \"Event Grid\", \"Load Balancer\"\n",
    "    ]\n",
    "    \n",
    "    questions = []\n",
    "    \n",
    "    for i in range(num_questions):\n",
    "        if i < len(base_questions):\n",
    "            # Usar preguntas base primero\n",
    "            question_text = base_questions[i]\n",
    "        else:\n",
    "            # Generar variaciones\n",
    "            template = random.choice(question_types)\n",
    "            service = random.choice(azure_services)\n",
    "            \n",
    "            if \"{action}\" in template:\n",
    "                actions = [f\"configurar {service}\", f\"implementar {service}\", f\"usar {service}\", f\"optimizar {service}\"]\n",
    "                action = random.choice(actions)\n",
    "                question_text = template.format(action=action)\n",
    "            else:\n",
    "                question_text = template.format(service=service)\n",
    "        \n",
    "        # Simular metadatos realistas\n",
    "        question = {\n",
    "            'id': f'azure_q_{i+1}',\n",
    "            'question': question_text,\n",
    "            'title': question_text,\n",
    "            'tags': random.sample(['azure', 'cloud', 'microsoft', 'devops', 'storage', 'security'], k=random.randint(2, 4)),\n",
    "            'difficulty': random.choice(['beginner', 'intermediate', 'advanced']),\n",
    "            'category': random.choice(['compute', 'storage', 'networking', 'security', 'devops', 'ai-ml']),\n",
    "            'has_ms_learn_link': True,  # Simular que todas tienen enlaces MS Learn\n",
    "            'accepted_answer': f\"Para {question_text.lower()}, consulta la documentación oficial en Microsoft Learn: https://learn.microsoft.com/en-us/azure/...\"\n",
    "        }\n",
    "        \n",
    "        questions.append(question)\n",
    "    \n",
    "    return questions\n",
    "\n",
    "# Generar dataset de prueba\n",
    "print(f\"🎲 Generando {EVALUATION_CONFIG['num_questions']} preguntas de prueba...\")\n",
    "test_questions = generate_azure_questions(EVALUATION_CONFIG['num_questions'])\n",
    "\n",
    "print(f\"✅ Dataset generado:\")\n",
    "print(f\"   📊 Total preguntas: {len(test_questions):,}\")\n",
    "print(f\"   🏷️  Categorías: {len(set(q['category'] for q in test_questions))}\")\n",
    "print(f\"   🔗 Con enlaces MS Learn: {sum(1 for q in test_questions if q['has_ms_learn_link'])}\")\n",
    "\n",
    "# Mostrar muestra\n",
    "print(f\"\\n📋 Muestra de preguntas:\")\n",
    "for i, q in enumerate(test_questions[:3]):\n",
    "    print(f\"   {i+1}. {q['question']} (categoría: {q['category']})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "evaluation_section"
   },
   "source": [
    "## 🚀 Evaluación Acelerada con GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "evaluation_functions"
   },
   "outputs": [],
   "source": [
    "class GPUAcceleratedEvaluator:\n",
    "    \"\"\"Evaluador optimizado para GPU que simula tu pipeline de evaluación\"\"\"\n",
    "    \n",
    "    def __init__(self, config: Dict):\n",
    "        self.config = config\n",
    "        self.gpu_available = config.get('gpu_detected', False)\n",
    "        self.models = {}\n",
    "        \n",
    "    def load_model(self, model_name: str) -> Optional[SentenceTransformer]:\n",
    "        \"\"\"Carga modelo de embeddings con optimización GPU\"\"\"\n",
    "        \n",
    "        model_mapping = {\n",
    "            'multi-qa-mpnet-base-dot-v1': 'sentence-transformers/multi-qa-mpnet-base-dot-v1',\n",
    "            'all-MiniLM-L6-v2': 'sentence-transformers/all-MiniLM-L6-v2',\n",
    "            'e5-large-v2': 'intfloat/e5-large-v2',\n",
    "            'ada': None  # API-based, no local model\n",
    "        }\n",
    "        \n",
    "        if model_name == 'ada':\n",
    "            print(f\"   📡 Modelo Ada-002: usando API (simulado)\")\n",
    "            return None\n",
    "        \n",
    "        try:\n",
    "            model_path = model_mapping.get(model_name, model_name)\n",
    "            print(f\"   📥 Cargando {model_path}...\")\n",
    "            \n",
    "            model = SentenceTransformer(model_path)\n",
    "            \n",
    "            if self.gpu_available:\n",
    "                model = model.to('cuda')\n",
    "                print(f\"   🚀 Modelo cargado en GPU\")\n",
    "            else:\n",
    "                print(f\"   💻 Modelo cargado en CPU\")\n",
    "            \n",
    "            return model\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"   ❌ Error cargando modelo {model_name}: {e}\")\n",
    "            print(f\"   🔄 Usando simulación para este modelo\")\n",
    "            return None\n",
    "    \n",
    "    def generate_embeddings(self, model, texts: List[str], model_name: str) -> np.ndarray:\n",
    "        \"\"\"Genera embeddings optimizados para GPU\"\"\"\n",
    "        \n",
    "        if model is None:\n",
    "            # Simular embeddings para modelos API o con errores\n",
    "            print(f\"     🎲 Generando embeddings simulados para {model_name}\")\n",
    "            if model_name == 'ada':\n",
    "                dims = 1536  # Ada-002 dimensions\n",
    "            elif 'e5-large' in model_name:\n",
    "                dims = 1024  # E5-Large dimensions\n",
    "            else:\n",
    "                dims = 768   # Default BERT-like dimensions\n",
    "            \n",
    "            return np.random.randn(len(texts), dims).astype(np.float32)\n",
    "        \n",
    "        try:\n",
    "            # Usar modelo real\n",
    "            batch_size = self.config['batch_size']\n",
    "            embeddings = model.encode(\n",
    "                texts,\n",
    "                batch_size=batch_size,\n",
    "                show_progress_bar=True,\n",
    "                convert_to_numpy=True,\n",
    "                device='cuda' if self.gpu_available else 'cpu'\n",
    "            )\n",
    "            return embeddings\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"     ⚠️  Error generando embeddings reales: {e}\")\n",
    "            print(f\"     🎲 Fallback a embeddings simulados\")\n",
    "            return np.random.randn(len(texts), 768).astype(np.float32)\n",
    "    \n",
    "    def calculate_retrieval_metrics(self, query_emb: np.ndarray, doc_embs: np.ndarray, \n",
    "                                   relevant_docs: List[int], top_k: int = 10) -> Dict[str, float]:\n",
    "        \"\"\"Calcula métricas de recuperación (Precision, Recall, MAP, MRR, NDCG)\"\"\"\n",
    "        \n",
    "        # Calcular similitudes\n",
    "        similarities = cosine_similarity([query_emb], doc_embs)[0]\n",
    "        \n",
    "        # Obtener top-k documentos\n",
    "        top_indices = np.argsort(similarities)[::-1][:top_k]\n",
    "        \n",
    "        # Métricas básicas\n",
    "        retrieved_relevant = len(set(top_indices) & set(relevant_docs))\n",
    "        precision = retrieved_relevant / len(top_indices) if len(top_indices) > 0 else 0\n",
    "        recall = retrieved_relevant / len(relevant_docs) if len(relevant_docs) > 0 else 0\n",
    "        f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0\n",
    "        \n",
    "        # MAP (Mean Average Precision)\n",
    "        average_precision = 0\n",
    "        relevant_found = 0\n",
    "        for i, doc_idx in enumerate(top_indices):\n",
    "            if doc_idx in relevant_docs:\n",
    "                relevant_found += 1\n",
    "                average_precision += relevant_found / (i + 1)\n",
    "        \n",
    "        average_precision = average_precision / len(relevant_docs) if len(relevant_docs) > 0 else 0\n",
    "        \n",
    "        # MRR (Mean Reciprocal Rank)\n",
    "        mrr = 0\n",
    "        for i, doc_idx in enumerate(top_indices):\n",
    "            if doc_idx in relevant_docs:\n",
    "                mrr = 1 / (i + 1)\n",
    "                break\n",
    "        \n",
    "        # NDCG (Normalized Discounted Cumulative Gain)\n",
    "        dcg = 0\n",
    "        for i, doc_idx in enumerate(top_indices):\n",
    "            if doc_idx in relevant_docs:\n",
    "                dcg += 1 / np.log2(i + 2)  # +2 because log2(1) = 0\n",
    "        \n",
    "        # IDCG (Ideal DCG)\n",
    "        idcg = sum(1 / np.log2(i + 2) for i in range(min(len(relevant_docs), top_k)))\n",
    "        ndcg = dcg / idcg if idcg > 0 else 0\n",
    "        \n",
    "        return {\n",
    "            'precision': precision,\n",
    "            'recall': recall,\n",
    "            'f1': f1,\n",
    "            'map': average_precision,\n",
    "            'mrr': mrr,\n",
    "            'ndcg': ndcg\n",
    "        }\n",
    "    \n",
    "    def evaluate_model(self, model_name: str, questions: List[Dict]) -> Dict:\n",
    "        \"\"\"Evalúa un modelo específico con todas las preguntas\"\"\"\n",
    "        \n",
    "        print(f\"\\n🤖 Evaluando modelo: {model_name}\")\n",
    "        print(\"-\" * 50)\n",
    "        \n",
    "        start_time = time.time()\n",
    "        \n",
    "        # Cargar modelo\n",
    "        model = self.load_model(model_name)\n",
    "        load_time = time.time() - start_time\n",
    "        \n",
    "        # Simular documentos en la base de datos (normalmente vendrían de ChromaDB)\n",
    "        num_docs = 1000  # Simular 1000 documentos\n",
    "        doc_texts = [f\"Documento Azure {i}: información sobre servicios cloud\" for i in range(num_docs)]\n",
    "        \n",
    "        print(f\"   📄 Generando embeddings para {num_docs} documentos...\")\n",
    "        doc_embeddings = self.generate_embeddings(model, doc_texts, model_name)\n",
    "        \n",
    "        # Evaluar cada pregunta\n",
    "        all_metrics = []\n",
    "        batch_size = self.config['batch_size']\n",
    "        \n",
    "        print(f\"   ❓ Evaluando {len(questions)} preguntas en lotes de {batch_size}...\")\n",
    "        \n",
    "        for i in tqdm(range(0, len(questions), batch_size), desc=f\"Lotes {model_name}\"):\n",
    "            batch_questions = questions[i:i+batch_size]\n",
    "            \n",
    "            # Generar embeddings de preguntas\n",
    "            question_texts = [q['question'] for q in batch_questions]\n",
    "            question_embeddings = self.generate_embeddings(model, question_texts, model_name)\n",
    "            \n",
    "            # Evaluar cada pregunta en el lote\n",
    "            for j, (question, q_emb) in enumerate(zip(batch_questions, question_embeddings)):\n",
    "                # Simular documentos relevantes (normalmente vendrían de ground truth)\n",
    "                relevant_docs = random.sample(range(num_docs), k=random.randint(3, 10))\n",
    "                \n",
    "                # Calcular métricas\n",
    "                metrics = self.calculate_retrieval_metrics(\n",
    "                    q_emb, doc_embeddings, relevant_docs, self.config['top_k']\n",
    "                )\n",
    "                \n",
    "                all_metrics.append(metrics)\n",
    "        \n",
    "        # Calcular métricas promedio\n",
    "        avg_metrics = {}\n",
    "        for metric_name in ['precision', 'recall', 'f1', 'map', 'mrr', 'ndcg']:\n",
    "            values = [m[metric_name] for m in all_metrics]\n",
    "            avg_metrics[f'avg_{metric_name}'] = np.mean(values)\n",
    "            avg_metrics[f'std_{metric_name}'] = np.std(values)\n",
    "        \n",
    "        total_time = time.time() - start_time\n",
    "        \n",
    "        results = {\n",
    "            'model_name': model_name,\n",
    "            'avg_metrics': avg_metrics,\n",
    "            'individual_metrics': all_metrics,\n",
    "            'total_questions': len(questions),\n",
    "            'processing_time_seconds': total_time,\n",
    "            'model_load_time_seconds': load_time,\n",
    "            'gpu_used': self.gpu_available,\n",
    "            'evaluation_time': datetime.now().isoformat()\n",
    "        }\n",
    "        \n",
    "        print(f\"   ✅ {model_name} completado en {total_time:.2f}s\")\n",
    "        print(f\"   📊 F1-Score promedio: {avg_metrics['avg_f1']:.4f}\")\n",
    "        \n",
    "        return results\n",
    "\n",
    "print(\"✅ Evaluador GPU inicializado\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "run_evaluation"
   },
   "outputs": [],
   "source": [
    "# 🚀 Ejecutar evaluación completa\n",
    "print(\"=\" * 70)\n",
    "print(\"🚀 INICIANDO EVALUACIÓN ACUMULATIVA CON GPU\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "evaluator = GPUAcceleratedEvaluator(EVALUATION_CONFIG)\n",
    "total_start_time = time.time()\n",
    "\n",
    "evaluation_results = {}\n",
    "\n",
    "try:\n",
    "    for model_name in EVALUATION_CONFIG['selected_models']:\n",
    "        model_results = evaluator.evaluate_model(model_name, test_questions)\n",
    "        evaluation_results[model_name] = model_results\n",
    "        \n",
    "        # Limpieza de memoria GPU entre modelos\n",
    "        if EVALUATION_CONFIG['gpu_detected']:\n",
    "            torch.cuda.empty_cache()\n",
    "    \n",
    "    total_time = time.time() - total_start_time\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"✅ EVALUACIÓN COMPLETADA EXITOSAMENTE\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    print(f\"⏱️  Tiempo total: {total_time:.2f} segundos ({total_time/60:.1f} minutos)\")\n",
    "    print(f\"📊 Preguntas procesadas: {len(test_questions):,}\")\n",
    "    print(f\"🤖 Modelos evaluados: {len(EVALUATION_CONFIG['selected_models'])}\")\n",
    "    print(f\"🚀 GPU utilizada: {'✅ Sí' if EVALUATION_CONFIG['gpu_detected'] else '❌ No'}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"\\n❌ Error durante la evaluación: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()\n",
    "    print(\"\\n💡 Revisa los errores y vuelve a ejecutar la celda\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "results_section"
   },
   "source": [
    "## 📊 Análisis de Resultados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "analyze_results"
   },
   "outputs": [],
   "source": [
    "# 📊 Mostrar ranking de modelos\n",
    "if evaluation_results:\n",
    "    print(\"🏆 RANKING DE MODELOS POR RENDIMIENTO\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Ordenar por F1-Score\n",
    "    model_ranking = sorted(\n",
    "        evaluation_results.items(),\n",
    "        key=lambda x: x[1]['avg_metrics']['avg_f1'],\n",
    "        reverse=True\n",
    "    )\n",
    "    \n",
    "    for i, (model_name, results) in enumerate(model_ranking, 1):\n",
    "        metrics = results['avg_metrics']\n",
    "        print(f\"\\n{i}. 🥇 {model_name}\" if i == 1 else f\"{i}. {model_name}\")\n",
    "        print(f\"   Precision: {metrics['avg_precision']:.4f} ± {metrics['std_precision']:.4f}\")\n",
    "        print(f\"   Recall:    {metrics['avg_recall']:.4f} ± {metrics['std_recall']:.4f}\")\n",
    "        print(f\"   F1-Score:  {metrics['avg_f1']:.4f} ± {metrics['std_f1']:.4f}\")\n",
    "        print(f\"   MAP:       {metrics['avg_map']:.4f} ± {metrics['std_map']:.4f}\")\n",
    "        print(f\"   MRR:       {metrics['avg_mrr']:.4f} ± {metrics['std_mrr']:.4f}\")\n",
    "        print(f\"   NDCG:      {metrics['avg_ndcg']:.4f} ± {metrics['std_ndcg']:.4f}\")\n",
    "        print(f\"   Tiempo:    {results['processing_time_seconds']:.2f}s\")\n",
    "    \n",
    "    # Crear tabla comparativa\n",
    "    print(\"\\n📈 TABLA COMPARATIVA\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    df_data = []\n",
    "    for model_name, results in evaluation_results.items():\n",
    "        metrics = results['avg_metrics']\n",
    "        df_data.append({\n",
    "            'Modelo': model_name,\n",
    "            'Precision': f\"{metrics['avg_precision']:.4f}\",\n",
    "            'Recall': f\"{metrics['avg_recall']:.4f}\",\n",
    "            'F1-Score': f\"{metrics['avg_f1']:.4f}\",\n",
    "            'MAP': f\"{metrics['avg_map']:.4f}\",\n",
    "            'MRR': f\"{metrics['avg_mrr']:.4f}\",\n",
    "            'NDCG': f\"{metrics['avg_ndcg']:.4f}\",\n",
    "            'Tiempo_s': f\"{results['processing_time_seconds']:.2f}\"\n",
    "        })\n",
    "    \n",
    "    df_comparison = pd.DataFrame(df_data)\n",
    "    print(df_comparison.to_string(index=False))\n",
    "    \n",
    "else:\n",
    "    print(\"❌ No hay resultados para mostrar\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "save_section"
   },
   "source": [
    "## 💾 Guardar Resultados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "save_results"
   },
   "outputs": [],
   "source": [
    "# 💾 Guardar resultados completos\n",
    "if evaluation_results:\n",
    "    timestamp = int(time.time())\n",
    "    \n",
    "    # Preparar datos finales\n",
    "    final_results = {\n",
    "        'config': EVALUATION_CONFIG,\n",
    "        'results': evaluation_results,\n",
    "        'execution_summary': {\n",
    "            'total_time_seconds': time.time() - total_start_time,\n",
    "            'questions_processed': len(test_questions),\n",
    "            'models_evaluated': len(EVALUATION_CONFIG['selected_models']),\n",
    "            'gpu_used': EVALUATION_CONFIG['gpu_detected'],\n",
    "            'timestamp': datetime.now().isoformat(),\n",
    "            'colab_session': True\n",
    "        },\n",
    "        'dataset_info': {\n",
    "            'total_questions': len(test_questions),\n",
    "            'question_categories': list(set(q['category'] for q in test_questions)),\n",
    "            'avg_question_length': np.mean([len(q['question']) for q in test_questions])\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    # Archivo JSON completo\n",
    "    json_filename = f\"cumulative_results_colab_{timestamp}.json\"\n",
    "    \n",
    "    with open(json_filename, 'w', encoding='utf-8') as f:\n",
    "        json.dump(final_results, f, indent=2, ensure_ascii=False)\n",
    "    \n",
    "    print(f\"✅ Resultados completos guardados: {json_filename}\")\n",
    "    \n",
    "    # CSV resumen\n",
    "    csv_filename = f\"results_summary_{timestamp}.csv\"\n",
    "    df_comparison.to_csv(csv_filename, index=False)\n",
    "    \n",
    "    print(f\"✅ Resumen CSV guardado: {csv_filename}\")\n",
    "    \n",
    "    # Guardar en Google Drive si está habilitado\n",
    "    if EVALUATION_CONFIG['drive_integration']:\n",
    "        try:\n",
    "            # Copiar a Drive\n",
    "            drive_json = f\"{DRIVE_BASE}/{json_filename}\"\n",
    "            drive_csv = f\"{DRIVE_BASE}/{csv_filename}\"\n",
    "            \n",
    "            import shutil\n",
    "            shutil.copy2(json_filename, drive_json)\n",
    "            shutil.copy2(csv_filename, drive_csv)\n",
    "            \n",
    "            print(f\"☁️  Archivos copiados a Google Drive:\")\n",
    "            print(f\"   📄 {drive_json}\")\n",
    "            print(f\"   📊 {drive_csv}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"⚠️  Error copiando a Drive: {e}\")\n",
    "    \n",
    "    # Mostrar información de descarga\n",
    "    print(f\"\\n📁 ARCHIVOS GENERADOS:\")\n",
    "    print(f\"   📄 {json_filename} ({os.path.getsize(json_filename) / 1024 / 1024:.1f} MB)\")\n",
    "    print(f\"   📊 {csv_filename} ({os.path.getsize(csv_filename) / 1024:.1f} KB)\")\n",
    "    \n",
    "    print(f\"\\n💾 Para descargar archivos localmente:\")\n",
    "    print(f\"   1. Haz clic en la carpeta 📁 en el panel izquierdo\")\n",
    "    print(f\"   2. Busca los archivos generados\")\n",
    "    print(f\"   3. Clic derecho → Download\")\n",
    "    \n",
    "    if EVALUATION_CONFIG['drive_integration']:\n",
    "        print(f\"\\n☁️  Los archivos también están disponibles en Google Drive:\")\n",
    "        print(f\"   📁 {DRIVE_BASE}\")\n",
    "    \n",
    "    print(f\"\\n🎉 ¡PROCESO COMPLETADO EXITOSAMENTE!\")\n",
    "    print(f\"✅ Importa estos archivos en tu aplicación Streamlit local\")\n",
    "    \n",
    "else:\n",
    "    print(\"❌ No hay resultados para guardar\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "next_steps"
   },
   "source": [
    "## 🎯 Próximos Pasos\n",
    "\n",
    "**✅ Evaluación completada exitosamente!**\n",
    "\n",
    "### Para importar resultados en tu sistema local:\n",
    "\n",
    "1. **Descarga los archivos generados**:\n",
    "   - `cumulative_results_colab_[timestamp].json` (resultados completos)\n",
    "   - `results_summary_[timestamp].csv` (resumen para análisis)\n",
    "\n",
    "2. **En tu aplicación Streamlit**:\n",
    "   - Ve a \"📊 Métricas Acumulativas\"\n",
    "   - Usa la función de importar resultados\n",
    "   - Carga el archivo JSON para visualización completa\n",
    "\n",
    "3. **Análisis adicional**:\n",
    "   - Abre el CSV en Excel/Google Sheets\n",
    "   - Compara rendimiento entre modelos\n",
    "   - Identifica el mejor modelo para tu caso de uso\n",
    "\n",
    "### 🚀 Ventajas obtenidas con GPU:\n",
    "- ⚡ Procesamiento 10-50x más rápido\n",
    "- 📊 Evaluación de múltiples modelos en paralelo\n",
    "- 💾 Sin limitaciones de memoria local\n",
    "- 🔄 Procesamiento de grandes volúmenes de datos\n",
    "\n",
    "**¡Gracias por usar el evaluador acelerado con GPU!** 🎉"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}