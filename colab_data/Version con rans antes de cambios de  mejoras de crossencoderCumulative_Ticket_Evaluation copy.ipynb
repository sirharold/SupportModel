{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3lJUissrFiJ9"
   },
   "source": [
    "# üìä Cumulative Process of Tickets - RAG Evaluation\n",
    "\n",
    "**Version**: 1.0  \n",
    "**Features**: Real data evaluation, complete RAG metrics, multi-model comparison  \n",
    "**Output**: Compatible cumulative_results_xxxxx.json for Streamlit visualization  \n",
    "\n",
    "**Complete Evaluation Pipeline:**\n",
    "- Real embedding generation using actual models (Ada, E5-Large, MPNet, MiniLM)\n",
    "- CrossEncoder reranking with Min-Max normalization\n",
    "- Complete RAGAS metrics (faithfulness, answer_relevancy, answer_correctness, context_precision, context_recall, semantic_similarity)\n",
    "- Complete BERTScore metrics (bert_precision, bert_recall, bert_f1)\n",
    "- Proper URL normalization for ground truth matching\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Tqg8lFZHFiKB"
   },
   "source": [
    "## üöÄ 1. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3967,
     "status": "ok",
     "timestamp": 1763078143580,
     "user": {
      "displayName": "Harold G√≥mez",
      "userId": "03529158350759969358"
     },
     "user_tz": 180
    },
    "id": "Cz2kvqsExgTC",
    "outputId": "ac2ce237-d9f2-4bbe-af79-19aa227ecb35"
   },
   "outputs": [],
   "source": [
    "!pip install torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 10325,
     "status": "ok",
     "timestamp": 1763078153925,
     "user": {
      "displayName": "Harold G√≥mez",
      "userId": "03529158350759969358"
     },
     "user_tz": 180
    },
    "id": "2tYMaSZP2iDc",
    "outputId": "671c0866-2c53-49ac-ba22-4f8a815cfa48"
   },
   "outputs": [],
   "source": [
    "!pip install ranx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 18,
     "status": "ok",
     "timestamp": 1763078153945,
     "user": {
      "displayName": "Harold G√≥mez",
      "userId": "03529158350759969358"
     },
     "user_tz": 180
    },
    "id": "IagASHdj6BOZ",
    "outputId": "37a46bf6-4ce4-4c1b-93fb-d4e1f943a35e"
   },
   "outputs": [],
   "source": [
    "from importlib.metadata import version\n",
    "print(version(\"ranx\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 40315,
     "status": "ok",
     "timestamp": 1763078194262,
     "user": {
      "displayName": "Harold G√≥mez",
      "userId": "03529158350759969358"
     },
     "user_tz": 180
    },
    "id": "H5gCuIOIFiKB",
    "outputId": "8c844a77-2176-47a6-9841-f28f1b414c20"
   },
   "outputs": [],
   "source": [
    "# Mount Google Drive and install packages\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "!pip install -q sentence-transformers pandas numpy scikit-learn openai python-dotenv tqdm torch bert-score\n",
    "\n",
    "import sys\n",
    "import os\n",
    "import glob\n",
    "import re\n",
    "from datetime import datetime\n",
    "\n",
    "# Setup paths\n",
    "BASE_PATH = '/content/drive/MyDrive/TesisMagister/acumulative/colab_data/'\n",
    "ACUMULATIVE_PATH = '/content/drive/MyDrive/TesisMagister/acumulative/'\n",
    "RESULTS_OUTPUT_PATH = ACUMULATIVE_PATH\n",
    "\n",
    "# Add to Python path\n",
    "sys.path.append(BASE_PATH)\n",
    "\n",
    "# Load API keys\n",
    "try:\n",
    "    from google.colab import userdata\n",
    "    openai_key = userdata.get('OPENAI_API_KEY')\n",
    "    if openai_key:\n",
    "        os.environ['OPENAI_API_KEY'] = openai_key\n",
    "        print(\"‚úÖ OpenAI API key loaded\")\n",
    "\n",
    "    hf_token = userdata.get('HF_TOKEN')\n",
    "    if hf_token:\n",
    "        from huggingface_hub import login\n",
    "        login(token=hf_token)\n",
    "        print(\"‚úÖ HF token loaded\")\n",
    "except:\n",
    "    print(\"‚ö†Ô∏è API keys not found in secrets\")\n",
    "\n",
    "# Embedding files\n",
    "EMBEDDING_FILES = {\n",
    "    'ada': BASE_PATH + 'docs_ada_with_embeddings_20250721_123712.parquet',\n",
    "    'e5-large': BASE_PATH + 'docs_e5large_with_embeddings_20250721_124918.parquet',\n",
    "    'mpnet': BASE_PATH + 'docs_mpnet_with_embeddings_20250721_125254.parquet',\n",
    "    'minilm': BASE_PATH + 'docs_minilm_with_embeddings_20250721_125846.parquet'\n",
    "}\n",
    "\n",
    "print(\"‚úÖ Setup complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3793,
     "status": "ok",
     "timestamp": 1763078198064,
     "user": {
      "displayName": "Harold G√≥mez",
      "userId": "03529158350759969358"
     },
     "user_tz": 180
    },
    "id": "M_gHwXNrxFTf",
    "outputId": "55642d24-4393-416f-b8d2-b3c1cb10a701"
   },
   "outputs": [],
   "source": [
    "# GPU Memory Configuration (A√ëADIR COMO NUEVA CELDA)\n",
    "import torch\n",
    "import gc\n",
    "import os\n",
    "\n",
    "# Configurar PyTorch para mejor manejo de memoria\n",
    "os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'expandable_segments:True'\n",
    "\n",
    "# Limitar uso de GPU al 80%\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.set_per_process_memory_fraction(0.8)\n",
    "    print(f\"GPU disponible: {torch.cuda.get_device_name()}\")\n",
    "    print(f\"Memoria total: {torch.cuda.get_device_properties(0).total_memory/1024**3:.2f} GB\")\n",
    "\n",
    "def clear_gpu_memory():\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "    print(\"üßπ GPU memory cleared\")\n",
    "\n",
    "def check_gpu_memory():\n",
    "    if torch.cuda.is_available():\n",
    "        allocated = torch.cuda.memory_allocated()/1024**3\n",
    "        reserved = torch.cuda.memory_reserved()/1024**3\n",
    "        print(f\"üîç GPU Memory - Allocated: {allocated:.2f}GB, Reserved: {reserved:.2f}GB\")\n",
    "\n",
    "print(\"‚úÖ GPU configuration loaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "o1RV84OgFiKD"
   },
   "source": [
    "## üìö 2. Load Evaluation Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VqznMTWBzsQt"
   },
   "outputs": [],
   "source": [
    "# Complete evaluation code\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import time\n",
    "import pytz\n",
    "import hashlib\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sentence_transformers import SentenceTransformer, CrossEncoder\n",
    "from openai import OpenAI\n",
    "from urllib.parse import urlparse, urlunparse\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "21txtE0UzsQt"
   },
   "source": [
    "### openai cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZYldzV7szsQt"
   },
   "outputs": [],
   "source": [
    "# ===========================\n",
    "# OPENAI CACHE SYSTEM\n",
    "# ===========================\n",
    "\n",
    "class OpenAICache:\n",
    "    \"\"\"Manages caching of OpenAI API responses to minimize costs\"\"\"\n",
    "\n",
    "    def __init__(self, cache_path: str):\n",
    "        self.cache_path = cache_path\n",
    "        self.cache = {\"metadata\": {}, \"cached_responses\": {}}\n",
    "        self.hits = 0\n",
    "        self.misses = 0\n",
    "        self.modified = False\n",
    "        self.load_cache()\n",
    "\n",
    "    def _create_hash(self, question: str, context_links: list = None, prompt_type: str = \"answer\") -> str:\n",
    "        \"\"\"Create unique hash for question + context combination\"\"\"\n",
    "        # Normalize context links\n",
    "        context_str = \"\"\n",
    "        if context_links:\n",
    "            sorted_links = sorted([normalize_url(link) for link in context_links if link])\n",
    "            context_str = \"|\".join(sorted_links)\n",
    "\n",
    "        # Create hash string\n",
    "        hash_input = f\"{prompt_type}:{question}:{context_str}\"\n",
    "        return hashlib.md5(hash_input.encode('utf-8')).hexdigest()\n",
    "\n",
    "    def load_cache(self):\n",
    "        \"\"\"Load existing cache from file\"\"\"\n",
    "        try:\n",
    "            if os.path.exists(self.cache_path):\n",
    "                with open(self.cache_path, 'r', encoding='utf-8') as f:\n",
    "                    self.cache = json.load(f)\n",
    "\n",
    "                total = len(self.cache.get('cached_responses', {}))\n",
    "                print(f\"‚úÖ Loaded OpenAI cache: {total} cached entries\")\n",
    "            else:\n",
    "                print(\"üìù No existing cache found, creating new cache\")\n",
    "                self.cache = {\n",
    "                    \"metadata\": {\n",
    "                        \"cache_version\": \"1.0\",\n",
    "                        \"created_at\": datetime.now().isoformat()\n",
    "                    },\n",
    "                    \"cached_responses\": {}\n",
    "                }\n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è Error loading cache: {e}, starting fresh\")\n",
    "            self.cache = {\"metadata\": {}, \"cached_responses\": {}}\n",
    "\n",
    "    def save_cache(self):\n",
    "        \"\"\"Save cache to file\"\"\"\n",
    "        if not self.modified:\n",
    "            return\n",
    "\n",
    "        try:\n",
    "            # Update metadata\n",
    "            self.cache[\"metadata\"][\"total_entries\"] = len(self.cache[\"cached_responses\"])\n",
    "            self.cache[\"metadata\"][\"last_updated\"] = datetime.now().isoformat()\n",
    "            self.cache[\"metadata\"][\"stats\"] = {\n",
    "                \"hits\": self.hits,\n",
    "                \"misses\": self.misses,\n",
    "                \"hit_rate\": self.hits / (self.hits + self.misses) if (self.hits + self.misses) > 0 else 0\n",
    "            }\n",
    "\n",
    "            with open(self.cache_path, 'w', encoding='utf-8') as f:\n",
    "                json.dump(self.cache, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "            print(f\"üíæ Cache saved: {len(self.cache['cached_responses'])} entries\")\n",
    "            self.modified = False\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error saving cache: {e}\")\n",
    "\n",
    "    def get(self, question: str, context_links: list = None, prompt_type: str = \"answer\"):\n",
    "        \"\"\"Get cached response if exists\"\"\"\n",
    "        cache_key = self._create_hash(question, context_links, prompt_type)\n",
    "\n",
    "        if cache_key in self.cache[\"cached_responses\"]:\n",
    "            self.hits += 1\n",
    "            return self.cache[\"cached_responses\"][cache_key]\n",
    "\n",
    "        self.misses += 1\n",
    "        return None\n",
    "\n",
    "    def set(self, question: str, response_data: dict, context_links: list = None, prompt_type: str = \"answer\"):\n",
    "        \"\"\"Store response in cache\"\"\"\n",
    "        cache_key = self._create_hash(question, context_links, prompt_type)\n",
    "\n",
    "        self.cache[\"cached_responses\"][cache_key] = {\n",
    "            **response_data,\n",
    "            \"timestamp\": datetime.now().isoformat()\n",
    "        }\n",
    "        self.modified = True\n",
    "\n",
    "    def get_stats(self):\n",
    "        \"\"\"Get cache statistics\"\"\"\n",
    "        total = self.hits + self.misses\n",
    "        hit_rate = (self.hits / total * 100) if total > 0 else 0\n",
    "\n",
    "        return {\n",
    "            \"total_queries\": total,\n",
    "            \"cache_hits\": self.hits,\n",
    "            \"cache_misses\": self.misses,\n",
    "            \"hit_rate\": hit_rate,\n",
    "            \"estimated_cost_saved\": self.hits * 0.05  # Approx $0.05 per cached query\n",
    "        }\n",
    "\n",
    "# Global cache instance (will be initialized in evaluation)\n",
    "openai_cache = None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NvEhZEzhzsQt"
   },
   "source": [
    "### Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-VAbaFQDzsQt"
   },
   "outputs": [],
   "source": [
    "# Global semantic similarity model (will be initialized in evaluation to avoid repeated loading)\n",
    "semantic_similarity_model = None\n",
    "\n",
    "def normalize_url(url: str) -> str:\n",
    "    \"\"\"\n",
    "    Normalizes a URL by removing query parameters and fragments (anchors).\n",
    "\n",
    "    Examples:\n",
    "        https://learn.microsoft.com/en-us/azure/storage/blobs/storage-blob-overview?view=azure-cli-latest#overview\n",
    "        -> https://learn.microsoft.com/en-us/azure/storage/blobs/storage-blob-overview\n",
    "\n",
    "        https://learn.microsoft.com/azure/virtual-machines/windows/quick-create-portal?tabs=windows10#create-vm\n",
    "        -> https://learn.microsoft.com/azure/virtual-machines/windows/quick-create-portal\n",
    "\n",
    "    Args:\n",
    "        url: The URL to normalize\n",
    "\n",
    "    Returns:\n",
    "        The normalized URL without query parameters and fragments\n",
    "    \"\"\"\n",
    "    if not url or not url.strip():\n",
    "        return \"\"\n",
    "\n",
    "    try:\n",
    "        # Parse the URL\n",
    "        parsed = urlparse(url.strip())\n",
    "\n",
    "        # Reconstruct without query parameters and fragments\n",
    "        normalized = urlunparse((\n",
    "            parsed.scheme,    # https\n",
    "            parsed.netloc,    # learn.microsoft.com\n",
    "            parsed.path,      # /en-us/azure/storage/blobs/storage-blob-overview\n",
    "            '',               # params (empty)\n",
    "            '',               # query (empty) - removes ?view=azure-cli-latest\n",
    "            ''                # fragment (empty) - removes #overview\n",
    "        ))\n",
    "\n",
    "        return normalized\n",
    "    except Exception as e:\n",
    "        # If parsing fails, return the original URL stripped\n",
    "        return url.strip()\n",
    "\n",
    "class RealEmbeddingGenerator:\n",
    "    \"\"\"Generates real embeddings using actual models\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.models = {}\n",
    "        self._load_models()\n",
    "\n",
    "    def _load_models(self):\n",
    "        \"\"\"Load sentence transformer models\"\"\"\n",
    "        model_configs = {\n",
    "            'e5-large': 'intfloat/e5-large-v2',\n",
    "            'mpnet': 'sentence-transformers/all-mpnet-base-v2',\n",
    "            'minilm': 'sentence-transformers/all-MiniLM-L6-v2'\n",
    "        }\n",
    "\n",
    "        for name, model_path in model_configs.items():\n",
    "            try:\n",
    "                self.models[name] = SentenceTransformer(model_path)\n",
    "                print(f\"‚úÖ Loaded {name} model\")\n",
    "            except Exception as e:\n",
    "                print(f\"‚ùå Error loading {name}: {e}\")\n",
    "                self.models[name] = None\n",
    "\n",
    "    def validate_models(self):\n",
    "        \"\"\"Validate that all required models are loaded before evaluation\"\"\"\n",
    "        required_models = ['e5-large', 'mpnet', 'minilm']\n",
    "        failed_models = []\n",
    "\n",
    "        for model_name in required_models:\n",
    "            if model_name not in self.models or self.models[model_name] is None:\n",
    "                failed_models.append(model_name)\n",
    "\n",
    "        if failed_models:\n",
    "            raise RuntimeError(\n",
    "                f\"Required models failed to load: {', '.join(failed_models)}. \"\n",
    "                f\"Cannot proceed with evaluation. Please check model availability and try again.\"\n",
    "            )\n",
    "\n",
    "        print(\"‚úÖ All sentence-transformer models validated and ready\")\n",
    "\n",
    "    def generate_query_embedding(self, question: str, model_name: str) -> np.ndarray:\n",
    "        \"\"\"Generate real query embedding for the given question\"\"\"\n",
    "\n",
    "        if model_name == 'ada':\n",
    "            # Use REAL OpenAI API for Ada embeddings\n",
    "            try:\n",
    "                client = OpenAI(api_key=os.getenv('OPENAI_API_KEY'))\n",
    "                response = client.embeddings.create(\n",
    "                    input=question,\n",
    "                    model=\"text-embedding-ada-002\"\n",
    "                )\n",
    "                ada_embedding = np.array(response.data[0].embedding)\n",
    "                return ada_embedding.astype(np.float32)\n",
    "            except Exception as e:\n",
    "                # CRITICAL: Fail explicitly instead of using random data\n",
    "                raise RuntimeError(f\"Failed to generate Ada embedding for question. Error: {e}\")\n",
    "\n",
    "        elif model_name in self.models and self.models[model_name]:\n",
    "            try:\n",
    "                # For sentence-transformer models, encode directly\n",
    "                if model_name == 'mpnet':\n",
    "                    # For MPNet, add query prefix as recommended\n",
    "                    prefixed_question = f\"query: {question}\"\n",
    "                    embedding = self.models[model_name].encode(prefixed_question)\n",
    "                else:\n",
    "                    embedding = self.models[model_name].encode(question)\n",
    "\n",
    "                return embedding.astype(np.float32)\n",
    "            except Exception as e:\n",
    "                # CRITICAL: Fail explicitly instead of using random data\n",
    "                raise RuntimeError(f\"Failed to generate {model_name} embedding for question. Error: {e}\")\n",
    "\n",
    "        else:\n",
    "            # CRITICAL: Fail explicitly for unknown or unloaded models\n",
    "            raise ValueError(f\"Model '{model_name}' is unknown or failed to load. Cannot generate embedding.\")\n",
    "\n",
    "class EmbeddedRetriever:\n",
    "    \"\"\"Handles document embedding retrieval and search\"\"\"\n",
    "\n",
    "    def __init__(self, file_path: str, model_name: str):\n",
    "        self.model_name = model_name\n",
    "        self.file_path = file_path\n",
    "        self.df = None\n",
    "        self.embeddings = None\n",
    "        self.embedding_dim = None\n",
    "        self.load_data()\n",
    "\n",
    "    def load_data(self):\n",
    "        \"\"\"Load embedding data from parquet file\"\"\"\n",
    "        try:\n",
    "            self.df = pd.read_parquet(self.file_path)\n",
    "\n",
    "            # Get embeddings\n",
    "            if 'embedding' in self.df.columns:\n",
    "                embeddings_list = self.df['embedding'].tolist()\n",
    "                self.embeddings = np.array(embeddings_list)\n",
    "                self.embedding_dim = self.embeddings.shape[1] if len(self.embeddings) > 0 else 0\n",
    "                print(f\"‚úÖ Loaded {len(self.df)} documents for {self.model_name} ({self.embedding_dim}D)\")\n",
    "            else:\n",
    "                raise ValueError(\"No 'embedding' column found\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error loading {self.model_name}: {e}\")\n",
    "            self.df = pd.DataFrame()\n",
    "            self.embeddings = np.array([])\n",
    "            self.embedding_dim = 0\n",
    "\n",
    "    def search(self, query_embedding: np.ndarray, top_k: int = 10):\n",
    "        \"\"\"Search for similar documents\"\"\"\n",
    "        if len(self.embeddings) == 0:\n",
    "            return []\n",
    "\n",
    "        try:\n",
    "            # Calculate cosine similarities\n",
    "            similarities = cosine_similarity(query_embedding.reshape(1, -1), self.embeddings)[0]\n",
    "\n",
    "            # Get top-k indices\n",
    "            top_indices = np.argsort(similarities)[::-1][:top_k]\n",
    "\n",
    "            results = []\n",
    "            for idx in top_indices:\n",
    "                if idx < len(self.df):\n",
    "                    doc = self.df.iloc[idx]\n",
    "                    results.append({\n",
    "                        'rank': len(results) + 1,\n",
    "                        'cosine_similarity': float(similarities[idx]),\n",
    "                        'link': doc.get('link', ''),\n",
    "                        'title': doc.get('title', ''),\n",
    "                        'content': doc.get('content', '')\n",
    "                    })\n",
    "\n",
    "            # DETERMINISTIC: Sort by similarity desc, then by link asc (tie-breaking)\n",
    "            results = sorted(\n",
    "                results,\n",
    "                key=lambda x: (-x['cosine_similarity'], x['link'])\n",
    "            )\n",
    "\n",
    "            # Re-assign ranks after deterministic sort\n",
    "            for i, doc in enumerate(results):\n",
    "                doc['rank'] = i + 1\n",
    "\n",
    "            return results\n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è Search error for {self.model_name}: {e}\")\n",
    "            return []\n",
    "\n",
    "class EmbeddedDataPipeline:\n",
    "    \"\"\"Main pipeline for embedded document retrieval and evaluation\"\"\"\n",
    "\n",
    "    def __init__(self, base_path: str, embedding_files: dict):\n",
    "        self.base_path = base_path\n",
    "        self.embedding_files = embedding_files\n",
    "        self.retrievers = {}\n",
    "        self.real_embedding_generator = RealEmbeddingGenerator()\n",
    "        self.cross_encoder = None\n",
    "        self._load_retrievers()\n",
    "        self._load_cross_encoder()\n",
    "\n",
    "    def _load_retrievers(self):\n",
    "        \"\"\"Load all embedding retrievers\"\"\"\n",
    "        for model_name, file_path in self.embedding_files.items():\n",
    "            if os.path.exists(file_path):\n",
    "                self.retrievers[model_name] = EmbeddedRetriever(file_path, model_name)\n",
    "            else:\n",
    "                print(f\"‚ùå File not found for {model_name}: {file_path}\")\n",
    "\n",
    "    def _load_cross_encoder(self):\n",
    "        \"\"\"Load CrossEncoder for reranking\"\"\"\n",
    "        try:\n",
    "            self.cross_encoder = CrossEncoder('mixedbread-ai/mxbai-rerank-xsmall-v1')\n",
    "            print(\"‚úÖ CrossEncoder loaded\")\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error loading CrossEncoder: {e}\")\n",
    "\n",
    "    def load_config_file(self, config_path: str):\n",
    "        \"\"\"Load configuration file\"\"\"\n",
    "        try:\n",
    "            with open(config_path, 'r', encoding='utf-8') as f:\n",
    "                return json.load(f)\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error loading config: {e}\")\n",
    "            return None\n",
    "\n",
    "    def get_system_info(self):\n",
    "        \"\"\"Get system information\"\"\"\n",
    "        available_models = list(self.retrievers.keys())\n",
    "        models_info = {}\n",
    "\n",
    "        for model_name, retriever in self.retrievers.items():\n",
    "            if retriever.df is not None and len(retriever.df) > 0:\n",
    "                models_info[model_name] = {\n",
    "                    'num_documents': len(retriever.df),\n",
    "                    'embedding_dim': retriever.embedding_dim\n",
    "                }\n",
    "            else:\n",
    "                models_info[model_name] = {'error': 'Failed to load'}\n",
    "\n",
    "        return {\n",
    "            'available_models': available_models,\n",
    "            'models_info': models_info\n",
    "        }\n",
    "\n",
    "    def cleanup(self):\n",
    "        \"\"\"Clean up resources\"\"\"\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HOzaFoDRzsQu"
   },
   "source": [
    "### Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TmmidCT3zsQu"
   },
   "outputs": [],
   "source": [
    "def calculate_real_retrieval_metrics(ground_truth_links: list, retrieved_docs: list, top_k_values: list = None):\n",
    "    \"\"\"\n",
    "    Calculate retrieval metrics using ranx for MAP, MRR, nDCG, Precision, Recall, and F1.\n",
    "\n",
    "    This function is strict and relies exclusively on ranx. It will raise an\n",
    "    error if ranx is not installed or if any other issue occurs during evaluation.\n",
    "    No fallback to manual calculation is provided.\n",
    "    \"\"\"\n",
    "    import warnings\n",
    "    # 1. Strict dependency check\n",
    "    try:\n",
    "        from ranx import Qrels, Run, evaluate\n",
    "    except ImportError:\n",
    "        raise ImportError(\"ranx is not installed. Please install it with 'pip install ranx' to use this function.\")\n",
    "    '''\n",
    "    from pprint import pprint\n",
    "    print(\"\\n[DEBUG] Documents entering the method:\")\n",
    "    for i, doc in enumerate(retrieved_docs, start=1):\n",
    "      print(f\"\\nDocument #{i}:\")\n",
    "      safe_doc = doc.copy()\n",
    "      if \"content\" in safe_doc and isinstance(safe_doc[\"content\"], str):\n",
    "          text = safe_doc[\"content\"]\n",
    "          safe_doc[\"content\"] = text[:20] + \"...\" if len(text) > 20 else text\n",
    "      pprint(safe_doc)\n",
    "\n",
    "    print(\"\\n[DEBUG] ground_truth_links entering the method:\")\n",
    "    for i, doc in enumerate(ground_truth_links, start=1):\n",
    "      print(f\"\\ground_truth_links #{i}:\")\n",
    "      pprint(doc)\n",
    "    '''\n",
    "\n",
    "    if top_k_values is None:\n",
    "        top_k_values = list(range(1, 51))\n",
    "\n",
    "    # 2. Input validation\n",
    "    if not ground_truth_links or not retrieved_docs:\n",
    "        warnings.warn(\"Input validation failed: ground_truth_links or retrieved_docs is empty. Returning zeroed metrics.\")\n",
    "        metrics = {}\n",
    "        for k in top_k_values:\n",
    "            metrics.update({\n",
    "                f\"map@{k}\": 0.0, f\"mrr@{k}\": 0.0, f\"ndcg@{k}\": 0.0,\n",
    "                f\"precision@{k}\": 0.0, f\"recall@{k}\": 0.0, f\"f1@{k}\": 0.0\n",
    "            })\n",
    "        metrics.update({\"map\": 0.0, \"mrr\": 0.0, \"ndcg\": 0.0, \"document_scores\": []})\n",
    "        return metrics\n",
    "\n",
    "    # Normalize GT links and build Qrels\n",
    "    normalized_gt = {normalize_url(link) for link in ground_truth_links if link}\n",
    "    qrels_dict = {\"q_1\": {str(link): 1.0 for link in normalized_gt}}\n",
    "\n",
    "    # 3. Enhanced score handling and Run building\n",
    "    score_keys_priority = (\"crossencoder_score\", \"cosine_similarity\")\n",
    "    chosen_key = None\n",
    "    if retrieved_docs:\n",
    "        for key in score_keys_priority:\n",
    "            if key in retrieved_docs[0] and isinstance(retrieved_docs[0][key], (int, float)):\n",
    "                chosen_key = key\n",
    "                break\n",
    "\n",
    "    run_scores = {}\n",
    "    if chosen_key:\n",
    "        for doc in retrieved_docs:\n",
    "            link = normalize_url(doc.get(\"link\", \"\"))\n",
    "            if link:\n",
    "                run_scores[link] = float(doc.get(chosen_key, 0.0))\n",
    "    else:\n",
    "        # Warn if falling back to positional ranking\n",
    "        warnings.warn(f\"No score key found from {score_keys_priority}. Falling back to positional ranking.\")\n",
    "        n = len(retrieved_docs)\n",
    "        for i, doc in enumerate(retrieved_docs):\n",
    "            link = normalize_url(doc.get(\"link\", \"\"))\n",
    "            if link:\n",
    "                run_scores[link] = float(n - i) # Higher rank = higher score\n",
    "\n",
    "    run_dict = {\"q_1\": run_scores}\n",
    "\n",
    "    # 4. Strict ranx evaluation (no fallback)\n",
    "    qrels = Qrels(qrels_dict)\n",
    "    run = Run(run_dict)\n",
    "\n",
    "    # Prepare metric list for ranx\n",
    "    metrics_to_eval = []\n",
    "    for k in top_k_values:\n",
    "        metrics_to_eval.extend([\n",
    "            f\"map@{k}\", f\"mrr@{k}\", f\"ndcg@{k}\",\n",
    "            f\"precision@{k}\", f\"recall@{k}\", f\"f1@{k}\"\n",
    "        ])\n",
    "    metrics_to_eval.extend([\"map\", \"mrr\", \"ndcg\"])\n",
    "\n",
    "    # This will raise an exception if it fails\n",
    "    results = evaluate(qrels, run, metrics=metrics_to_eval)\n",
    "\n",
    "    # Add document scores for detailed analysis\n",
    "    doc_scores = []\n",
    "    for doc in retrieved_docs:\n",
    "        doc_link = normalize_url(doc.get('link', ''))\n",
    "        is_relevant = 1 if doc_link in normalized_gt else 0\n",
    "        doc_scores.append({\n",
    "            'rank': doc.get('rank', 0),\n",
    "            'score': run_scores.get(doc_link, 0.0),\n",
    "            'link': doc.get('link', ''),\n",
    "            'title': doc.get('title', ''),\n",
    "            'is_relevant': bool(is_relevant)\n",
    "        })\n",
    "    results['document_scores'] = doc_scores\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kD1natt2-6Mc"
   },
   "source": [
    "#### RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 307,
     "status": "ok",
     "timestamp": 1763078217599,
     "user": {
      "displayName": "Harold G√≥mez",
      "userId": "03529158350759969358"
     },
     "user_tz": 180
    },
    "id": "zXeuvvmvFiKD",
    "outputId": "54d43996-b873-4e44-bd1d-b99b9c460adb"
   },
   "outputs": [],
   "source": [
    "def calculate_rag_metrics_real(question: str, context_docs: list, generated_answer: str, ground_truth: str):\n",
    "    \"\"\"Calculate comprehensive RAG metrics using real OpenAI API and BERTScore (with caching)\"\"\"\n",
    "\n",
    "    global openai_cache, semantic_similarity_model\n",
    "\n",
    "    # Get context links for cache key\n",
    "    context_links = [doc.get('link', '') for doc in context_docs[:3]]\n",
    "\n",
    "    # Create unique cache key including generated_answer and ground_truth\n",
    "    cache_input = f\"{question}|{generated_answer}|{ground_truth}\"\n",
    "\n",
    "    # Try to get from cache first\n",
    "    if openai_cache:\n",
    "        cached = openai_cache.get(cache_input, context_links, prompt_type=\"rag_metrics\")\n",
    "        if cached:\n",
    "            # Return cached metrics (excluding timestamp and context_links)\n",
    "            return {k: v for k, v in cached.items() if k not in ['timestamp', 'context_links']}\n",
    "\n",
    "    # Not in cache, calculate metrics\n",
    "    try:\n",
    "        client = OpenAI(api_key=os.getenv('OPENAI_API_KEY'))\n",
    "\n",
    "        # Prepare context\n",
    "        context_text = \"\\n\".join([doc.get('content', '')[:3000] for doc in context_docs[:3]])\n",
    "\n",
    "        # OPTIMIZED: Single API call for all RAGAS metrics (6 calls ‚Üí 1 call, 83% cost reduction)\n",
    "        combined_prompt = f\"\"\"Evaluate this RAG system output across 5 dimensions. Respond ONLY with a JSON object.\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Context: {context_text}\n",
    "\n",
    "Generated Answer: {generated_answer}\n",
    "\n",
    "Ground Truth Answer: {ground_truth if ground_truth else \"Not provided\"}\n",
    "\n",
    "Rate each dimension on a 1-5 scale and respond in this EXACT JSON format:\n",
    "{{\n",
    "  \"faithfulness\": <1-5>,\n",
    "  \"answer_relevancy\": <1-5>,\n",
    "  \"answer_correctness\": <1-5>,\n",
    "  \"context_precision\": <1-5>,\n",
    "  \"context_recall\": <1-5>\n",
    "}}\n",
    "\n",
    "Dimension definitions:\n",
    "- faithfulness: Does the answer contradict the context? (1=contradicts, 5=fully supported)\n",
    "- answer_relevancy: Is the answer relevant to the question? (1=irrelevant, 5=perfect)\n",
    "- answer_correctness: How correct is the answer vs ground truth? (1=wrong, 5=correct, 3=no ground truth)\n",
    "- context_precision: How relevant is the context for answering? (1=irrelevant, 5=precise)\n",
    "- context_recall: Does context have all info needed for ground truth? (1=missing most, 5=complete, 3=no ground truth)\n",
    "\n",
    "Respond with ONLY the JSON object, no other text.\"\"\"\n",
    "\n",
    "        # Initialize scores\n",
    "        faithfulness_score = None\n",
    "        relevancy_score = None\n",
    "        correctness_score = None\n",
    "        context_precision_score = None\n",
    "        context_recall_score = None\n",
    "\n",
    "        try:\n",
    "            ragas_response = client.chat.completions.create(\n",
    "                model=\"gpt-3.5-turbo-0125\",\n",
    "                messages=[{\"role\": \"user\", \"content\": combined_prompt}],\n",
    "                max_tokens=150,\n",
    "                temperature=0,\n",
    "                response_format={\"type\": \"json_object\"}  # Force JSON response\n",
    "            )\n",
    "\n",
    "            # Parse JSON response\n",
    "            metrics_json = json.loads(ragas_response.choices[0].message.content)\n",
    "\n",
    "            # Extract and normalize scores (1-5 scale to 0-1)\n",
    "            faithfulness_score = float(metrics_json.get(\"faithfulness\", 0)) / 5.0 if metrics_json.get(\"faithfulness\") else None\n",
    "            relevancy_score = float(metrics_json.get(\"answer_relevancy\", 0)) / 5.0 if metrics_json.get(\"answer_relevancy\") else None\n",
    "            correctness_score = float(metrics_json.get(\"answer_correctness\", 0)) / 5.0 if metrics_json.get(\"answer_correctness\") else None\n",
    "            context_precision_score = float(metrics_json.get(\"context_precision\", 0)) / 5.0 if metrics_json.get(\"context_precision\") else None\n",
    "            context_recall_score = float(metrics_json.get(\"context_recall\", 0)) / 5.0 if metrics_json.get(\"context_recall\") else None\n",
    "\n",
    "        except json.JSONDecodeError as e:\n",
    "            print(f\"‚ö†Ô∏è Failed to parse RAGAS JSON response: {e}\")\n",
    "            # Scores remain None\n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è Failed to calculate RAGAS metrics: {e}\")\n",
    "            # Scores remain None\n",
    "\n",
    "        # 6. BERTScore metrics (precision, recall, f1) and Semantic Similarity\n",
    "        bert_precision = None\n",
    "        bert_recall = None\n",
    "        bert_f1 = None\n",
    "        semantic_similarity = None\n",
    "\n",
    "        try:\n",
    "            if ground_truth and generated_answer:\n",
    "                # Calculate real BERTScore using bert-score library\n",
    "                from bert_score import score as bert_score_fn\n",
    "\n",
    "                P, R, F1 = bert_score_fn(\n",
    "                    [generated_answer],\n",
    "                    [ground_truth],\n",
    "                    lang='en',\n",
    "                    model_type='microsoft/deberta-base-mnli',\n",
    "                    device='cuda' if torch.cuda.is_available() else 'cpu',\n",
    "                    batch_size=1,\n",
    "                    verbose=False\n",
    "                )\n",
    "\n",
    "                # Free up GPU memory to prevent error CUDA out of memory\n",
    "                torch.cuda.empty_cache()\n",
    "\n",
    "                bert_precision = float(P[0])\n",
    "                bert_recall = float(R[0])\n",
    "                bert_f1s = float(F1[0])\n",
    "\n",
    "                # Eliminar variables para liberar memoria\n",
    "                del P, R, F1\n",
    "\n",
    "                # Calculate semantic similarity separately using sentence transformers\n",
    "                # Use global model to avoid loading it 2067 times (GPU memory leak fix)\n",
    "                if semantic_similarity_model is not None:\n",
    "                    gt_embedding = semantic_similarity_model.encode(ground_truth)\n",
    "                    answer_embedding = semantic_similarity_model.encode(generated_answer)\n",
    "\n",
    "                    similarity = cosine_similarity(\n",
    "                        gt_embedding.reshape(1, -1),\n",
    "                        answer_embedding.reshape(1, -1)\n",
    "                    )[0][0]\n",
    "                    semantic_similarity = float(similarity)\n",
    "                else:\n",
    "                    # Fallback if model not initialized (shouldn't happen)\n",
    "                    print(\"‚ö†Ô∏è Warning: semantic_similarity_model not initialized\")\n",
    "                    semantic_similarity = None\n",
    "\n",
    "        except Exception as e:\n",
    "            # Fallback in case of errors\n",
    "            print(f\"‚ö†Ô∏è Failed to calculate BERTScore/semantic similarity: {e}\")\n",
    "            bert_precision = None\n",
    "            bert_recall = None\n",
    "            bert_f1 = None\n",
    "            semantic_similarity = None\n",
    "\n",
    "        # Prepare result dict\n",
    "        result = {\n",
    "            # RAGAS metrics\n",
    "            'faithfulness': faithfulness_score,\n",
    "            'answer_relevancy': relevancy_score,  # Note: using 'answer_relevancy' (with y) as expected by Streamlit\n",
    "            'answer_correctness': correctness_score,\n",
    "            'context_precision': context_precision_score,\n",
    "            'context_recall': context_recall_score,\n",
    "            'semantic_similarity': semantic_similarity,\n",
    "\n",
    "            # BERTScore metrics\n",
    "            'bert_precision': bert_precision,\n",
    "            'bert_recall': bert_recall,\n",
    "            'bert_f1': bert_f1,\n",
    "\n",
    "            # Additional fields\n",
    "            'evaluation_method': 'Complete_RAGAS_OpenAI_BERTScore'\n",
    "        }\n",
    "\n",
    "        # Save to cache\n",
    "        if openai_cache:\n",
    "            openai_cache.set(\n",
    "                cache_input,\n",
    "                {**result, \"context_links\": context_links},\n",
    "                context_links,\n",
    "                prompt_type=\"rag_metrics\"\n",
    "            )\n",
    "\n",
    "        return result\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è CRITICAL error in RAG metrics calculation: {e}\")\n",
    "        return {\n",
    "            # RAGAS metrics - all None on critical error\n",
    "            'faithfulness': None,\n",
    "            'answer_relevancy': None,\n",
    "            'answer_correctness': None,\n",
    "            'context_precision': None,\n",
    "            'context_recall': None,\n",
    "            'semantic_similarity': None,\n",
    "\n",
    "            # BERTScore metrics - all None on critical error\n",
    "            'bert_precision': None,\n",
    "            'bert_recall': None,\n",
    "            'bert_f1': None,\n",
    "\n",
    "            # Additional fields\n",
    "            'evaluation_method': 'Critical_Error_Fallback'\n",
    "        }\n",
    "\n",
    "def generate_rag_answer(question: str, context_docs: list):\n",
    "    \"\"\"Generate answer using OpenAI GPT and context documents (with caching)\"\"\"\n",
    "\n",
    "    global openai_cache\n",
    "\n",
    "    # Get context links for cache key\n",
    "    context_links = [doc.get('link', '') for doc in context_docs[:3]]\n",
    "\n",
    "    # Try to get from cache first\n",
    "    if openai_cache:\n",
    "        cached = openai_cache.get(question, context_links, prompt_type=\"rag_answer\")\n",
    "        if cached:\n",
    "            return cached.get('generated_answer', '')\n",
    "\n",
    "    # Not in cache, call OpenAI\n",
    "    try:\n",
    "        client = OpenAI(api_key=os.getenv('OPENAI_API_KEY'))\n",
    "\n",
    "        # Prepare context from top documents\n",
    "        context_text = \"\\n\\n\".join([\n",
    "            f\"Document {i+1}: {doc.get('content', '')[:800]}\"\n",
    "            for i, doc in enumerate(context_docs[:3])\n",
    "        ])\n",
    "\n",
    "        prompt = f\"\"\"\n",
    "        Based on the following context documents, answer the question accurately and concisely.\n",
    "\n",
    "        Context:\n",
    "        {context_text}\n",
    "\n",
    "        Question: {question}\n",
    "\n",
    "        Answer:\n",
    "        \"\"\"\n",
    "\n",
    "        response = client.chat.completions.create(\n",
    "            model=\"gpt-3.5-turbo-0125\",\n",
    "            messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "            max_tokens=200,\n",
    "            temperature=0  # DETERMINISTIC: Changed from 0.1 to 0 for reproducibility\n",
    "        )\n",
    "\n",
    "        generated_answer = response.choices[0].message.content.strip()\n",
    "\n",
    "        # Save to cache\n",
    "        if openai_cache:\n",
    "            openai_cache.set(\n",
    "                question,\n",
    "                {\"generated_answer\": generated_answer, \"context_links\": context_links},\n",
    "                context_links,\n",
    "                prompt_type=\"rag_answer\"\n",
    "            )\n",
    "\n",
    "        return generated_answer\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è Error generating RAG answer: {e}\")\n",
    "        return f\"Error generating answer: {str(e)}\"\n",
    "\n",
    "def get_optimal_batch_size(documents: list, max_content_length: int = 1200) -> int:\n",
    "    \"\"\"Calculate optimal batch size based on content length (MEJORA DE RENDIMIENTO)\"\"\"\n",
    "    if not documents:\n",
    "        return 4\n",
    "\n",
    "    # Estimar longitud promedio del contenido (sample primeros 5 documentos)\n",
    "    sample_size = min(5, len(documents))\n",
    "    avg_content_length = np.mean([\n",
    "        len(doc.get('content', '')[:max_content_length]) + len(doc.get('title', ''))\n",
    "        for doc in documents[:sample_size]\n",
    "    ])\n",
    "\n",
    "    # Batch size inversamente proporcional a longitud de contenido\n",
    "    if avg_content_length > 1200:\n",
    "        return 2  # Documentos largos ‚Üí batch peque√±o para evitar OOM\n",
    "    elif avg_content_length > 600:\n",
    "        return 4  # Documentos medianos ‚Üí batch est√°ndar\n",
    "    else:\n",
    "        return 8  # Documentos cortos ‚Üí batch grande para velocidad\n",
    "\n",
    "def rerank_with_cross_encoder(question: str, documents: list, cross_encoder, top_k: int = 10):\n",
    "    \"\"\"\n",
    "    Rerank documents using CrossEncoder with MEJORAS CR√çTICAS:\n",
    "    - Usa t√≠tulo + contenido (no solo contenido)\n",
    "    - Batch size adaptativo seg√∫n longitud de documentos\n",
    "    - Mejora esperada: +15-25% NDCG@10, +10-20% MRR\n",
    "    \"\"\"\n",
    "\n",
    "    if not cross_encoder or not documents:\n",
    "        return documents\n",
    "\n",
    "    try:\n",
    "        # MEJORA 1: Prepare query-document pairs WITH TITLE + CONTENT\n",
    "        pairs = []\n",
    "        for doc in documents:\n",
    "            title = doc.get('title', '').strip()\n",
    "            content = doc.get('content', '').strip()\n",
    "\n",
    "            # Combinar t√≠tulo y contenido limitado\n",
    "            if title:\n",
    "                # T√≠tulo + primeros 1200 caracteres de contenido\n",
    "                combined_text = f\"{title}: {content[:1200]}\"\n",
    "            else:\n",
    "                # Solo contenido limitado si no hay t√≠tulo\n",
    "                combined_text = content[:1500]\n",
    "\n",
    "            pairs.append([question, combined_text])\n",
    "\n",
    "        # MEJORA 2: Batch size adaptativo seg√∫n longitud de documentos\n",
    "        optimal_batch_size = get_optimal_batch_size(documents, max_content_length=1200)\n",
    "\n",
    "        # Get CrossEncoder scores con batch size optimizado\n",
    "        scores = cross_encoder.predict(pairs, batch_size=optimal_batch_size)\n",
    "\n",
    "        # Apply Min-Max normalization to convert logits to [0, 1] range\n",
    "        scores = np.array(scores)\n",
    "        if len(scores) > 1 and scores.max() != scores.min():\n",
    "            # Standard Min-Max normalization\n",
    "            normalized_scores = (scores - scores.min()) / (scores.max() - scores.min())\n",
    "        else:\n",
    "            # Fallback for edge cases (all scores identical)\n",
    "            normalized_scores = np.full_like(scores, 0.5)\n",
    "\n",
    "        # Add scores to documents and sort\n",
    "        for i, doc in enumerate(documents):\n",
    "            doc['crossencoder_score'] = float(normalized_scores[i])\n",
    "\n",
    "        # DETERMINISTIC: Sort by CrossEncoder score desc, then by link asc (tie-breaking)\n",
    "        reranked = sorted(\n",
    "            documents,\n",
    "            key=lambda x: (-x['crossencoder_score'], x.get('link', ''))\n",
    "        )\n",
    "\n",
    "        # Update ranks\n",
    "        for i, doc in enumerate(reranked):\n",
    "            doc['rank'] = i + 1\n",
    "\n",
    "        return reranked[:top_k]\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è CrossEncoder reranking error: {e}\")\n",
    "        return documents\n",
    "\n",
    "def evaluate_single_model_complete(model_name: str, data_pipeline: EmbeddedDataPipeline,\n",
    "                                   questions_data: list, reranking_method: str = 'crossencoder',\n",
    "                                   top_k: int = 10, generate_rag: bool = True):\n",
    "    \"\"\"Complete evaluation for a single model with real embeddings and metrics\"\"\"\n",
    "\n",
    "    print(f\"\\nüîÑ Evaluating {model_name}...\")\n",
    "\n",
    "    retriever = data_pipeline.retrievers.get(model_name)\n",
    "    if not retriever or len(retriever.df) == 0:\n",
    "        print(f\"‚ùå No valid retriever for {model_name}\")\n",
    "        return None\n",
    "\n",
    "    all_before_metrics = []\n",
    "    all_after_metrics = []\n",
    "    individual_rag_metrics = []\n",
    "\n",
    "    # Score accumulators\n",
    "    before_scores = []\n",
    "    after_scores = []\n",
    "    ce_scores = []\n",
    "    total_docs_reranked = 0\n",
    "\n",
    "    for i, question_data in enumerate(tqdm(questions_data, desc=f\"{model_name}\")):\n",
    "        question = question_data['question']\n",
    "        ground_truth_links = question_data.get('validated_links', [])\n",
    "        ground_truth_answer = question_data.get('accepted_answer', '')\n",
    "\n",
    "        # Memory cleanup and cache checkpoint\n",
    "        if i % 100 == 0 and i > 0:\n",
    "          clear_gpu_memory()\n",
    "          print(f\"üßπ Memoria limpiada en pregunta {i}\")\n",
    "\n",
    "        # Save cache checkpoint every 50 questions\n",
    "        if i % 50 == 0 and i > 0 and openai_cache:\n",
    "            openai_cache.save_cache()\n",
    "            stats = openai_cache.get_stats()\n",
    "            print(f\"üíæ Cache checkpoint @ {i}: {stats['cache_hits']} hits, {stats['cache_misses']} misses, hit rate: {stats['hit_rate']:.1f}%\")\n",
    "\n",
    "        # Generate real query embedding\n",
    "        query_embedding = data_pipeline.real_embedding_generator.generate_query_embedding(\n",
    "            question, model_name\n",
    "        )\n",
    "\n",
    "        # Retrieve documents\n",
    "        retrieved_docs = retriever.search(query_embedding, top_k=top_k)\n",
    "\n",
    "        if not retrieved_docs:\n",
    "            continue\n",
    "\n",
    "        # Calculate BEFORE reranking metrics\n",
    "        before_metrics = calculate_real_retrieval_metrics(\n",
    "            ground_truth_links, retrieved_docs, list(range(1, top_k + 1))\n",
    "        )\n",
    "\n",
    "        # Calculate average cosine similarity (before)\n",
    "        before_avg_score = np.mean([doc['cosine_similarity'] for doc in retrieved_docs])\n",
    "        before_scores.append(before_avg_score)\n",
    "\n",
    "        all_before_metrics.append(before_metrics)\n",
    "\n",
    "        # AFTER reranking\n",
    "        if reranking_method == 'crossencoder' and data_pipeline.cross_encoder:\n",
    "            # Rerank with CrossEncoder\n",
    "            reranked_docs = rerank_with_cross_encoder(\n",
    "                question, retrieved_docs, data_pipeline.cross_encoder, top_k\n",
    "            )\n",
    "\n",
    "            # Calculate AFTER reranking metrics\n",
    "            after_metrics = calculate_real_retrieval_metrics(\n",
    "                ground_truth_links, reranked_docs, list(range(1, top_k + 1))\n",
    "            )\n",
    "\n",
    "            # Calculate CrossEncoder scores\n",
    "            ce_question_scores = [doc.get('crossencoder_score', 0) for doc in reranked_docs]\n",
    "            ce_avg_score = np.mean(ce_question_scores) if ce_question_scores else 0\n",
    "            ce_scores.append(ce_avg_score)\n",
    "\n",
    "            # After score (using original cosine similarities)\n",
    "            after_avg_score = np.mean([doc['cosine_similarity'] for doc in reranked_docs])\n",
    "            after_scores.append(after_avg_score)\n",
    "\n",
    "            total_docs_reranked += len(reranked_docs)\n",
    "\n",
    "            # Store CrossEncoder specific metrics\n",
    "            after_metrics['model_crossencoder_scores'] = ce_question_scores\n",
    "            after_metrics['model_avg_crossencoder_score'] = ce_avg_score\n",
    "            after_metrics['model_total_documents_reranked'] = len(reranked_docs)\n",
    "\n",
    "        else:\n",
    "            # No reranking\n",
    "            after_metrics = before_metrics.copy()\n",
    "            reranked_docs = retrieved_docs\n",
    "            after_scores.append(before_avg_score)\n",
    "\n",
    "        all_after_metrics.append(after_metrics)\n",
    "\n",
    "        # RAG Metrics (using reranked docs as context)\n",
    "        if generate_rag:\n",
    "            try:\n",
    "                # Generate answer\n",
    "                generated_answer = generate_rag_answer(question, reranked_docs[:3])\n",
    "\n",
    "                # Calculate RAG metrics\n",
    "                rag_metrics = calculate_rag_metrics_real(\n",
    "                    question, reranked_docs[:3], generated_answer, ground_truth_answer\n",
    "                )\n",
    "\n",
    "                rag_metrics['question_index'] = i\n",
    "                rag_metrics['generated_answer'] = generated_answer\n",
    "                individual_rag_metrics.append(rag_metrics)\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"‚ö†Ô∏è RAG metrics error for question {i}: {e}\")\n",
    "\n",
    "    # Calculate averages\n",
    "    def calculate_averages(metrics_list):\n",
    "        if not metrics_list:\n",
    "            return {}\n",
    "\n",
    "        all_keys = set()\n",
    "        for metrics in metrics_list:\n",
    "            all_keys.update(metrics.keys())\n",
    "\n",
    "        averages = {}\n",
    "        for key in all_keys:\n",
    "            if key != 'document_scores':  # Skip document scores in averages\n",
    "                values = [m.get(key, 0) for m in metrics_list if isinstance(m.get(key), (int, float))]\n",
    "                if values:\n",
    "                    averages[key] = np.mean(values)\n",
    "\n",
    "        return averages\n",
    "\n",
    "    avg_before_metrics = calculate_averages(all_before_metrics)\n",
    "    avg_after_metrics = calculate_averages(all_after_metrics)\n",
    "\n",
    "    # Add model-level score metrics\n",
    "    avg_before_metrics['model_avg_score'] = np.mean(before_scores) if before_scores else 0\n",
    "    avg_after_metrics['model_avg_score'] = np.mean(after_scores) if after_scores else 0\n",
    "\n",
    "    if reranking_method == 'crossencoder' and ce_scores:\n",
    "        avg_after_metrics['model_avg_crossencoder_score'] = np.mean(ce_scores)\n",
    "        avg_after_metrics['model_total_documents_reranked'] = total_docs_reranked\n",
    "\n",
    "    # RAG averages - Complete RAGAS + BERTScore metrics\n",
    "    # CRITICAL: Filter out None values before calculating averages\n",
    "    rag_averages = {}\n",
    "    if individual_rag_metrics:\n",
    "        # Helper function to safely calculate average excluding None values\n",
    "        def safe_mean(metric_name):\n",
    "            values = [r[metric_name] for r in individual_rag_metrics if r[metric_name] is not None]\n",
    "            return np.mean(values) if values else None\n",
    "\n",
    "        rag_averages = {\n",
    "            # RAGAS metrics averages (excluding None values)\n",
    "            'avg_faithfulness': safe_mean('faithfulness'),\n",
    "            'avg_answer_relevance': safe_mean('answer_relevancy'),  # Note: 'answer_relevancy' with y\n",
    "            'avg_answer_correctness': safe_mean('answer_correctness'),\n",
    "            'avg_context_precision': safe_mean('context_precision'),\n",
    "            'avg_context_recall': safe_mean('context_recall'),\n",
    "            'avg_semantic_similarity': safe_mean('semantic_similarity'),\n",
    "\n",
    "            # BERTScore metrics averages (excluding None values)\n",
    "            'avg_bert_precision': safe_mean('bert_precision'),\n",
    "            'avg_bert_recall': safe_mean('bert_recall'),\n",
    "            'avg_bert_f1': safe_mean('bert_f1'),\n",
    "\n",
    "            # Status and count\n",
    "            'rag_available': True,\n",
    "            'total_rag_evaluations': len(individual_rag_metrics),\n",
    "            'successful_calculations': {\n",
    "                'faithfulness': len([r for r in individual_rag_metrics if r['faithfulness'] is not None]),\n",
    "                'answer_relevancy': len([r for r in individual_rag_metrics if r['answer_relevancy'] is not None]),\n",
    "                'answer_correctness': len([r for r in individual_rag_metrics if r['answer_correctness'] is not None]),\n",
    "                'context_precision': len([r for r in individual_rag_metrics if r['context_precision'] is not None]),\n",
    "                'context_recall': len([r for r in individual_rag_metrics if r['context_recall'] is not None]),\n",
    "                'semantic_similarity': len([r for r in individual_rag_metrics if r['semantic_similarity'] is not None]),\n",
    "                'bert_precision': len([r for r in individual_rag_metrics if r['bert_precision'] is not None]),\n",
    "                'bert_recall': len([r for r in individual_rag_metrics if r['bert_recall'] is not None]),\n",
    "                'bert_f1': len([r for r in individual_rag_metrics if r['bert_f1'] is not None])\n",
    "            }\n",
    "        }\n",
    "    else:\n",
    "        rag_averages = {'rag_available': False}\n",
    "\n",
    "    return {\n",
    "        'model_name': model_name,\n",
    "        'full_model_name': model_name,\n",
    "        'num_questions_evaluated': len(questions_data),\n",
    "        'embedding_dimensions': retriever.embedding_dim,\n",
    "        'total_documents': len(retriever.df),\n",
    "        'avg_before_metrics': avg_before_metrics,\n",
    "        'avg_after_metrics': avg_after_metrics,\n",
    "        'all_before_metrics': all_before_metrics,\n",
    "        'all_after_metrics': all_after_metrics,\n",
    "        'rag_metrics': rag_averages,\n",
    "        'individual_rag_metrics': individual_rag_metrics\n",
    "    }\n",
    "\n",
    "def run_real_complete_evaluation(available_models: list, config_data: dict,\n",
    "                                 data_pipeline: EmbeddedDataPipeline,\n",
    "                                 reranking_method: str = 'crossencoder',\n",
    "                                 max_questions: int = None, debug: bool = False):\n",
    "    \"\"\"Run complete evaluation with real embeddings and metrics\"\"\"\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    # Buscar questions_data primero, luego questions (compatibilidad)\n",
    "    questions_data = config_data.get('questions_data', config_data.get('questions', []))\n",
    "    if max_questions:\n",
    "        questions_data = questions_data[:max_questions]\n",
    "\n",
    "    params = config_data.get('params', {})\n",
    "    # Buscar top_k primero en el nivel ra√≠z, luego en params\n",
    "    top_k = config_data.get('top_k', params.get('top_k', 10))\n",
    "    # Buscar generate_rag_metrics en nivel ra√≠z, luego en params\n",
    "    generate_rag = config_data.get('generate_rag_metrics', params.get('generate_rag_metrics', True))\n",
    "\n",
    "    # Validate that all models are loaded before starting evaluation\n",
    "    print(\"üîç Validating models...\")\n",
    "    data_pipeline.real_embedding_generator.validate_models()\n",
    "\n",
    "    # Initialize OpenAI cache\n",
    "    global openai_cache\n",
    "    cache_path = os.path.join(data_pipeline.base_path, '..', 'openai_responses_cache.json')\n",
    "    openai_cache = OpenAICache(cache_path)\n",
    "    print(f\"üíæ OpenAI cache initialized: {cache_path}\")\n",
    "\n",
    "    # Initialize semantic similarity model ONCE (GPU memory leak fix)\n",
    "    global semantic_similarity_model\n",
    "    print(\"üîÑ Loading semantic similarity model...\")\n",
    "    semantic_similarity_model = SentenceTransformer('sentence-transformers/all-mpnet-base-v2')\n",
    "    print(\"‚úÖ Semantic similarity model loaded (will be reused across all 2067 questions)\")\n",
    "\n",
    "    print(f\"üöÄ Starting evaluation of {len(available_models)} models on {len(questions_data)} questions\")\n",
    "    print(f\"üìä Reranking method: {reranking_method}\")\n",
    "    print(f\"üéØ Top-K: {top_k}\")\n",
    "    print(f\"ü§ñ RAG metrics: {generate_rag}\")\n",
    "\n",
    "    all_model_results = {}\n",
    "\n",
    "    for model_name in available_models:\n",
    "        result = evaluate_single_model_complete(\n",
    "            model_name=model_name,\n",
    "            data_pipeline=data_pipeline,\n",
    "            questions_data=questions_data,\n",
    "            reranking_method=reranking_method,\n",
    "            top_k=top_k,\n",
    "            generate_rag=generate_rag\n",
    "        )\n",
    "\n",
    "        if result:\n",
    "            all_model_results[model_name] = result\n",
    "\n",
    "            # Brief summary\n",
    "            avg_f1 = result['avg_after_metrics'].get('f1@5', 0)\n",
    "            avg_score = result['avg_after_metrics'].get('model_avg_score', 0)\n",
    "            print(f\"  ‚úÖ {model_name}: F1@5={avg_f1:.3f}, Score={avg_score:.3f}\")\n",
    "\n",
    "    end_time = time.time()\n",
    "    evaluation_duration = end_time - start_time\n",
    "\n",
    "    # Save final cache and show statistics\n",
    "    if openai_cache:\n",
    "        openai_cache.save_cache()\n",
    "        stats = openai_cache.get_stats()\n",
    "        print(f\"\\nüíæ OpenAI Cache Statistics:\")\n",
    "        print(f\"  üìä Total queries: {stats['total_queries']}\")\n",
    "        print(f\"  ‚úÖ Cache hits: {stats['cache_hits']} ({stats['hit_rate']:.1f}%)\")\n",
    "        print(f\"  ‚ö†Ô∏è  Cache misses: {stats['cache_misses']}\")\n",
    "        print(f\"  üí∞ Estimated cost saved: ${stats['estimated_cost_saved']:.2f}\")\n",
    "\n",
    "    evaluation_params = {\n",
    "        'num_questions': len(questions_data),\n",
    "        'models_evaluated': len(all_model_results),\n",
    "        'reranking_method': reranking_method,\n",
    "        'top_k': top_k,\n",
    "        'generate_rag_metrics': generate_rag\n",
    "    }\n",
    "\n",
    "    return {\n",
    "        'all_model_results': all_model_results,\n",
    "        'evaluation_duration': evaluation_duration,\n",
    "        'evaluation_params': evaluation_params\n",
    "    }\n",
    "\n",
    "def embedded_process_and_save_results(all_model_results: dict, output_path: str,\n",
    "                                      evaluation_params: dict, evaluation_duration: float):\n",
    "    \"\"\"Process and save results in Streamlit-compatible format\"\"\"\n",
    "\n",
    "    # Chile timezone\n",
    "    chile_tz = pytz.timezone('America/Santiago')\n",
    "    now_utc = datetime.now(pytz.UTC)\n",
    "    now_chile = now_utc.astimezone(chile_tz)\n",
    "\n",
    "    # Generate filename with date format YYYYMMDD_HHMMSS\n",
    "    timestamp_str = now_chile.strftime('%Y%m%d_%H%M%S')\n",
    "    filename = f\"cumulative_results_{timestamp_str}.json\"\n",
    "    filepath = os.path.join(output_path, filename)\n",
    "\n",
    "    # Create comprehensive results structure\n",
    "    results_data = {\n",
    "        'config': evaluation_params,\n",
    "        'evaluation_info': {\n",
    "            'timestamp': now_chile.isoformat(),\n",
    "            'timezone': 'America/Santiago',\n",
    "            'evaluation_type': 'cumulative_metrics_colab_multi_model',\n",
    "            'total_duration_seconds': evaluation_duration,\n",
    "            'models_evaluated': len(all_model_results),\n",
    "            'questions_per_model': evaluation_params.get('num_questions', 0),\n",
    "            'enhanced_display_compatible': True,\n",
    "            'data_verification': {\n",
    "                'is_real_data': True,\n",
    "                'no_simulation': True,\n",
    "                'no_random_values': True,\n",
    "                'rag_framework': 'Complete_RAGAS_with_OpenAI_API',\n",
    "                'reranking_method': f\"{evaluation_params.get('reranking_method', 'none')}_reranking\"\n",
    "            }\n",
    "        },\n",
    "        'results': all_model_results\n",
    "    }\n",
    "\n",
    "    # Save to file\n",
    "    try:\n",
    "        with open(filepath, 'w', encoding='utf-8') as f:\n",
    "            json.dump(results_data, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "        print(f\"‚úÖ Results saved: {filename}\")\n",
    "\n",
    "        return {\n",
    "            'json': filepath,\n",
    "            'filename': filename,\n",
    "            'chile_time': now_chile.strftime('%Y-%m-%d %H:%M:%S %Z')\n",
    "        }\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error saving results: {e}\")\n",
    "        return None\n",
    "\n",
    "print(\"‚úÖ Complete evaluation code loaded\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "A0y7-dY8FiKG"
   },
   "source": [
    "## ‚öôÔ∏è 3. Load Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 247335,
     "status": "ok",
     "timestamp": 1763078464936,
     "user": {
      "displayName": "Harold G√≥mez",
      "userId": "03529158350759969358"
     },
     "user_tz": 180
    },
    "id": "qPazEnE4FiKG",
    "outputId": "32b46376-5e54-4c1c-9c04-5da8ea9fd1ea"
   },
   "outputs": [],
   "source": [
    "# Find latest config file\n",
    "config_files = glob.glob(ACUMULATIVE_PATH + 'evaluation_config_*.json')\n",
    "\n",
    "if config_files:\n",
    "    files_with_timestamps = []\n",
    "    for file in config_files:\n",
    "        match = re.search(r'evaluation_config_(\\d+)\\.json', file)\n",
    "        if match:\n",
    "            timestamp = int(match.group(1))\n",
    "            files_with_timestamps.append((timestamp, file))\n",
    "\n",
    "    if files_with_timestamps:\n",
    "        files_with_timestamps.sort(reverse=True)\n",
    "        CONFIG_FILE_PATH = files_with_timestamps[0][1]\n",
    "        latest_timestamp = files_with_timestamps[0][0]\n",
    "        readable_time = datetime.fromtimestamp(latest_timestamp).strftime('%Y-%m-%d %H:%M:%S')\n",
    "        print(f\"‚úÖ Latest config: {os.path.basename(CONFIG_FILE_PATH)} ({readable_time})\")\n",
    "    else:\n",
    "        CONFIG_FILE_PATH = ACUMULATIVE_PATH + 'questions_with_links.json'\n",
    "        print(\"‚ö†Ô∏è Using default questions file\")\n",
    "else:\n",
    "    CONFIG_FILE_PATH = ACUMULATIVE_PATH + 'questions_with_links.json'\n",
    "    print(\"‚ö†Ô∏è No config files found, using default\")\n",
    "\n",
    "# Initialize pipeline and load config\n",
    "data_pipeline = EmbeddedDataPipeline(BASE_PATH, EMBEDDING_FILES)\n",
    "config_data = data_pipeline.load_config_file(CONFIG_FILE_PATH)\n",
    "\n",
    "if config_data and config_data.get('questions_data'):\n",
    "    # Handle new config format with questions_data\n",
    "    questions_data = config_data['questions_data']\n",
    "    params = config_data.get('data_config', {})  # New format uses data_config instead of params\n",
    "\n",
    "    # Convert new format to expected format for compatibility\n",
    "    config_data['questions'] = questions_data\n",
    "    config_data['params'] = {\n",
    "        'top_k': params.get('top_k', 10),\n",
    "        'reranking_method': params.get('reranking_method', 'crossencoder'),\n",
    "        'generate_rag_metrics': True,  # Default for new format\n",
    "        'use_llm_reranker': params.get('reranking_method', 'crossencoder') == 'crossencoder',\n",
    "        'num_questions': params.get('num_questions', len(questions_data))\n",
    "    }\n",
    "    params = config_data['params']\n",
    "\n",
    "    print(f\"‚úÖ New config format loaded: {len(questions_data)} questions\")\n",
    "\n",
    "elif config_data and config_data.get('questions'):\n",
    "    # Handle legacy format\n",
    "    params = config_data['params']\n",
    "    print(f\"‚úÖ Legacy config format loaded: {len(config_data['questions'])} questions\")\n",
    "\n",
    "else:\n",
    "    print(\"‚ùå Error loading config - no questions found\")\n",
    "    if config_data:\n",
    "        print(f\"Available keys: {list(config_data.keys())}\")\n",
    "    RERANKING_METHOD = 'crossencoder'\n",
    "    params = {'top_k': 10, 'reranking_method': 'crossencoder', 'generate_rag_metrics': True}\n",
    "\n",
    "if config_data and config_data.get('questions'):\n",
    "    # Get reranking method with backward compatibility\n",
    "    RERANKING_METHOD = params.get('reranking_method', 'crossencoder')\n",
    "    USE_LLM_RERANKING = params.get('use_llm_reranker', True)\n",
    "\n",
    "    if RERANKING_METHOD == 'crossencoder' and not USE_LLM_RERANKING:\n",
    "        RERANKING_METHOD = 'none'\n",
    "\n",
    "    print(f\"üîÑ Reranking method: {RERANKING_METHOD}\")\n",
    "    print(f\"üéØ Top-K: {params.get('top_k', 10)}\")\n",
    "    print(f\"üìä RAG metrics: {params.get('generate_rag_metrics', False)}\")\n",
    "else:\n",
    "    print(\"‚ùå Error loading config\")\n",
    "    RERANKING_METHOD = 'crossencoder'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VuVs9gqoFiKH"
   },
   "source": [
    "## üìä 4. Check Available Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 45,
     "status": "ok",
     "timestamp": 1763078464994,
     "user": {
      "displayName": "Harold G√≥mez",
      "userId": "03529158350759969358"
     },
     "user_tz": 180
    },
    "id": "LlpkPdSMFiKH",
    "outputId": "69e1b8fa-c2fd-4b01-fcc4-01f75fd229e8"
   },
   "outputs": [],
   "source": [
    "# Get system info\n",
    "system_info = data_pipeline.get_system_info()\n",
    "\n",
    "print(f\"üìä Available models:\")\n",
    "for model_name in system_info['available_models']:\n",
    "    model_info = system_info['models_info'].get(model_name, {})\n",
    "    if 'error' not in model_info:\n",
    "        print(f\"  ‚úÖ {model_name}: {model_info.get('num_documents', 0):,} docs, {model_info.get('embedding_dim', 0)}D\")\n",
    "    else:\n",
    "        print(f\"  ‚ùå {model_name}: {model_info.get('error', 'Error')}\")\n",
    "\n",
    "available_models = [name for name in system_info['available_models']\n",
    "                   if 'error' not in system_info['models_info'].get(name, {})]\n",
    "\n",
    "print(f\"\\nüéØ Models for evaluation: {available_models}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "q2GUkyxwFiKI"
   },
   "source": [
    "## üöÄ 5. Run Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 22,
     "status": "ok",
     "timestamp": 1763078465344,
     "user": {
      "displayName": "Harold G√≥mez",
      "userId": "03529158350759969358"
     },
     "user_tz": 180
    },
    "id": "UynetqSmzBRU",
    "outputId": "0dbffdcc-b797-45ea-fbc9-1219a6bb52a0"
   },
   "outputs": [],
   "source": [
    "# CONFIGURACIONES MEJORADAS\n",
    "BATCH_QUESTIONS = 15  # Reducido de 50 a 15\n",
    "\n",
    "# Limpiar memoria antes de empezar\n",
    "clear_gpu_memory()\n",
    "check_gpu_memory()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true,
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "referenced_widgets": [
      "4428d46e33e747fabbf37e75f0914359",
      "0c0573ad24b34923a4504044d11d7487",
      "a50f3aa8549e4917a3f6573c82736fd8",
      "1a9be47fbcf847af95e03d12562b739c",
      "0448cbfa3d494b8ab64cb44cb26dc3ed",
      "4fad22e4eb31482aa0415c3eba123daa",
      "6d68981eb5ae4651914009be25b0a981",
      "0615ad03c4c045d0b1a1108cc197a528",
      "fa98e3e6979543a6a6e8687f257af676",
      "0d2bdcecf8cc4d0ab221f771053534d8",
      "163d2703ed134576a86d8da550843877",
      "b3612c5641f14bbfae9f87034c5d44c7",
      "40e70e638561424897d7798925d45854",
      "79522b2d5a6f4ada85462a71160109f2",
      "a3aad76457c4437f8f901fe43f3a2761",
      "9b4be78e6034446d87e1b575a7cb9658",
      "4ccbf8e8994748b098d19d6cd79b0eca",
      "3fad4ab210bb455393e8e0e789c3a25c",
      "88d4f7ecd6024395a09683038aae27ab",
      "e6f0be3861864cedaf903f3665ab74e5",
      "d3c44059f1764dec9a248acc0e699473",
      "b8433527c1a840fbb29a93bfaf817d89",
      "d5c0fbe825b94ffdb5d24fa2936961ff",
      "fb648162c3334005ac289c774c2cd3f1",
      "2d24c0757de24dcfb9d33ad2175df8c4",
      "f955972eef4d45d9a01b76935498c74d",
      "a6dfdf7f574e4c7fb9d97c81dbf8f1ec",
      "7cc3dac5f4a04e8bac41f21c661681ed",
      "4f7c059eb9274af5a06c5ee80a333911",
      "716504937fdb456fa91f0953f183bb10",
      "fbc184ccec1d4067819f61587c277c6a",
      "13ddf33736e9480aa17d934d5b517cc7",
      "d06291ed03244bf28264d3570a77cf4b",
      "5bc21fd22f40415aa5f16a62bbf0bdf3",
      "ff35c406ba304906a9ef13426dd0fbda",
      "a80b953746134c39981d9103bfa3f44e",
      "dabdb5f1a8e24c1a9064671a429aebca",
      "0ce0fc81b9084bcd8810c4679bf5e44c",
      "376e965b23b8421b9e8c0332eb0d0266",
      "7b2a453004e241cd923a8bd440b341fd",
      "48b6ea57ecfb4c5684e031861f834a80",
      "fbf1948f6d634772878b1d953ccf73cf",
      "fe72f1e2ba66417d89633644e83addeb",
      "d55766d4b5324437b4f4c312fa991194",
      "28cde1633f2049fd984c87c71e0aba17",
      "4ab84bc1663d46e5b8780e113f94efae",
      "b6a838d380114440ba14a14ebee8d0c1",
      "20c8ad35267946ac977b177a630bee7e",
      "84bf31610aa04ca488b666fafb0f49e7",
      "92f182888768491aa04e9c90518c1dc6",
      "7a85846da9894dd6ac49946d3a983529",
      "210d9ab54bde4c289a1667cae5beb337",
      "e5cd69e17a11486ab4d25cb51ee76ee9",
      "5922d7e8ecc745809196b179e214aee7",
      "d5d68ab15499485ca297770f8ecc37b7"
     ]
    },
    "id": "WZzfzB9GFiKI",
    "outputId": "e9f7dfe0-17d3-4bf9-fc46-33c4a82b592d"
   },
   "outputs": [],
   "source": [
    "# Run evaluation\n",
    "evaluation_result = run_real_complete_evaluation(\n",
    "    available_models=available_models,\n",
    "    config_data=config_data,\n",
    "    data_pipeline=data_pipeline,\n",
    "    reranking_method=RERANKING_METHOD,\n",
    "    max_questions=None,  # Use all questions from config\n",
    "    debug=False\n",
    ")\n",
    "\n",
    "all_models_results = evaluation_result['all_model_results']\n",
    "evaluation_duration = evaluation_result['evaluation_duration']\n",
    "evaluation_params = evaluation_result['evaluation_params']\n",
    "\n",
    "print(f\"\\n‚úÖ Evaluation completed in {evaluation_duration/60:.2f} minutes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ntJ0pgfkFiKI"
   },
   "source": [
    "## üíæ 6. Save Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "S9VKAW4_FiKI",
    "outputId": "27655f9c-4f4d-4aa8-8f9b-6b263b1492d8"
   },
   "outputs": [],
   "source": [
    "# Save results\n",
    "saved_files = embedded_process_and_save_results(\n",
    "    all_model_results=all_models_results,\n",
    "    output_path=RESULTS_OUTPUT_PATH,\n",
    "    evaluation_params=evaluation_params,\n",
    "    evaluation_duration=evaluation_duration\n",
    ")\n",
    "\n",
    "if saved_files:\n",
    "    print(f\"‚úÖ Results saved:\")\n",
    "    print(f\"  üìÑ File: {os.path.basename(saved_files['json'])}\")\n",
    "    print(f\"  üåç Time: {saved_files['chile_time']}\")\n",
    "    print(f\"  ‚úÖ Format: Streamlit compatible\")\n",
    "else:\n",
    "    print(\"‚ùå Error saving results\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "F0JTubLXFiKJ"
   },
   "source": [
    "## üìà 7. Results Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "jq26SMZmFiKJ",
    "outputId": "b8513b8e-1852-4d7f-aeb8-1c880420d9d3"
   },
   "outputs": [],
   "source": [
    "# Display results summary\n",
    "# Make sure to run the previous cell (Save Results) first!\n",
    "if saved_files and 'json' in saved_files:\n",
    "    import json\n",
    "\n",
    "    with open(saved_files['json'], 'r') as f:\n",
    "        final_results = json.load(f)\n",
    "\n",
    "    print(\"üìä RESULTS SUMMARY\")\n",
    "    print(\"=\"*50)\n",
    "\n",
    "    if 'results' in final_results:\n",
    "        results_data = final_results['results']\n",
    "\n",
    "        for model_name, model_data in results_data.items():\n",
    "            before_metrics = model_data.get('avg_before_metrics', {})\n",
    "            after_metrics = model_data.get('avg_after_metrics', {})\n",
    "\n",
    "            print(f\"\\nüìä {model_name.upper()}:\")\n",
    "            print(f\"  üìù Questions: {model_data.get('num_questions_evaluated', 0)}\")\n",
    "            print(f\"  üìÑ Documents: {model_data.get('total_documents', 0):,}\")\n",
    "\n",
    "            if before_metrics and after_metrics:\n",
    "                # Performance metrics\n",
    "                f1_before = before_metrics.get('f1@5', 0)\n",
    "                f1_after = after_metrics.get('f1@5', 0)\n",
    "                improvement = ((f1_after - f1_before) / f1_before * 100) if f1_before > 0 else 0\n",
    "\n",
    "                print(f\"  üìà F1@5: {f1_before:.3f} ‚Üí {f1_after:.3f} ({improvement:+.1f}%)\")\n",
    "                print(f\"  üìà MRR: {before_metrics.get('mrr', 0):.3f} ‚Üí {after_metrics.get('mrr', 0):.3f}\")\n",
    "\n",
    "                # Score metrics\n",
    "                score_before = before_metrics.get('model_avg_score', 0)\n",
    "                score_after = after_metrics.get('model_avg_score', 0)\n",
    "\n",
    "                print(f\"  üìä Avg Score: {score_before:.3f} ‚Üí {score_after:.3f}\")\n",
    "\n",
    "                if 'model_avg_crossencoder_score' in after_metrics:\n",
    "                    ce_score = after_metrics.get('model_avg_crossencoder_score', 0)\n",
    "                    print(f\"  üß† CrossEncoder Score: {ce_score:.3f}\")\n",
    "                    print(f\"  üìä Documents Reranked: {after_metrics.get('model_total_documents_reranked', 0)}\")\n",
    "\n",
    "            # RAG metrics - FIX: Check for None values before formatting\n",
    "            rag_metrics = model_data.get('rag_metrics', {})\n",
    "            if rag_metrics.get('rag_available'):\n",
    "                print(f\"  ü§ñ RAG Metrics Available: ‚úÖ\")\n",
    "\n",
    "                # Display only available metrics (non-None)\n",
    "                if 'avg_faithfulness' in rag_metrics and rag_metrics['avg_faithfulness'] is not None:\n",
    "                    print(f\"    üìã Faithfulness: {rag_metrics['avg_faithfulness']:.3f}\")\n",
    "\n",
    "                if 'avg_answer_relevance' in rag_metrics and rag_metrics['avg_answer_relevance'] is not None:\n",
    "                    print(f\"    üéØ Answer Relevance: {rag_metrics['avg_answer_relevance']:.3f}\")\n",
    "\n",
    "                if 'avg_answer_correctness' in rag_metrics and rag_metrics['avg_answer_correctness'] is not None:\n",
    "                    print(f\"    ‚úÖ Answer Correctness: {rag_metrics['avg_answer_correctness']:.3f}\")\n",
    "\n",
    "                if 'avg_context_precision' in rag_metrics and rag_metrics['avg_context_precision'] is not None:\n",
    "                    print(f\"    üéØ Context Precision: {rag_metrics['avg_context_precision']:.3f}\")\n",
    "\n",
    "                if 'avg_context_recall' in rag_metrics and rag_metrics['avg_context_recall'] is not None:\n",
    "                    print(f\"    üì• Context Recall: {rag_metrics['avg_context_recall']:.3f}\")\n",
    "\n",
    "                if 'avg_semantic_similarity' in rag_metrics and rag_metrics['avg_semantic_similarity'] is not None:\n",
    "                    print(f\"    üîó Semantic Similarity: {rag_metrics['avg_semantic_similarity']:.3f}\")\n",
    "\n",
    "                if 'avg_bert_precision' in rag_metrics and rag_metrics['avg_bert_precision'] is not None:\n",
    "                    print(f\"    üìä BERT Precision: {rag_metrics['avg_bert_precision']:.3f}\")\n",
    "\n",
    "                if 'avg_bert_recall' in rag_metrics and rag_metrics['avg_bert_recall'] is not None:\n",
    "                    print(f\"    üìä BERT Recall: {rag_metrics['avg_bert_recall']:.3f}\")\n",
    "\n",
    "                if 'avg_bert_f1' in rag_metrics and rag_metrics['avg_bert_f1'] is not None:\n",
    "                    print(f\"    üéØ BERT F1: {rag_metrics['avg_bert_f1']:.3f}\")\n",
    "            else:\n",
    "                print(f\"  ü§ñ RAG Metrics: ‚ùå\")\n",
    "\n",
    "        # Overall comparison\n",
    "        print(f\"\\nüèÜ OVERALL:\")\n",
    "        best_f1 = (\"\", 0)\n",
    "        best_score = (\"\", 0)\n",
    "\n",
    "        for model_name, model_data in results_data.items():\n",
    "            after_metrics = model_data.get('avg_after_metrics', {})\n",
    "            f1 = after_metrics.get('f1@5', 0)\n",
    "            score = after_metrics.get('model_avg_score', 0)\n",
    "\n",
    "            if f1 > best_f1[1]:\n",
    "                best_f1 = (model_name, f1)\n",
    "            if score > best_score[1]:\n",
    "                best_score = (model_name, score)\n",
    "\n",
    "        print(f\"  ü•á Best F1@5: {best_f1[0]} ({best_f1[1]:.3f})\")\n",
    "        print(f\"  üìä Best Score: {best_score[0]} ({best_score[1]:.3f})\")\n",
    "\n",
    "        # Methodology info\n",
    "        data_verification = final_results.get('evaluation_info', {}).get('data_verification', {})\n",
    "        print(f\"\\nüî¨ VERIFICATION:\")\n",
    "        print(f\"  ‚úÖ Real data: {data_verification.get('is_real_data', False)}\")\n",
    "        print(f\"  üìä Framework: {data_verification.get('rag_framework', 'N/A')}\")\n",
    "        print(f\"  üîÑ Method: {data_verification.get('reranking_method', 'N/A')}\")\n",
    "\n",
    "print(\"\\nüéâ EVALUATION COMPLETE!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3O4-kIG1FiKJ"
   },
   "outputs": [],
   "source": [
    "# Play an audio beep. Any audio URL will do.\n",
    "from google.colab import output\n",
    "output.eval_js('new Audio(\"https://upload.wikimedia.org/wikipedia/commons/0/05/Beep-09.ogg\").play()')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tnuaWZQcFiKJ"
   },
   "source": [
    "## üßπ 8. Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iq4wLOXcFiKJ"
   },
   "outputs": [],
   "source": [
    "# Cleanup\n",
    "data_pipeline.cleanup()\n",
    "import gc\n",
    "gc.collect()\n",
    "\n",
    "print(\"üßπ Cleanup completed\")\n",
    "print(\"üéØ Results ready for Streamlit import\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "Tqg8lFZHFiKB",
    "21txtE0UzsQt",
    "NvEhZEzhzsQt",
    "kD1natt2-6Mc",
    "A0y7-dY8FiKG",
    "VuVs9gqoFiKH"
   ],
   "gpuType": "T4",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "0448cbfa3d494b8ab64cb44cb26dc3ed": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "0615ad03c4c045d0b1a1108cc197a528": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "0c0573ad24b34923a4504044d11d7487": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_4fad22e4eb31482aa0415c3eba123daa",
      "placeholder": "‚Äã",
      "style": "IPY_MODEL_6d68981eb5ae4651914009be25b0a981",
      "value": "tokenizer_config.json:‚Äá100%"
     }
    },
    "0ce0fc81b9084bcd8810c4679bf5e44c": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "0d2bdcecf8cc4d0ab221f771053534d8": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "13ddf33736e9480aa17d934d5b517cc7": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "163d2703ed134576a86d8da550843877": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "1a9be47fbcf847af95e03d12562b739c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_0d2bdcecf8cc4d0ab221f771053534d8",
      "placeholder": "‚Äã",
      "style": "IPY_MODEL_163d2703ed134576a86d8da550843877",
      "value": "‚Äá52.0/52.0‚Äá[00:00&lt;00:00,‚Äá7.01kB/s]"
     }
    },
    "20c8ad35267946ac977b177a630bee7e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_5922d7e8ecc745809196b179e214aee7",
      "placeholder": "‚Äã",
      "style": "IPY_MODEL_d5d68ab15499485ca297770f8ecc37b7",
      "value": "‚Äá557M/557M‚Äá[00:02&lt;00:00,‚Äá403MB/s]"
     }
    },
    "210d9ab54bde4c289a1667cae5beb337": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "28cde1633f2049fd984c87c71e0aba17": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_4ab84bc1663d46e5b8780e113f94efae",
       "IPY_MODEL_b6a838d380114440ba14a14ebee8d0c1",
       "IPY_MODEL_20c8ad35267946ac977b177a630bee7e"
      ],
      "layout": "IPY_MODEL_84bf31610aa04ca488b666fafb0f49e7"
     }
    },
    "2d24c0757de24dcfb9d33ad2175df8c4": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_716504937fdb456fa91f0953f183bb10",
      "max": 1,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_fbc184ccec1d4067819f61587c277c6a",
      "value": 1
     }
    },
    "376e965b23b8421b9e8c0332eb0d0266": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "3fad4ab210bb455393e8e0e789c3a25c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "40e70e638561424897d7798925d45854": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_4ccbf8e8994748b098d19d6cd79b0eca",
      "placeholder": "‚Äã",
      "style": "IPY_MODEL_3fad4ab210bb455393e8e0e789c3a25c",
      "value": "config.json:‚Äá100%"
     }
    },
    "4428d46e33e747fabbf37e75f0914359": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_0c0573ad24b34923a4504044d11d7487",
       "IPY_MODEL_a50f3aa8549e4917a3f6573c82736fd8",
       "IPY_MODEL_1a9be47fbcf847af95e03d12562b739c"
      ],
      "layout": "IPY_MODEL_0448cbfa3d494b8ab64cb44cb26dc3ed"
     }
    },
    "48b6ea57ecfb4c5684e031861f834a80": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": "20px"
     }
    },
    "4ab84bc1663d46e5b8780e113f94efae": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_92f182888768491aa04e9c90518c1dc6",
      "placeholder": "‚Äã",
      "style": "IPY_MODEL_7a85846da9894dd6ac49946d3a983529",
      "value": "pytorch_model.bin:‚Äá100%"
     }
    },
    "4ccbf8e8994748b098d19d6cd79b0eca": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "4f7c059eb9274af5a06c5ee80a333911": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "4fad22e4eb31482aa0415c3eba123daa": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "5922d7e8ecc745809196b179e214aee7": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "5bc21fd22f40415aa5f16a62bbf0bdf3": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_ff35c406ba304906a9ef13426dd0fbda",
       "IPY_MODEL_a80b953746134c39981d9103bfa3f44e",
       "IPY_MODEL_dabdb5f1a8e24c1a9064671a429aebca"
      ],
      "layout": "IPY_MODEL_0ce0fc81b9084bcd8810c4679bf5e44c"
     }
    },
    "6d68981eb5ae4651914009be25b0a981": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "716504937fdb456fa91f0953f183bb10": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": "20px"
     }
    },
    "79522b2d5a6f4ada85462a71160109f2": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_88d4f7ecd6024395a09683038aae27ab",
      "max": 728,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_e6f0be3861864cedaf903f3665ab74e5",
      "value": 728
     }
    },
    "7a85846da9894dd6ac49946d3a983529": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "7b2a453004e241cd923a8bd440b341fd": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "7cc3dac5f4a04e8bac41f21c661681ed": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "84bf31610aa04ca488b666fafb0f49e7": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "88d4f7ecd6024395a09683038aae27ab": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "92f182888768491aa04e9c90518c1dc6": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "9b4be78e6034446d87e1b575a7cb9658": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "a3aad76457c4437f8f901fe43f3a2761": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_d3c44059f1764dec9a248acc0e699473",
      "placeholder": "‚Äã",
      "style": "IPY_MODEL_b8433527c1a840fbb29a93bfaf817d89",
      "value": "‚Äá728/728‚Äá[00:00&lt;00:00,‚Äá99.7kB/s]"
     }
    },
    "a50f3aa8549e4917a3f6573c82736fd8": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_0615ad03c4c045d0b1a1108cc197a528",
      "max": 52,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_fa98e3e6979543a6a6e8687f257af676",
      "value": 52
     }
    },
    "a6dfdf7f574e4c7fb9d97c81dbf8f1ec": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "a80b953746134c39981d9103bfa3f44e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_48b6ea57ecfb4c5684e031861f834a80",
      "max": 1,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_fbf1948f6d634772878b1d953ccf73cf",
      "value": 1
     }
    },
    "b3612c5641f14bbfae9f87034c5d44c7": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_40e70e638561424897d7798925d45854",
       "IPY_MODEL_79522b2d5a6f4ada85462a71160109f2",
       "IPY_MODEL_a3aad76457c4437f8f901fe43f3a2761"
      ],
      "layout": "IPY_MODEL_9b4be78e6034446d87e1b575a7cb9658"
     }
    },
    "b6a838d380114440ba14a14ebee8d0c1": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_210d9ab54bde4c289a1667cae5beb337",
      "max": 556811547,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_e5cd69e17a11486ab4d25cb51ee76ee9",
      "value": 556811547
     }
    },
    "b8433527c1a840fbb29a93bfaf817d89": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "d06291ed03244bf28264d3570a77cf4b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "d3c44059f1764dec9a248acc0e699473": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "d55766d4b5324437b4f4c312fa991194": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "d5c0fbe825b94ffdb5d24fa2936961ff": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_fb648162c3334005ac289c774c2cd3f1",
       "IPY_MODEL_2d24c0757de24dcfb9d33ad2175df8c4",
       "IPY_MODEL_f955972eef4d45d9a01b76935498c74d"
      ],
      "layout": "IPY_MODEL_a6dfdf7f574e4c7fb9d97c81dbf8f1ec"
     }
    },
    "d5d68ab15499485ca297770f8ecc37b7": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "dabdb5f1a8e24c1a9064671a429aebca": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_fe72f1e2ba66417d89633644e83addeb",
      "placeholder": "‚Äã",
      "style": "IPY_MODEL_d55766d4b5324437b4f4c312fa991194",
      "value": "‚Äá456k/?‚Äá[00:00&lt;00:00,‚Äá34.9MB/s]"
     }
    },
    "e5cd69e17a11486ab4d25cb51ee76ee9": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "e6f0be3861864cedaf903f3665ab74e5": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "f955972eef4d45d9a01b76935498c74d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_13ddf33736e9480aa17d934d5b517cc7",
      "placeholder": "‚Äã",
      "style": "IPY_MODEL_d06291ed03244bf28264d3570a77cf4b",
      "value": "‚Äá899k/?‚Äá[00:00&lt;00:00,‚Äá51.3MB/s]"
     }
    },
    "fa98e3e6979543a6a6e8687f257af676": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "fb648162c3334005ac289c774c2cd3f1": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_7cc3dac5f4a04e8bac41f21c661681ed",
      "placeholder": "‚Äã",
      "style": "IPY_MODEL_4f7c059eb9274af5a06c5ee80a333911",
      "value": "vocab.json:‚Äá"
     }
    },
    "fbc184ccec1d4067819f61587c277c6a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "fbf1948f6d634772878b1d953ccf73cf": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "fe72f1e2ba66417d89633644e83addeb": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "ff35c406ba304906a9ef13426dd0fbda": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_376e965b23b8421b9e8c0332eb0d0266",
      "placeholder": "‚Äã",
      "style": "IPY_MODEL_7b2a453004e241cd923a8bd440b341fd",
      "value": "merges.txt:‚Äá"
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
