# ðŸ“ˆ MÃ©tricas Acumulativas - DocumentaciÃ³n

## DescripciÃ³n

La pÃ¡gina de **MÃ©tricas Acumulativas** permite evaluar mÃºltiples preguntas de forma automÃ¡tica y calcular mÃ©tricas promedio para obtener una visiÃ³n estadÃ­stica del rendimiento del sistema de recuperaciÃ³n de informaciÃ³n.

## CaracterÃ­sticas Principales

### ðŸŽ¯ **Filtrado Inteligente**
- **Solo preguntas con links**: Se evalÃºan Ãºnicamente preguntas que contienen enlaces de Microsoft Learn en la respuesta aceptada
- **ExtracciÃ³n automÃ¡tica**: Los links se extraen automÃ¡ticamente usando regex pattern: `https://learn\.microsoft\.com[\w/\-\?=&%\.]+`
- **ValidaciÃ³n de calidad**: Asegura que cada pregunta tenga ground truth vÃ¡lido

### ðŸ“Š **MÃ©tricas Calculadas**
- **MRR**: Mean Reciprocal Rank global
- **Recall@k**: Cobertura para k=1,3,5,10
- **Precision@k**: PrecisiÃ³n para k=1,3,5,10
- **F1@k**: Balance entre precisiÃ³n y recall para k=1,3,5,10
- **Accuracy@k**: Exactitud de clasificaciÃ³n para k=1,3,5,10

### ðŸ”„ **Antes y DespuÃ©s del Reranking**
- **MÃ©tricas base**: Resultados usando solo similarity search
- **MÃ©tricas post-LLM**: Resultados despuÃ©s del reranking con GPT-4
- **ComparaciÃ³n visual**: GrÃ¡ficos que muestran la mejora/deterioro
- **CÃ¡lculo de delta**: Diferencia entre antes y despuÃ©s

## ConfiguraciÃ³n

### ParÃ¡metros Principales

| ParÃ¡metro | Valor Por Defecto | DescripciÃ³n |
|-----------|-------------------|-------------|
| **NÃºmero de preguntas** | 5 | Cantidad de preguntas a evaluar (rango: 5-3035) |
| **Modelo de Embedding** | multi-qa-mpnet-base-dot-v1 | Modelo para generar embeddings |
| **Top-K documentos** | 10 | NÃºmero de documentos a recuperar |
| **LLM Reranking** | Habilitado | Usar GPT-4 para reordenar documentos |

### Fuente de Datos
- **Archivos disponibles**: 
  - `data/val_set.json` (1,035 preguntas de validaciÃ³n)
  - `data/train_set.json` (2,000 preguntas de entrenamiento)
  - **Total combinado**: 3,035 preguntas
- **Formato**: JSON con structure `{title, question_content, accepted_answer, tags, url}`
- **Filtrado**: Solo preguntas con enlaces de Microsoft Learn (100% del dataset)
- **Opciones de dataset**: 
  - Dataset Completo (train + val): 3,035 preguntas
  - Solo ValidaciÃ³n: 1,035 preguntas  
  - Solo Entrenamiento: 2,000 preguntas

## Uso Paso a Paso

### 1. **ConfiguraciÃ³n**
```python
# En la interfaz de Streamlit:
- Seleccionar nÃºmero de preguntas (1-50)
- Elegir modelo de embedding
- Configurar Top-K documentos
- Habilitar/deshabilitar LLM reranking
```

### 2. **EjecuciÃ³n**
```python
# Al hacer clic en "ðŸš€ Ejecutar EvaluaciÃ³n":
1. Carga preguntas desde val_set.json
2. Filtra preguntas con links de MS Learn
3. Selecciona N preguntas aleatoriamente
4. EvalÃºa cada pregunta individualmente
5. Calcula mÃ©tricas promedio
```

### 3. **Resultados**
```python
# VisualizaciÃ³n de resultados:
- MÃ©tricas promedio en columnas
- GrÃ¡fico comparativo (antes vs despuÃ©s)
- Tabla detallada por pregunta
- EstadÃ­sticas de evaluaciÃ³n
```

## Algoritmo de CÃ¡lculo

### MÃ©tricas Promedio
```python
def calculate_average_metrics(all_metrics):
    metric_sums = {}
    metric_counts = {}
    
    for metrics in all_metrics:
        for key, value in metrics.items():
            if isinstance(value, (int, float)) and not np.isnan(value):
                metric_sums[key] = metric_sums.get(key, 0) + value
                metric_counts[key] = metric_counts.get(key, 0) + 1
    
    return {key: metric_sums[key] / metric_counts[key] 
            for key in metric_sums if metric_counts[key] > 0}
```

### Ejemplo de CÃ¡lculo
```python
# Si F1@5 en 3 preguntas es: [0.3, 0.4, 0.5]
# Promedio = (0.3 + 0.4 + 0.5) / 3 = 0.4

# Si una pregunta falla y no tiene F1@5:
# F1@5 en 3 preguntas: [0.3, NaN, 0.5]
# Promedio = (0.3 + 0.5) / 2 = 0.4
```

## InterpretaciÃ³n de Resultados

### ðŸ“Š **MÃ©tricas Principales**
- **Valores â‰¥ 0.7**: ðŸŸ¢ *Muy buenos*
- **Valores 0.4 - 0.7**: ðŸŸ¡ *Buenos*
- **Valores < 0.4**: ðŸ”´ *Malos*
- **MRR** evalÃºa la posiciÃ³n del primer relevante
- **Recall/Precision/F1/Accuracy@k** se reportan para k=1,3,5,10

### ðŸ”„ **Impacto del Reranking**
- **Delta positivo**: El reranking LLM mejora la mÃ©trica
- **Delta negativo**: El reranking LLM empeora la mÃ©trica
- **Delta ~0**: El reranking no tiene impacto significativo

## ExportaciÃ³n de Datos

### ðŸ“‹ **CSV Detallado**
```csv
question_num,ground_truth_links,docs_retrieved,before_precision_5,after_precision_5,...
1,3,10,0.400,0.600,...
2,2,10,0.200,0.400,...
```

### ðŸ“Š **CSV Promedio**
```csv
Metric,Before_Reranking,After_Reranking
Precision@5,0.350,0.450
Recall@5,0.280,0.380
F1@5,0.310,0.410
```

## Casos de Uso

### ðŸ”¬ **EvaluaciÃ³n de Modelos**
```python
# Comparar diferentes modelos de embedding:
1. Ejecutar con multi-qa-mpnet-base-dot-v1
2. Ejecutar con all-MiniLM-L6-v2
3. Comparar mÃ©tricas promedio
4. TambiÃ©n puedes habilitar **Evaluar los 3 modelos** para ejecutarlos en una sola corrida
```

### ðŸ“ˆ **AnÃ¡lisis de Rendimiento**
```python
# Evaluar impacto del reranking:
1. Ejecutar con reranking habilitado
2. Ejecutar con reranking deshabilitado
3. Analizar diferencias en mÃ©tricas
```

### ðŸŽ¯ **OptimizaciÃ³n de ParÃ¡metros**
```python
# Encontrar mejor configuraciÃ³n:
1. Probar diferentes valores de Top-K
2. Evaluar con diferentes nÃºmeros de preguntas
3. Seleccionar configuraciÃ³n Ã³ptima
```

## Limitaciones

### âš ï¸ **Consideraciones**
- **Tiempo de ejecuciÃ³n**: ~2-5 segundos por pregunta con reranking
- **Dependencia de GPT-4**: Reranking requiere acceso a OpenAI API
- **SelecciÃ³n aleatoria**: Resultados pueden variar entre ejecuciones
- **Sesgo del dataset**: Solo preguntas con links de MS Learn

### ðŸ”§ **Recomendaciones**
- **Usar 5-10 preguntas** para pruebas rÃ¡pidas
- **Usar 20-50 preguntas** para evaluaciones mÃ¡s robustas
- **Ejecutar mÃºltiples veces** para obtener intervalos de confianza
- **Comparar mÃ©tricas** antes y despuÃ©s del reranking

## IntegraciÃ³n

### ðŸ“ **Archivos Principales**
- `cumulative_metrics_page.py`: LÃ³gica principal de la pÃ¡gina
- `utils/qa_pipeline_with_metrics.py`: Pipeline con cÃ¡lculo de mÃ©tricas
- `data/val_set.json`: Dataset de preguntas de validaciÃ³n

### ðŸ”— **Dependencias**
- `streamlit`: Interfaz de usuario
- `pandas`: ManipulaciÃ³n de datos
- `numpy`: CÃ¡lculos numÃ©ricos
- `plotly`: VisualizaciÃ³n de datos
- `json`: Carga de dataset
- `re`: ExtracciÃ³n de links

---

## PrÃ³ximas Mejoras

1. **Intervalos de confianza**: Calcular IC para mÃ©tricas promedio
2. **AnÃ¡lisis estadÃ­stico**: Pruebas de significancia entre configuraciones
3. **MÃ¡s mÃ©tricas**: MAP, NDCG@10, Hit Rate
4. **Cache de resultados**: Evitar re-evaluaciÃ³n de preguntas
5. **ExportaciÃ³n avanzada**: Informes PDF con visualizaciones