#  M茅tricas Acumulativas - Documentaci贸n

## Descripci贸n

La p谩gina de **M茅tricas Acumulativas** permite evaluar m煤ltiples preguntas de forma autom谩tica y calcular m茅tricas promedio para obtener una visi贸n estad铆stica del rendimiento del sistema de recuperaci贸n de informaci贸n.

## Caracter铆sticas Principales

###  **Filtrado Inteligente**
- **Solo preguntas con links**: Se eval煤an 煤nicamente preguntas que contienen enlaces de Microsoft Learn en la respuesta aceptada
- **Extracci贸n autom谩tica**: Los links se extraen autom谩ticamente usando regex pattern: `https://learn\.microsoft\.com[\w/\-\?=&%\.]+`
- **Validaci贸n de calidad**: Asegura que cada pregunta tenga ground truth v谩lido

###  **M茅tricas Calculadas**
- **Precision@5**: Precisi贸n en los primeros 5 documentos
- **Recall@5**: Cobertura en los primeros 5 documentos  
- **F1@5**: Balance entre precisi贸n y cobertura
- **MRR@5**: Mean Reciprocal Rank en top-5
- **nDCG@5**: Normalized Discounted Cumulative Gain

###  **Antes y Despu茅s del Reranking**
- **M茅tricas base**: Resultados usando solo similarity search
- **M茅tricas post-LLM**: Resultados despu茅s del reranking con GPT-4
- **Comparaci贸n visual**: Gr谩ficos que muestran la mejora/deterioro
- **C谩lculo de delta**: Diferencia entre antes y despu茅s

## Configuraci贸n

### Par谩metros Principales

| Par谩metro | Valor Por Defecto | Descripci贸n |
|-----------|-------------------|-------------|
| **N煤mero de preguntas** | 5 | Cantidad de preguntas a evaluar (rango: 1-50) |
| **Modelo de Embedding** | multi-qa-mpnet-base-dot-v1 | Modelo para generar embeddings |
| **Top-K documentos** | 10 | N煤mero de documentos a recuperar |
| **LLM Reranking** | Habilitado | Usar GPT-4 para reordenar documentos |

### Fuente de Datos
- **Archivo**: `data/val_set.json` (1,035 preguntas)
- **Formato**: JSON con structure `{title, question_content, accepted_answer, tags, url}`
- **Filtrado**: Solo preguntas con enlaces de Microsoft Learn

## Uso Paso a Paso

### 1. **Configuraci贸n**
```python
# En la interfaz de Streamlit:
- Seleccionar n煤mero de preguntas (1-50)
- Elegir modelo de embedding
- Configurar Top-K documentos
- Habilitar/deshabilitar LLM reranking
```

### 2. **Ejecuci贸n**
```python
# Al hacer clic en " Ejecutar Evaluaci贸n":
1. Carga preguntas desde val_set.json
2. Filtra preguntas con links de MS Learn
3. Selecciona N preguntas aleatoriamente
4. Eval煤a cada pregunta individualmente
5. Calcula m茅tricas promedio
```

### 3. **Resultados**
```python
# Visualizaci贸n de resultados:
- M茅tricas promedio en columnas
- Gr谩fico comparativo (antes vs despu茅s)
- Tabla detallada por pregunta
- Estad铆sticas de evaluaci贸n
```

## Algoritmo de C谩lculo

### M茅tricas Promedio
```python
def calculate_average_metrics(all_metrics):
    metric_sums = {}
    metric_counts = {}
    
    for metrics in all_metrics:
        for key, value in metrics.items():
            if isinstance(value, (int, float)) and not np.isnan(value):
                metric_sums[key] = metric_sums.get(key, 0) + value
                metric_counts[key] = metric_counts.get(key, 0) + 1
    
    return {key: metric_sums[key] / metric_counts[key] 
            for key in metric_sums if metric_counts[key] > 0}
```

### Ejemplo de C谩lculo
```python
# Si F1@5 en 3 preguntas es: [0.3, 0.4, 0.5]
# Promedio = (0.3 + 0.4 + 0.5) / 3 = 0.4

# Si una pregunta falla y no tiene F1@5:
# F1@5 en 3 preguntas: [0.3, NaN, 0.5]
# Promedio = (0.3 + 0.5) / 2 = 0.4
```

## Interpretaci贸n de Resultados

###  **M茅tricas Principales**
- **Precision@5 > 0.5**: Buena precisi贸n en top-5
- **Recall@5 > 0.3**: Buena cobertura del ground truth
- **F1@5 > 0.4**: Buen balance precision/recall
- **MRR@5 > 0.6**: Primer resultado relevante aparece temprano
- **nDCG@5 > 0.5**: Buen ranking general

###  **Impacto del Reranking**
- **Delta positivo**: El reranking LLM mejora la m茅trica
- **Delta negativo**: El reranking LLM empeora la m茅trica
- **Delta ~0**: El reranking no tiene impacto significativo

## Exportaci贸n de Datos

###  **CSV Detallado**
```csv
question_num,ground_truth_links,docs_retrieved,before_precision_5,after_precision_5,...
1,3,10,0.400,0.600,...
2,2,10,0.200,0.400,...
```

###  **CSV Promedio**
```csv
Metric,Before_Reranking,After_Reranking
Precision@5,0.350,0.450
Recall@5,0.280,0.380
F1@5,0.310,0.410
```

## Casos de Uso

###  **Evaluaci贸n de Modelos**
```python
# Comparar diferentes modelos de embedding:
1. Ejecutar con multi-qa-mpnet-base-dot-v1
2. Ejecutar con all-MiniLM-L6-v2
3. Comparar m茅tricas promedio
```

###  **An谩lisis de Rendimiento**
```python
# Evaluar impacto del reranking:
1. Ejecutar con reranking habilitado
2. Ejecutar con reranking deshabilitado
3. Analizar diferencias en m茅tricas
```

###  **Optimizaci贸n de Par谩metros**
```python
# Encontrar mejor configuraci贸n:
1. Probar diferentes valores de Top-K
2. Evaluar con diferentes n煤meros de preguntas
3. Seleccionar configuraci贸n 贸ptima
```

## Limitaciones

### 锔 **Consideraciones**
- **Tiempo de ejecuci贸n**: ~2-5 segundos por pregunta con reranking
- **Dependencia de GPT-4**: Reranking requiere acceso a OpenAI API
- **Selecci贸n aleatoria**: Resultados pueden variar entre ejecuciones
- **Sesgo del dataset**: Solo preguntas con links de MS Learn

###  **Recomendaciones**
- **Usar 5-10 preguntas** para pruebas r谩pidas
- **Usar 20-50 preguntas** para evaluaciones m谩s robustas
- **Ejecutar m煤ltiples veces** para obtener intervalos de confianza
- **Comparar m茅tricas** antes y despu茅s del reranking

## Integraci贸n

###  **Archivos Principales**
- `cumulative_metrics_page.py`: L贸gica principal de la p谩gina
- `utils/qa_pipeline_with_metrics.py`: Pipeline con c谩lculo de m茅tricas
- `data/val_set.json`: Dataset de preguntas de validaci贸n

###  **Dependencias**
- `streamlit`: Interfaz de usuario
- `pandas`: Manipulaci贸n de datos
- `numpy`: C谩lculos num茅ricos
- `plotly`: Visualizaci贸n de datos
- `json`: Carga de dataset
- `re`: Extracci贸n de links

---

## Pr贸ximas Mejoras

1. **Intervalos de confianza**: Calcular IC para m茅tricas promedio
2. **An谩lisis estad铆stico**: Pruebas de significancia entre configuraciones
3. **M谩s m茅tricas**: MAP, NDCG@10, Hit Rate
4. **Cache de resultados**: Evitar re-evaluaci贸n de preguntas
5. **Exportaci贸n avanzada**: Informes PDF con visualizaciones