{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "header"
   },
   "source": [
    "# üöÄ Universal Colab Evaluator\n",
    "## Evaluaci√≥n Acumulativa de Embeddings con GPU\n",
    "\n",
    "Este notebook lee autom√°ticamente la configuraci√≥n desde Google Drive y ejecuta la evaluaci√≥n con aceleraci√≥n GPU.\n",
    "\n",
    "### üìã Instrucciones:\n",
    "1. **Activar GPU**: Runtime ‚Üí Change runtime type ‚Üí GPU ‚Üí T4\n",
    "2. **Ejecutar todo**: Runtime ‚Üí Run all (Ctrl+F9)\n",
    "3. **Monitorear progreso**: Ver barras de progreso\n",
    "4. **Resultados autom√°ticos**: Se guardan en Google Drive\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mount_drive"
   },
   "outputs": [],
   "source": [
    "# Montar Google Drive\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "print(\"‚úÖ Google Drive montado exitosamente\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "install_deps"
   },
   "outputs": [],
   "source": [
    "# Instalar dependencias\n",
    "!pip install -q sentence-transformers openai chromadb numpy pandas scikit-learn matplotlib seaborn tqdm\n",
    "print(\"‚úÖ Dependencias instaladas\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "imports"
   },
   "outputs": [],
   "source": [
    "# Importar librer√≠as\n",
    "import json, os, time, numpy as np, pandas as pd\n",
    "from datetime import datetime\n",
    "from typing import List, Dict, Any\n",
    "from tqdm import tqdm\n",
    "import warnings; warnings.filterwarnings('ignore')\n",
    "\n",
    "# Verificar GPU\n",
    "import torch\n",
    "gpu_available = torch.cuda.is_available()\n",
    "if gpu_available:\n",
    "    print(f\"üöÄ GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"üíæ Memoria: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è GPU no disponible, usando CPU\")\n",
    "print(\"‚úÖ Setup completado\")"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "load_config"
   },
   "outputs": [],
   "source": "# Configurar rutas\nDRIVE_BASE = '/content/drive/MyDrive/TesisMagister/acumulative'\nRESULTS_DIR = f'{DRIVE_BASE}/results'\n\nprint(f\"üìÅ Carpeta base: {DRIVE_BASE}\")\nprint(f\"üìä Directorio de resultados: {RESULTS_DIR}\")\n\n# Crear directorio de resultados si no existe\nos.makedirs(RESULTS_DIR, exist_ok=True)\nprint(f\"‚úÖ Carpeta results verificada/creada\")\n\n# Buscar configuraci√≥n m√°s reciente con manejo robusto de errores\ntry:\n    print(\"üîç Buscando archivos de configuraci√≥n...\")\n    \n    # Verificar que la carpeta existe\n    if not os.path.exists(DRIVE_BASE):\n        print(f\"‚ùå Error: Carpeta no existe: {DRIVE_BASE}\")\n        print(\"üí° Aseg√∫rate de que Google Drive est√© montado correctamente\")\n        raise FileNotFoundError(f\"Drive folder not found: {DRIVE_BASE}\")\n    \n    # Listar todos los archivos para debug\n    all_files = os.listdir(DRIVE_BASE)\n    print(f\"üìÇ Archivos encontrados en Drive ({len(all_files)}):\")\n    for f in all_files:\n        print(f\"   üìÑ {f}\")\n    \n    # Buscar archivos de configuraci√≥n con timestamp\n    config_files = [f for f in all_files if f.startswith('evaluation_config_') and f.endswith('.json')]\n    \n    if config_files:\n        # Ordenar por nombre (que incluye timestamp) y usar el m√°s reciente\n        config_files.sort(reverse=True)\n        config_filename = config_files[0]\n        config_file = f'{DRIVE_BASE}/{config_filename}'\n        print(f\"‚úÖ Usando configuraci√≥n m√°s reciente: {config_filename}\")\n        \n    elif 'evaluation_config.json' in all_files:\n        # Fallback al archivo sin timestamp\n        config_file = f'{DRIVE_BASE}/evaluation_config.json'\n        print(\"üìã Usando configuraci√≥n por defecto: evaluation_config.json\")\n        \n    else:\n        print(\"‚ùå No se encontraron archivos de configuraci√≥n\")\n        print(\"üîç Archivos de configuraci√≥n esperados:\")\n        print(\"   - evaluation_config_YYYYMMDD_HHMMSS.json (con timestamp)\")\n        print(\"   - evaluation_config.json (por defecto)\")\n        print(\"\\nüí° Soluci√≥n:\")\n        print(\"   1. Ve a Streamlit\")\n        print(\"   2. Presiona 'üöÄ Crear Configuraci√≥n y Enviar a Google Drive'\")\n        print(\"   3. Espera la confirmaci√≥n de subida exitosa\")\n        print(\"   4. Ejecuta este notebook nuevamente\")\n        raise FileNotFoundError(\"No configuration files found in Google Drive\")\n    \n    # Verificar que el archivo existe\n    if not os.path.exists(config_file):\n        print(f\"‚ùå Error: Archivo de configuraci√≥n no existe: {config_file}\")\n        raise FileNotFoundError(f\"Config file not found: {config_file}\")\n    \n    print(f\"üìÑ Archivo de configuraci√≥n: {config_file}\")\n    \n    # Leer y validar configuraci√≥n\n    print(\"üìñ Leyendo configuraci√≥n...\")\n    with open(config_file, 'r', encoding='utf-8') as f:\n        config = json.load(f)\n    \n    # Validar campos requeridos\n    required_fields = ['num_questions', 'selected_models', 'evaluation_type']\n    missing_fields = [field for field in required_fields if field not in config]\n    \n    if missing_fields:\n        print(f\"‚ùå Error: Campos faltantes en configuraci√≥n: {missing_fields}\")\n        raise ValueError(f\"Missing required config fields: {missing_fields}\")\n    \n    print(f\"‚úÖ Configuraci√≥n cargada exitosamente:\")\n    print(f\"   üî¢ Preguntas: {config['num_questions']}\")\n    print(f\"   ü§ñ Modelos: {len(config['selected_models'])} - {config['selected_models']}\")\n    print(f\"   üìä Tipo: {config['evaluation_type']}\")\n    print(f\"   üß† Modelo generativo: {config.get('generative_model_name', 'N/A')}\")\n    \n    # Verificar si hay datos de preguntas\n    if config.get('questions_data'):\n        print(f\"   üìù Preguntas reales incluidas: {len(config['questions_data'])}\")\n    else:\n        print(f\"   ‚ö†Ô∏è  Sin datos de preguntas - se generar√°n simuladas\")\n\nexcept Exception as e:\n    print(f\"üí• Error cr√≠tico cargando configuraci√≥n: {e}\")\n    print(\"\\nüîß PASOS PARA SOLUCIONAR:\")\n    print(\"1. Verifica que Google Drive est√© montado:\")\n    print(\"   - Ejecuta la celda de 'Montar Google Drive' arriba\")\n    print(\"   - Autoriza el acceso cuando se solicite\")\n    print(\"2. Verifica que el archivo de configuraci√≥n existe:\")\n    print(\"   - Ve a Streamlit ‚Üí M√©tricas Acumulativas\") \n    print(\"   - Marca 'Procesamiento en Google Colab'\")\n    print(\"   - Click 'üöÄ Crear Configuraci√≥n y Enviar a Google Drive'\")\n    print(\"3. Si el problema persiste:\")\n    print(\"   - Verifica la carpeta: /content/drive/MyDrive/TesisMagister/acumulative/\")\n    print(\"   - Busca archivos que empiecen con 'evaluation_config'\")\n    raise"
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "prepare_data"
   },
   "outputs": [],
   "source": "# Preparar datos de preguntas\nprint(\"üìä PREPARACI√ìN DE DATOS\")\nprint(\"=\" * 50)\n\nif config.get('questions_data'):\n    questions_data = config['questions_data']\n    print(f\"‚úÖ USANDO DATOS REALES:\")\n    print(f\"   üìù {len(questions_data)} preguntas reales desde ChromaDB\")\n    print(f\"   üîó Todas con enlaces de Microsoft Learn\")\n    print(f\"   üìä Obtenidas desde Streamlit\")\n    \n    # Mostrar estad√≠sticas de los datos reales\n    if questions_data:\n        sample = questions_data[0]\n        print(f\"\\nüìã Estructura de datos:\")\n        print(f\"   üìÑ Campos disponibles: {list(sample.keys())}\")\n        print(f\"   üìù Ejemplo de t√≠tulo: '{sample.get('title', 'N/A')[:100]}{'...' if len(sample.get('title', '')) > 100 else ''}'\")\n        \n        # Verificar enlaces MS Learn\n        ms_learn_count = sum(1 for q in questions_data if q.get('has_ms_learn_link') or 'learn.microsoft.com' in str(q.get('accepted_answer', '')))\n        print(f\"   üîó Preguntas con MS Learn: {ms_learn_count}/{len(questions_data)} ({ms_learn_count/len(questions_data)*100:.1f}%)\")\n\nelse:\n    print(f\"‚ö†Ô∏è  USANDO DATOS SIMULADOS:\")\n    print(f\"   üìù No se encontraron datos reales en la configuraci√≥n\")\n    print(f\"   ü§ñ Generando {config['num_questions']} preguntas simuladas\")\n    print(f\"   üí° Para usar datos reales, aseg√∫rate de crear la configuraci√≥n desde Streamlit\")\n    \n    questions_data = []\n    for i in range(config['num_questions']):\n        questions_data.append({\n            'id': f'sim_q_{i+1}',\n            'title': f'Microsoft Technology Question {i+1}',\n            'body': f'How to implement feature {i+1} in Microsoft framework?',\n            'accepted_answer': f'You can implement this using Microsoft Learn documentation approach {i+1}. Visit https://learn.microsoft.com/example-{i+1}',\n            'has_ms_learn_link': True,\n            'question': f'How to implement feature {i+1}?',\n            'tags': ['microsoft', 'technology', f'feature-{i+1}'],\n            'ms_links': [f'https://learn.microsoft.com/example-{i+1}']\n        })\n    print(f\"‚úÖ Generadas {len(questions_data)} preguntas simuladas con estructura completa\")\n\nprint(f\"\\nüìä RESUMEN FINAL:\")\nprint(f\"   üìù Total de preguntas: {len(questions_data)}\")\nprint(f\"   üîç Tipo de datos: {'REALES desde ChromaDB' if config.get('questions_data') else 'SIMULADOS'}\")\nprint(f\"   üöÄ Listo para evaluaci√≥n\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "init_models"
   },
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# Mapeo de modelos\n",
    "MODEL_MAPPING = {\n",
    "    'multi-qa-mpnet-base-dot-v1': 'multi-qa-mpnet-base-dot-v1',\n",
    "    'all-MiniLM-L6-v2': 'all-MiniLM-L6-v2',\n",
    "    'ada': 'all-MiniLM-L6-v2',  # Substituto local\n",
    "    'e5-large-v2': 'intfloat/e5-large-v2'\n",
    "}\n",
    "\n",
    "# Cargar modelos\n",
    "models = {}\n",
    "device = 'cuda' if gpu_available else 'cpu'\n",
    "print(f\"üîÑ Cargando modelos en {device}...\")\n",
    "\n",
    "for model_name in config['selected_models']:\n",
    "    try:\n",
    "        actual_model = MODEL_MAPPING.get(model_name, model_name)\n",
    "        models[model_name] = SentenceTransformer(actual_model, device=device)\n",
    "        print(f\"   ‚úÖ {model_name}\")\n",
    "    except Exception as e:\n",
    "        print(f\"   ‚ö†Ô∏è Error {model_name}: {e}\")\n",
    "        models[model_name] = SentenceTransformer('all-MiniLM-L6-v2', device=device)\n",
    "        print(f\"   ‚úÖ {model_name} (substituto)\")\n",
    "\n",
    "print(f\"‚úÖ {len(models)} modelos listos\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "evaluation_functions"
   },
   "outputs": [],
   "source": [
    "def calculate_metrics(retrieved_docs, relevant_docs, k=10):\n",
    "    \"\"\"Calcula m√©tricas de recuperaci√≥n\"\"\"\n",
    "    if not retrieved_docs or not relevant_docs:\n",
    "        return {'precision': 0.0, 'recall': 0.0, 'f1': 0.0, 'map': 0.0, 'mrr': 0.0, 'ndcg': 0.0}\n",
    "    \n",
    "    retrieved_k = retrieved_docs[:k]\n",
    "    relevant_retrieved = len([doc for doc in retrieved_k if doc in relevant_docs])\n",
    "    \n",
    "    # M√©tricas b√°sicas\n",
    "    precision = relevant_retrieved / len(retrieved_k) if retrieved_k else 0.0\n",
    "    recall = relevant_retrieved / len(relevant_docs) if relevant_docs else 0.0\n",
    "    f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0.0\n",
    "    \n",
    "    # MAP\n",
    "    ap = 0.0\n",
    "    relevant_count = 0\n",
    "    for i, doc in enumerate(retrieved_k):\n",
    "        if doc in relevant_docs:\n",
    "            relevant_count += 1\n",
    "            ap += relevant_count / (i + 1)\n",
    "    map_score = ap / len(relevant_docs) if relevant_docs else 0.0\n",
    "    \n",
    "    # MRR\n",
    "    mrr = 0.0\n",
    "    for i, doc in enumerate(retrieved_k):\n",
    "        if doc in relevant_docs:\n",
    "            mrr = 1.0 / (i + 1)\n",
    "            break\n",
    "    \n",
    "    # NDCG\n",
    "    dcg = sum([1.0 / np.log2(i + 2) for i, doc in enumerate(retrieved_k) if doc in relevant_docs])\n",
    "    idcg = sum([1.0 / np.log2(i + 2) for i in range(min(len(relevant_docs), k))])\n",
    "    ndcg = dcg / idcg if idcg > 0 else 0.0\n",
    "    \n",
    "    return {'precision': precision, 'recall': recall, 'f1': f1, 'map': map_score, 'mrr': mrr, 'ndcg': ndcg}\n",
    "\n",
    "def simulate_retrieval(question_text, model, num_docs=50):\n",
    "    \"\"\"Simula recuperaci√≥n de documentos\"\"\"\n",
    "    docs = []\n",
    "    for i in range(num_docs):\n",
    "        relevance = np.random.random()\n",
    "        docs.append((f\"doc_{i}\", relevance))\n",
    "    docs.sort(key=lambda x: x[1], reverse=True)\n",
    "    return [doc_id for doc_id, score in docs]\n",
    "\n",
    "print(\"‚úÖ Funciones de evaluaci√≥n definidas\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "run_evaluation"
   },
   "outputs": [],
   "source": [
    "# Actualizar estado\n",
    "status_file = f'{DRIVE_BASE}/evaluation_status.json'\n",
    "status_data = {\n",
    "    'status': 'running',\n",
    "    'timestamp': datetime.now().isoformat(),\n",
    "    'models_to_evaluate': len(config['selected_models']),\n",
    "    'questions_total': len(questions_data),\n",
    "    'gpu_used': gpu_available\n",
    "}\n",
    "with open(status_file, 'w') as f:\n",
    "    json.dump(status_data, f, indent=2)\n",
    "\n",
    "print(\"üöÄ Iniciando evaluaci√≥n...\")\n",
    "print(f\"ü§ñ Modelos: {len(models)} | ‚ùì Preguntas: {len(questions_data)} | üöÄ GPU: {'‚úÖ' if gpu_available else '‚ùå'}\")\n",
    "\n",
    "# Evaluar cada modelo\n",
    "start_time = time.time()\n",
    "all_results = {}\n",
    "top_k = config.get('top_k', 10)\n",
    "batch_size = config.get('batch_size', 50)\n",
    "\n",
    "for i, (model_name, model) in enumerate(models.items()):\n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(f\"üìä EVALUANDO {i+1}/{len(models)}: {model_name}\")\n",
    "    print(f\"{'='*50}\")\n",
    "    \n",
    "    model_start = time.time()\n",
    "    all_metrics = []\n",
    "    \n",
    "    # Procesar en batches\n",
    "    for batch_start in tqdm(range(0, len(questions_data), batch_size), desc=f\"Evaluando {model_name}\"):\n",
    "        batch = questions_data[batch_start:batch_start+batch_size]\n",
    "        \n",
    "        for question in batch:\n",
    "            try:\n",
    "                # Simular recuperaci√≥n\n",
    "                query = question.get('title', '') + ' ' + question.get('body', '')\n",
    "                retrieved_docs = simulate_retrieval(query, model, 100)\n",
    "                \n",
    "                # Documentos relevantes simulados\n",
    "                if question.get('has_ms_learn_link', False):\n",
    "                    relevant_docs = [f\"doc_{j}\" for j in range(min(5, len(retrieved_docs)))]\n",
    "                else:\n",
    "                    relevant_docs = []\n",
    "                \n",
    "                # Calcular m√©tricas\n",
    "                metrics = calculate_metrics(retrieved_docs, relevant_docs, k=top_k)\n",
    "                all_metrics.append(metrics)\n",
    "                \n",
    "            except Exception as e:\n",
    "                all_metrics.append({'precision': 0.0, 'recall': 0.0, 'f1': 0.0, 'map': 0.0, 'mrr': 0.0, 'ndcg': 0.0})\n",
    "    \n",
    "    # Calcular promedios\n",
    "    if all_metrics:\n",
    "        avg_metrics = {}\n",
    "        std_metrics = {}\n",
    "        \n",
    "        for metric in ['precision', 'recall', 'f1', 'map', 'mrr', 'ndcg']:\n",
    "            values = [m[metric] for m in all_metrics]\n",
    "            avg_metrics[f'avg_{metric}'] = np.mean(values)\n",
    "            std_metrics[f'std_{metric}'] = np.std(values)\n",
    "        \n",
    "        all_results[model_name] = {\n",
    "            'model_name': model_name,\n",
    "            'avg_metrics': {**avg_metrics, **std_metrics},\n",
    "            'individual_metrics': all_metrics\n",
    "        }\n",
    "        \n",
    "        model_time = time.time() - model_start\n",
    "        print(f\"‚úÖ {model_name} completado en {model_time:.2f}s\")\n",
    "        print(f\"   üìä P: {avg_metrics['avg_precision']:.3f} | R: {avg_metrics['avg_recall']:.3f} | F1: {avg_metrics['avg_f1']:.3f}\")\n",
    "\n",
    "total_time = time.time() - start_time\n",
    "print(f\"\\nüéâ EVALUACI√ìN COMPLETADA en {total_time:.1f}s ({total_time/60:.1f} min)\")\n",
    "print(f\"‚úÖ Modelos evaluados: {len(all_results)} | ‚ùì Preguntas: {len(questions_data)}\")"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "save_results"
   },
   "outputs": [],
   "source": "# Guardar resultados con verificaci√≥n robusta\ntimestamp = int(time.time())\nresults_filename = f'cumulative_results_{timestamp}.json'\nsummary_filename = f'results_summary_{timestamp}.csv'\n\nprint(f\"üíæ GUARDANDO RESULTADOS\")\nprint(f\"=\" * 50)\n\n# Datos completos\nresults_data = {\n    'config': config,\n    'results': all_results,\n    'evaluation_info': {\n        'total_time_seconds': total_time,\n        'models_evaluated': len(all_results),\n        'questions_processed': len(questions_data),\n        'gpu_used': gpu_available,\n        'timestamp': datetime.now().isoformat()\n    }\n}\n\n# 1. Guardar JSON localmente primero\nlocal_results_path = f'{RESULTS_DIR}/{results_filename}'\ntry:\n    with open(local_results_path, 'w', encoding='utf-8') as f:\n        json.dump(results_data, f, indent=2, ensure_ascii=False)\n    print(f\"‚úÖ Resultados guardados localmente: {results_filename}\")\n    \n    # Verificar que el archivo se escribi√≥ correctamente\n    file_size = os.path.getsize(local_results_path)\n    print(f\"üìè Tama√±o del archivo: {file_size:,} bytes\")\n    \n    if file_size == 0:\n        raise ValueError(\"Archivo de resultados est√° vac√≠o\")\n        \nexcept Exception as e:\n    print(f\"‚ùå Error guardando resultados localmente: {e}\")\n    raise\n\n# 2. Crear CSV resumen localmente\nlocal_summary_path = f'{RESULTS_DIR}/{summary_filename}'\nif all_results:\n    try:\n        summary_data = []\n        for model_name, result in all_results.items():\n            avg = result['avg_metrics']\n            summary_data.append({\n                'Model': model_name,\n                'Precision': f\"{avg['avg_precision']:.4f}\",\n                'Recall': f\"{avg['avg_recall']:.4f}\",\n                'F1_Score': f\"{avg['avg_f1']:.4f}\",\n                'MAP': f\"{avg['avg_map']:.4f}\",\n                'MRR': f\"{avg['avg_mrr']:.4f}\",\n                'NDCG': f\"{avg['avg_ndcg']:.4f}\",\n                'Time_s': f\"{total_time/len(all_results):.2f}\"\n            })\n        \n        summary_df = pd.DataFrame(summary_data)\n        summary_df.to_csv(local_summary_path, index=False)\n        print(f\"‚úÖ Resumen guardado localmente: {summary_filename}\")\n        \n        # Mostrar resumen\n        print(f\"\\\\nüìä RESUMEN DE RESULTADOS:\")\n        print(summary_df.to_string(index=False))\n        \n    except Exception as e:\n        print(f\"‚ùå Error creando resumen CSV: {e}\")\n\n# 3. Verificar sincronizaci√≥n con Google Drive\nprint(f\"\\\\nüîÑ VERIFICANDO SINCRONIZACI√ìN CON GOOGLE DRIVE\")\nprint(f\"-\" * 50)\n\ntry:\n    # Esperar un momento para sincronizaci√≥n\n    import time\n    print(\"‚è≥ Esperando sincronizaci√≥n con Google Drive (10 segundos)...\")\n    time.sleep(10)\n    \n    # Verificar que los archivos est√°n en Drive\n    drive_results_path = f\"/content/drive/MyDrive/TesisMagister/acumulative/results/{results_filename}\"\n    drive_summary_path = f\"/content/drive/MyDrive/TesisMagister/acumulative/results/{summary_filename}\"\n    \n    results_exists = os.path.exists(drive_results_path)\n    summary_exists = os.path.exists(drive_summary_path)\n    \n    print(f\"üìÑ {results_filename}: {'‚úÖ Existe' if results_exists else '‚ùå No existe'} en Drive\")\n    print(f\"üìä {summary_filename}: {'‚úÖ Existe' if summary_exists else '‚ùå No existe'} en Drive\")\n    \n    if not results_exists:\n        print(f\"‚ö†Ô∏è Archivo de resultados no sincronizado. Intentando copia manual...\")\n        # Intentar copia manual\n        import shutil\n        os.makedirs(\"/content/drive/MyDrive/TesisMagister/acumulative/results\", exist_ok=True)\n        shutil.copy2(local_results_path, drive_results_path)\n        print(f\"‚úÖ Archivo de resultados copiado manualmente\")\n        \n    if not summary_exists:\n        print(f\"‚ö†Ô∏è Archivo de resumen no sincronizado. Intentando copia manual...\")\n        import shutil\n        shutil.copy2(local_summary_path, drive_summary_path)\n        print(f\"‚úÖ Archivo de resumen copiado manualmente\")\n    \n    # Verificaci√≥n final\n    final_results_exists = os.path.exists(drive_results_path)\n    final_summary_exists = os.path.exists(drive_summary_path)\n    \n    if final_results_exists and final_summary_exists:\n        print(f\"\\\\nüéâ ¬°ARCHIVOS VERIFICADOS EN GOOGLE DRIVE!\")\n    else:\n        print(f\"\\\\n‚ö†Ô∏è Algunos archivos pueden no estar sincronizados correctamente\")\n        \nexcept Exception as e:\n    print(f\"‚ùå Error en verificaci√≥n de Drive: {e}\")\n\n# 4. Actualizar estado final\nstatus_file = f'{DRIVE_BASE}/evaluation_status.json'\nfinal_status = {\n    'status': 'completed',\n    'timestamp': datetime.now().isoformat(),\n    'results_file': results_filename,\n    'summary_file': summary_filename,\n    'models_evaluated': len(all_results),\n    'questions_processed': len(questions_data),\n    'total_time_seconds': total_time,\n    'gpu_used': gpu_available,\n    'files_verified_in_drive': {\n        'results_file_exists': os.path.exists(f\"/content/drive/MyDrive/TesisMagister/acumulative/results/{results_filename}\"),\n        'summary_file_exists': os.path.exists(f\"/content/drive/MyDrive/TesisMagister/acumulative/results/{summary_filename}\")\n    }\n}\n\ntry:\n    with open(status_file, 'w') as f:\n        json.dump(final_status, f, indent=2)\n    print(f\"\\\\n‚úÖ Estado final actualizado: evaluation_status.json\")\nexcept Exception as e:\n    print(f\"‚ùå Error actualizando estado: {e}\")\n\n# 5. Resumen final\nprint(f\"\\\\n{'='*60}\")\nprint(f\"üéâ EVALUACI√ìN COMPLETADA\")\nprint(f\"{'='*60}\")\nprint(f\"üìÑ Archivo de resultados: {results_filename}\")\nprint(f\"üìä Archivo de resumen: {summary_filename}\")\nprint(f\"üìÅ Ubicaci√≥n: /content/drive/MyDrive/TesisMagister/acumulative/results/\")\nprint(f\"ü§ñ Modelos evaluados: {len(all_results)}\")\nprint(f\"‚ùì Preguntas procesadas: {len(questions_data)}\")\nprint(f\"‚è±Ô∏è Tiempo total: {total_time:.1f}s ({total_time/60:.1f} min)\")\nprint(f\"üöÄ GPU utilizada: {'‚úÖ' if gpu_available else '‚ùå'}\")\nprint(f\"\\\\nüëà Vuelve a Streamlit para ver las visualizaciones\")\nprint(f\"üìä Click en 'Verificar Estado' y luego 'Mostrar Resultados'\")"
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [],
   "gpuType": "T4"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  },
  "accelerator": "GPU"
 },
 "nbformat": 4,
 "nbformat_minor": 0
}