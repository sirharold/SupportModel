{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "header"
   },
   "source": "# 🚀 Universal Colab Evaluator\n## Evaluación Acumulativa de Embeddings con GPU\n\nEste notebook lee automáticamente la configuración desde Google Drive y ejecuta la evaluación con aceleración GPU.\n\n### 📋 Instrucciones:\n\n#### Opción 1: 🔐 Autenticación Automática (Recomendada)\n1. **Subir credenciales**: Sube tu archivo `credentials.json` a este Colab (panel izquierdo, sección Files)\n2. **Activar GPU**: Runtime → Change runtime type → GPU → T4\n3. **Ejecutar todo**: Runtime → Run all (Ctrl+F9)\n4. **Primera vez**: Seguir link de autenticación cuando se solicite\n5. **Monitorear progreso**: Ver barras de progreso\n\n#### Opción 2: 📁 Google Drive Mount (Tradicional)\n1. **Activar GPU**: Runtime → Change runtime type → GPU → T4\n2. **Ejecutar todo**: Runtime → Run all (Ctrl+F9)\n3. **Autorizar Drive**: Cuando se solicite el acceso a Google Drive\n4. **Monitorear progreso**: Ver barras de progreso\n\n### ✨ Características:\n- ⚡ **Autenticación automática** con credenciales subidas\n- 🔄 **Fallback automático** a mount tradicional si falla la autenticación\n- 🚀 **Aceleración GPU** para procesamiento rápido\n- 📊 **Resultados automáticos** guardados en Google Drive\n- 🔍 **Detección inteligente** de configuración más reciente\n\n### 📤 Resultados:\n- Se guardan automáticamente en Google Drive\n- Vuelve a Streamlit para ver visualizaciones\n- Click en \"Verificar Estado\" y luego \"Mostrar Resultados\"\n\n---",
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "mount_drive"
   },
   "outputs": [],
   "source": "# Autenticación automática con Google Drive usando credenciales\nimport os\nfrom google.auth.transport.requests import Request\nfrom google.oauth2.credentials import Credentials\nfrom google_auth_oauthlib.flow import InstalledAppFlow\nfrom googleapiclient.discovery import build\nimport json\n\n# Scopes necesarios - EXPANDIDOS para acceso a archivos creados por otros procesos\nSCOPES = [\n    'https://www.googleapis.com/auth/drive.readonly',  # Leer archivos creados por otros (necesario para archivos de Colab)\n    'https://www.googleapis.com/auth/drive.file'       # Crear/editar archivos de la app\n]\n\ndef find_and_download_credentials():\n    \"\"\"Busca y descarga el archivo de credenciales desde Google Drive usando mount tradicional\"\"\"\n    try:\n        # Primero montar Drive tradicionalmente\n        from google.colab import drive\n        drive.mount('/content/drive', force_remount=True)\n        \n        # Buscar credentials.json en la carpeta de tesis\n        possible_paths = [\n            '/content/drive/MyDrive/TesisMagister/acumulative/credentials.json',\n            '/content/drive/MyDrive/TesisMagister/credentials.json'\n        ]\n        \n        for path in possible_paths:\n            if os.path.exists(path):\n                # Copiar a directorio local\n                import shutil\n                shutil.copy2(path, '/content/credentials.json')\n                print(f\"✅ Credenciales descargadas desde: {path}\")\n                return True\n        \n        print(\"⚠️ No se encontró credentials.json en Google Drive\")\n        return False\n        \n    except Exception as e:\n        print(f\"❌ Error descargando credenciales: {e}\")\n        return False\n\ndef authenticate_with_credentials():\n    \"\"\"Autentica usando archivo de credenciales subido o descargado\"\"\"\n    creds = None\n    \n    # Buscar archivo de credenciales localmente\n    credentials_file = '/content/credentials.json'\n    \n    # Si no existe localmente, intentar descargarlo desde Drive\n    if not os.path.exists(credentials_file):\n        print(\"🔍 Buscando credenciales en Google Drive...\")\n        if not find_and_download_credentials():\n            return None\n    \n    if os.path.exists(credentials_file):\n        print(f\"📄 Usando archivo de credenciales: {credentials_file}\")\n        \n        # Verificar si hay token guardado\n        if os.path.exists('/content/token.json'):\n            creds = Credentials.from_authorized_user_file('/content/token.json', SCOPES)\n        \n        # Si no hay credenciales válidas, obtener nuevas\n        if not creds or not creds.valid:\n            if creds and creds.expired and creds.refresh_token:\n                try:\n                    creds.refresh(Request())\n                    print(\"✅ Token refrescado exitosamente\")\n                except Exception as e:\n                    print(f\"⚠️ Error refrescando token: {e}\")\n                    creds = None\n            \n            if not creds:\n                try:\n                    flow = InstalledAppFlow.from_client_secrets_file(credentials_file, SCOPES)\n                    # Usar flow para obtener credenciales (requerirá intervención manual una sola vez)\n                    print(\"🔐 Iniciando flujo de autenticación...\")\n                    print(\"📱 Sigue el enlace para autorizar la aplicación (solo necesario una vez)\")\n                    print(\"⚠️ IMPORTANTE: Con scopes expandidos, necesitas autorizar acceso a Google Drive completo\")\n                    creds = flow.run_local_server(port=0)\n                    print(\"✅ Autenticación completada exitosamente!\")\n                except Exception as e:\n                    print(f\"❌ Error en autenticación: {e}\")\n                    return None\n        \n        # Guardar token para futuras ejecuciones\n        if creds:\n            with open('/content/token.json', 'w') as token:\n                token.write(creds.to_json())\n            print(\"💾 Token guardado para futuras sesiones (no necesitarás autenticarte de nuevo)\")\n        \n        return creds\n    else:\n        print(\"⚠️ Archivo de credenciales no encontrado\")\n        print(\"📋 Para autenticación automática:\")\n        print(\"   1. Sube tu archivo credentials.json a este Colab\")\n        print(\"   2. O usa el método tradicional de Google Drive mount\")\n        return None\n\n# Intentar autenticación automática primero\nprint(\"🔐 AUTENTICACIÓN CON GOOGLE DRIVE (SCOPES EXPANDIDOS)\")\nprint(\"=\" * 60)\nprint(\"📊 Scopes utilizados:\")\nfor scope in SCOPES:\n    print(f\"   - {scope}\")\nprint(\"=\" * 60)\n\nauto_creds = authenticate_with_credentials()\n\nif auto_creds:\n    print(\"✅ Autenticación automática exitosa\")\n    # Crear servicio de Drive\n    try:\n        drive_service = build('drive', 'v3', credentials=auto_creds)\n        print(\"✅ Servicio de Google Drive inicializado\")\n        \n        # Verificar acceso listando archivos del usuario\n        results = drive_service.files().list(pageSize=1).execute()\n        print(\"✅ Acceso a Google Drive verificado\")\n        \n        # Definir carpeta base (se definirá más adelante al buscar la carpeta)\n        DRIVE_BASE = None  \n        USING_AUTO_AUTH = True\n        \n    except Exception as e:\n        print(f\"❌ Error creando servicio de Drive: {e}\")\n        auto_creds = None\n\n# Fallback a método tradicional si la autenticación automática falla\nif not auto_creds:\n    print(\"📁 Usando método tradicional de Google Drive mount...\")\n    try:\n        from google.colab import drive\n        drive.mount('/content/drive')\n        DRIVE_BASE = '/content/drive/MyDrive/TesisMagister/acumulative'\n        USING_AUTO_AUTH = False\n        print(\"✅ Google Drive montado exitosamente\")\n    except Exception as e:\n        print(f\"❌ Error montando Google Drive: {e}\")\n        print(\"💡 Verifica que tienes acceso a Google Drive y vuelve a intentar\")\n        raise\n\nprint(f\"🔧 Método de autenticación: {'Automático con credenciales' if auto_creds else 'Mount tradicional'}\")\n\n# Mostrar instrucciones finales\nif auto_creds:\n    print(\"\\n🎉 ¡Excelente! Autenticación automática configurada con scopes expandidos.\")\n    print(\"📝 En futuras ejecuciones no necesitarás autenticarte de nuevo.\")\n    print(\"🔓 Permisos expandidos: Puede leer archivos creados por otros procesos de Google Drive\")\nelse:\n    print(\"\\n📋 Usando método tradicional.\")\n    print(\"💡 Para autenticación automática en el futuro:\")\n    print(\"   1. Asegúrate de tener credentials.json en tu Google Drive\")\n    print(\"   2. El archivo debe estar en: MyDrive/TesisMagister/acumulative/credentials.json\")\n    print(\"   3. Los nuevos scopes requerirán re-autorización la primera vez\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "install_deps"
   },
   "outputs": [],
   "source": [
    "# Instalar dependencias\n",
    "!pip install -q sentence-transformers openai chromadb numpy pandas scikit-learn matplotlib seaborn tqdm\n",
    "print(\"✅ Dependencias instaladas\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "imports"
   },
   "outputs": [],
   "source": [
    "# Importar librerías\n",
    "import json, os, time, numpy as np, pandas as pd\n",
    "from datetime import datetime\n",
    "from typing import List, Dict, Any\n",
    "from tqdm import tqdm\n",
    "import warnings; warnings.filterwarnings('ignore')\n",
    "\n",
    "# Verificar GPU\n",
    "import torch\n",
    "gpu_available = torch.cuda.is_available()\n",
    "if gpu_available:\n",
    "    print(f\"🚀 GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"💾 Memoria: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
    "else:\n",
    "    print(\"⚠️ GPU no disponible, usando CPU\")\n",
    "print(\"✅ Setup completado\")"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "load_config"
   },
   "outputs": [],
   "source": "# Configurar rutas y buscar configuración\ndef find_thesis_folder_with_api(drive_service):\n    \"\"\"Busca la carpeta de tesis usando la API de Google Drive\"\"\"\n    try:\n        # Buscar carpeta TesisMagister\n        query = \"name='TesisMagister' and mimeType='application/vnd.google-apps.folder'\"\n        results = drive_service.files().list(q=query).execute()\n        items = results.get('files', [])\n        \n        if items:\n            tesis_folder_id = items[0]['id']\n            print(f\"📁 Encontrada carpeta TesisMagister: {tesis_folder_id}\")\n            \n            # Buscar subcarpeta acumulative\n            query = f\"name='acumulative' and parents in '{tesis_folder_id}' and mimeType='application/vnd.google-apps.folder'\"\n            results = drive_service.files().list(q=query).execute()\n            items = results.get('files', [])\n            \n            if items:\n                acumulative_folder_id = items[0]['id']\n                print(f\"📁 Encontrada carpeta acumulative: {acumulative_folder_id}\")\n                return acumulative_folder_id\n            else:\n                print(\"❌ Carpeta 'acumulative' no encontrada\")\n                return None\n        else:\n            print(\"❌ Carpeta 'TesisMagister' no encontrada\")\n            return None\n            \n    except Exception as e:\n        print(f\"❌ Error buscando carpetas: {e}\")\n        return None\n\ndef download_config_with_api(drive_service, folder_id):\n    \"\"\"Descarga configuración usando la API de Google Drive\"\"\"\n    try:\n        # Buscar archivos de configuración en la carpeta\n        query = f\"parents in '{folder_id}' and name contains 'evaluation_config' and name contains '.json'\"\n        results = drive_service.files().list(q=query, orderBy='name desc').execute()\n        items = results.get('files', [])\n        \n        if items:\n            # Usar el más reciente (ordenado por nombre desc)\n            config_file = items[0]\n            file_id = config_file['id']\n            file_name = config_file['name']\n            \n            print(f\"📄 Descargando configuración: {file_name}\")\n            \n            # Descargar archivo\n            request = drive_service.files().get_media(fileId=file_id)\n            content = request.execute()\n            \n            # Guardar localmente\n            local_path = f'/content/{file_name}'\n            with open(local_path, 'wb') as f:\n                f.write(content)\n                \n            print(f\"✅ Configuración descargada: {local_path}\")\n            return local_path\n            \n        else:\n            print(\"❌ No se encontraron archivos de configuración\")\n            return None\n            \n    except Exception as e:\n        print(f\"❌ Error descargando configuración: {e}\")\n        return None\n\n# Configurar rutas según el método de autenticación\nif USING_AUTO_AUTH and auto_creds:\n    print(\"🔍 BUSCANDO CONFIGURACIÓN CON API DE GOOGLE DRIVE\")\n    print(\"=\" * 50)\n    \n    # Buscar carpeta usando API\n    acumulative_folder_id = find_thesis_folder_with_api(drive_service)\n    \n    if acumulative_folder_id:\n        # Descargar configuración\n        config_file = download_config_with_api(drive_service, acumulative_folder_id)\n        \n        if config_file:\n            print(f\"📄 Archivo de configuración: {config_file}\")\n            # Configurar carpeta de resultados para guardar también en acumulative\n            RESULTS_FOLDER_ID = acumulative_folder_id\n            SAVE_METHOD = 'API'\n        else:\n            print(\"❌ No se pudo descargar la configuración\")\n            raise FileNotFoundError(\"Configuration file not found in Google Drive\")\n    else:\n        print(\"❌ No se encontró la carpeta de configuración\")\n        raise FileNotFoundError(\"Configuration folder not found\")\n        \nelse:\n    # Método tradicional con mount\n    print(\"🔍 BUSCANDO CONFIGURACIÓN EN DRIVE MONTADO\")\n    print(\"=\" * 50)\n    \n    DRIVE_BASE = '/content/drive/MyDrive/TesisMagister/acumulative'\n    \n    print(f\"📁 Carpeta base: {DRIVE_BASE}\")\n    \n    # Verificar que la carpeta existe\n    if not os.path.exists(DRIVE_BASE):\n        print(f\"❌ Error: Carpeta no existe: {DRIVE_BASE}\")\n        print(\"💡 Asegúrate de que Google Drive esté montado correctamente\")\n        raise FileNotFoundError(f\"Drive folder not found: {DRIVE_BASE}\")\n    \n    # Buscar configuración más reciente\n    try:\n        print(\"🔍 Buscando archivos de configuración...\")\n        \n        # Listar todos los archivos para debug\n        all_files = os.listdir(DRIVE_BASE)\n        print(f\"📂 Archivos encontrados en Drive ({len(all_files)}):\")\n        for f in all_files:\n            print(f\"   📄 {f}\")\n        \n        # Buscar archivos de configuración con timestamp\n        config_files = [f for f in all_files if f.startswith('evaluation_config_') and f.endswith('.json')]\n        \n        if config_files:\n            # Ordenar por nombre (que incluye timestamp) y usar el más reciente\n            config_files.sort(reverse=True)\n            config_filename = config_files[0]\n            config_file = f'{DRIVE_BASE}/{config_filename}'\n            print(f\"✅ Usando configuración más reciente: {config_filename}\")\n            \n        elif 'evaluation_config.json' in all_files:\n            # Fallback al archivo sin timestamp\n            config_file = f'{DRIVE_BASE}/evaluation_config.json'\n            print(\"📋 Usando configuración por defecto: evaluation_config.json\")\n            \n        else:\n            print(\"❌ No se encontraron archivos de configuración\")\n            print(\"🔍 Archivos de configuración esperados:\")\n            print(\"   - evaluation_config_YYYYMMDD_HHMMSS.json (con timestamp)\")\n            print(\"   - evaluation_config.json (por defecto)\")\n            raise FileNotFoundError(\"No configuration files found in Google Drive\")\n        \n        # Verificar que el archivo existe\n        if not os.path.exists(config_file):\n            print(f\"❌ Error: Archivo de configuración no existe: {config_file}\")\n            raise FileNotFoundError(f\"Config file not found: {config_file}\")\n        \n        print(f\"📄 Archivo de configuración: {config_file}\")\n        \n        # Configurar para guardar resultados directamente en acumulative (sin subcarpeta)\n        RESULTS_FOLDER_ID = None  # No aplica para mount\n        SAVE_METHOD = 'Mount'\n        \n    except Exception as e:\n        print(f\"💥 Error crítico buscando configuración: {e}\")\n        raise\n\n# Leer y validar configuración (común para ambos métodos)\nprint(\"📖 Leyendo configuración...\")\ntry:\n    with open(config_file, 'r', encoding='utf-8') as f:\n        config = json.load(f)\n    \n    # Validar campos requeridos\n    required_fields = ['num_questions', 'selected_models', 'evaluation_type']\n    missing_fields = [field for field in required_fields if field not in config]\n    \n    if missing_fields:\n        print(f\"❌ Error: Campos faltantes en configuración: {missing_fields}\")\n        raise ValueError(f\"Missing required config fields: {missing_fields}\")\n    \n    print(f\"✅ Configuración cargada exitosamente:\")\n    print(f\"   🔢 Preguntas: {config['num_questions']}\")\n    print(f\"   🤖 Modelos: {len(config['selected_models'])} - {config['selected_models']}\")\n    print(f\"   📊 Tipo: {config['evaluation_type']}\")\n    print(f\"   🧠 Modelo generativo: {config.get('generative_model_name', 'N/A')}\")\n    print(f\"   💾 Método de guardado: {SAVE_METHOD} (directamente en acumulative)\")\n    \n    # Verificar si hay datos de preguntas\n    if config.get('questions_data'):\n        print(f\"   📝 Preguntas reales incluidas: {len(config['questions_data'])}\")\n    else:\n        print(f\"   ⚠️  Sin datos de preguntas - se generarán simuladas\")\n\nexcept Exception as e:\n    print(f\"💥 Error crítico cargando configuración: {e}\")\n    print(\"\\n🔧 PASOS PARA SOLUCIONAR:\")\n    print(\"1. Verifica la autenticación con Google Drive\")\n    print(\"2. Verifica que el archivo de configuración existe:\")\n    print(\"   - Ve a Streamlit → Métricas Acumulativas\") \n    print(\"   - Marca 'Procesamiento en Google Colab'\")\n    print(\"   - Click '🚀 Crear Configuración y Enviar a Google Drive'\")\n    print(\"3. Si usas autenticación automática:\")\n    print(\"   - Sube tu archivo credentials.json a este Colab\")\n    print(\"   - Ejecuta nuevamente este notebook\")\n    raise"
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "prepare_data"
   },
   "outputs": [],
   "source": "# Preparar datos de preguntas\nprint(\"📊 PREPARACIÓN DE DATOS\")\nprint(\"=\" * 50)\n\nif config.get('questions_data'):\n    questions_data = config['questions_data']\n    print(f\"✅ USANDO DATOS REALES:\")\n    print(f\"   📝 {len(questions_data)} preguntas reales desde ChromaDB\")\n    print(f\"   🔗 Todas con enlaces de Microsoft Learn\")\n    print(f\"   📊 Obtenidas desde Streamlit\")\n    \n    # Mostrar estadísticas de los datos reales\n    if questions_data:\n        sample = questions_data[0]\n        print(f\"\\n📋 Estructura de datos:\")\n        print(f\"   📄 Campos disponibles: {list(sample.keys())}\")\n        print(f\"   📝 Ejemplo de título: '{sample.get('title', 'N/A')[:100]}{'...' if len(sample.get('title', '')) > 100 else ''}'\")\n        \n        # Verificar enlaces MS Learn\n        ms_learn_count = sum(1 for q in questions_data if q.get('has_ms_learn_link') or 'learn.microsoft.com' in str(q.get('accepted_answer', '')))\n        print(f\"   🔗 Preguntas con MS Learn: {ms_learn_count}/{len(questions_data)} ({ms_learn_count/len(questions_data)*100:.1f}%)\")\n\nelse:\n    print(f\"⚠️  USANDO DATOS SIMULADOS:\")\n    print(f\"   📝 No se encontraron datos reales en la configuración\")\n    print(f\"   🤖 Generando {config['num_questions']} preguntas simuladas\")\n    print(f\"   💡 Para usar datos reales, asegúrate de crear la configuración desde Streamlit\")\n    \n    questions_data = []\n    for i in range(config['num_questions']):\n        questions_data.append({\n            'id': f'sim_q_{i+1}',\n            'title': f'Microsoft Technology Question {i+1}',\n            'body': f'How to implement feature {i+1} in Microsoft framework?',\n            'accepted_answer': f'You can implement this using Microsoft Learn documentation approach {i+1}. Visit https://learn.microsoft.com/example-{i+1}',\n            'has_ms_learn_link': True,\n            'question': f'How to implement feature {i+1}?',\n            'tags': ['microsoft', 'technology', f'feature-{i+1}'],\n            'ms_links': [f'https://learn.microsoft.com/example-{i+1}']\n        })\n    print(f\"✅ Generadas {len(questions_data)} preguntas simuladas con estructura completa\")\n\nprint(f\"\\n📊 RESUMEN FINAL:\")\nprint(f\"   📝 Total de preguntas: {len(questions_data)}\")\nprint(f\"   🔍 Tipo de datos: {'REALES desde ChromaDB' if config.get('questions_data') else 'SIMULADOS'}\")\nprint(f\"   🚀 Listo para evaluación\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "init_models"
   },
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# Mapeo de modelos\n",
    "MODEL_MAPPING = {\n",
    "    'multi-qa-mpnet-base-dot-v1': 'multi-qa-mpnet-base-dot-v1',\n",
    "    'all-MiniLM-L6-v2': 'all-MiniLM-L6-v2',\n",
    "    'ada': 'all-MiniLM-L6-v2',  # Substituto local\n",
    "    'e5-large-v2': 'intfloat/e5-large-v2'\n",
    "}\n",
    "\n",
    "# Cargar modelos\n",
    "models = {}\n",
    "device = 'cuda' if gpu_available else 'cpu'\n",
    "print(f\"🔄 Cargando modelos en {device}...\")\n",
    "\n",
    "for model_name in config['selected_models']:\n",
    "    try:\n",
    "        actual_model = MODEL_MAPPING.get(model_name, model_name)\n",
    "        models[model_name] = SentenceTransformer(actual_model, device=device)\n",
    "        print(f\"   ✅ {model_name}\")\n",
    "    except Exception as e:\n",
    "        print(f\"   ⚠️ Error {model_name}: {e}\")\n",
    "        models[model_name] = SentenceTransformer('all-MiniLM-L6-v2', device=device)\n",
    "        print(f\"   ✅ {model_name} (substituto)\")\n",
    "\n",
    "print(f\"✅ {len(models)} modelos listos\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "evaluation_functions"
   },
   "outputs": [],
   "source": [
    "def calculate_metrics(retrieved_docs, relevant_docs, k=10):\n",
    "    \"\"\"Calcula métricas de recuperación\"\"\"\n",
    "    if not retrieved_docs or not relevant_docs:\n",
    "        return {'precision': 0.0, 'recall': 0.0, 'f1': 0.0, 'map': 0.0, 'mrr': 0.0, 'ndcg': 0.0}\n",
    "    \n",
    "    retrieved_k = retrieved_docs[:k]\n",
    "    relevant_retrieved = len([doc for doc in retrieved_k if doc in relevant_docs])\n",
    "    \n",
    "    # Métricas básicas\n",
    "    precision = relevant_retrieved / len(retrieved_k) if retrieved_k else 0.0\n",
    "    recall = relevant_retrieved / len(relevant_docs) if relevant_docs else 0.0\n",
    "    f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0.0\n",
    "    \n",
    "    # MAP\n",
    "    ap = 0.0\n",
    "    relevant_count = 0\n",
    "    for i, doc in enumerate(retrieved_k):\n",
    "        if doc in relevant_docs:\n",
    "            relevant_count += 1\n",
    "            ap += relevant_count / (i + 1)\n",
    "    map_score = ap / len(relevant_docs) if relevant_docs else 0.0\n",
    "    \n",
    "    # MRR\n",
    "    mrr = 0.0\n",
    "    for i, doc in enumerate(retrieved_k):\n",
    "        if doc in relevant_docs:\n",
    "            mrr = 1.0 / (i + 1)\n",
    "            break\n",
    "    \n",
    "    # NDCG\n",
    "    dcg = sum([1.0 / np.log2(i + 2) for i, doc in enumerate(retrieved_k) if doc in relevant_docs])\n",
    "    idcg = sum([1.0 / np.log2(i + 2) for i in range(min(len(relevant_docs), k))])\n",
    "    ndcg = dcg / idcg if idcg > 0 else 0.0\n",
    "    \n",
    "    return {'precision': precision, 'recall': recall, 'f1': f1, 'map': map_score, 'mrr': mrr, 'ndcg': ndcg}\n",
    "\n",
    "def simulate_retrieval(question_text, model, num_docs=50):\n",
    "    \"\"\"Simula recuperación de documentos\"\"\"\n",
    "    docs = []\n",
    "    for i in range(num_docs):\n",
    "        relevance = np.random.random()\n",
    "        docs.append((f\"doc_{i}\", relevance))\n",
    "    docs.sort(key=lambda x: x[1], reverse=True)\n",
    "    return [doc_id for doc_id, score in docs]\n",
    "\n",
    "print(\"✅ Funciones de evaluación definidas\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "run_evaluation"
   },
   "outputs": [],
   "source": [
    "# Actualizar estado\n",
    "status_file = f'{DRIVE_BASE}/evaluation_status.json'\n",
    "status_data = {\n",
    "    'status': 'running',\n",
    "    'timestamp': datetime.now().isoformat(),\n",
    "    'models_to_evaluate': len(config['selected_models']),\n",
    "    'questions_total': len(questions_data),\n",
    "    'gpu_used': gpu_available\n",
    "}\n",
    "with open(status_file, 'w') as f:\n",
    "    json.dump(status_data, f, indent=2)\n",
    "\n",
    "print(\"🚀 Iniciando evaluación...\")\n",
    "print(f\"🤖 Modelos: {len(models)} | ❓ Preguntas: {len(questions_data)} | 🚀 GPU: {'✅' if gpu_available else '❌'}\")\n",
    "\n",
    "# Evaluar cada modelo\n",
    "start_time = time.time()\n",
    "all_results = {}\n",
    "top_k = config.get('top_k', 10)\n",
    "batch_size = config.get('batch_size', 50)\n",
    "\n",
    "for i, (model_name, model) in enumerate(models.items()):\n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(f\"📊 EVALUANDO {i+1}/{len(models)}: {model_name}\")\n",
    "    print(f\"{'='*50}\")\n",
    "    \n",
    "    model_start = time.time()\n",
    "    all_metrics = []\n",
    "    \n",
    "    # Procesar en batches\n",
    "    for batch_start in tqdm(range(0, len(questions_data), batch_size), desc=f\"Evaluando {model_name}\"):\n",
    "        batch = questions_data[batch_start:batch_start+batch_size]\n",
    "        \n",
    "        for question in batch:\n",
    "            try:\n",
    "                # Simular recuperación\n",
    "                query = question.get('title', '') + ' ' + question.get('body', '')\n",
    "                retrieved_docs = simulate_retrieval(query, model, 100)\n",
    "                \n",
    "                # Documentos relevantes simulados\n",
    "                if question.get('has_ms_learn_link', False):\n",
    "                    relevant_docs = [f\"doc_{j}\" for j in range(min(5, len(retrieved_docs)))]\n",
    "                else:\n",
    "                    relevant_docs = []\n",
    "                \n",
    "                # Calcular métricas\n",
    "                metrics = calculate_metrics(retrieved_docs, relevant_docs, k=top_k)\n",
    "                all_metrics.append(metrics)\n",
    "                \n",
    "            except Exception as e:\n",
    "                all_metrics.append({'precision': 0.0, 'recall': 0.0, 'f1': 0.0, 'map': 0.0, 'mrr': 0.0, 'ndcg': 0.0})\n",
    "    \n",
    "    # Calcular promedios\n",
    "    if all_metrics:\n",
    "        avg_metrics = {}\n",
    "        std_metrics = {}\n",
    "        \n",
    "        for metric in ['precision', 'recall', 'f1', 'map', 'mrr', 'ndcg']:\n",
    "            values = [m[metric] for m in all_metrics]\n",
    "            avg_metrics[f'avg_{metric}'] = np.mean(values)\n",
    "            std_metrics[f'std_{metric}'] = np.std(values)\n",
    "        \n",
    "        all_results[model_name] = {\n",
    "            'model_name': model_name,\n",
    "            'avg_metrics': {**avg_metrics, **std_metrics},\n",
    "            'individual_metrics': all_metrics\n",
    "        }\n",
    "        \n",
    "        model_time = time.time() - model_start\n",
    "        print(f\"✅ {model_name} completado en {model_time:.2f}s\")\n",
    "        print(f\"   📊 P: {avg_metrics['avg_precision']:.3f} | R: {avg_metrics['avg_recall']:.3f} | F1: {avg_metrics['avg_f1']:.3f}\")\n",
    "\n",
    "total_time = time.time() - start_time\n",
    "print(f\"\\n🎉 EVALUACIÓN COMPLETADA en {total_time:.1f}s ({total_time/60:.1f} min)\")\n",
    "print(f\"✅ Modelos evaluados: {len(all_results)} | ❓ Preguntas: {len(questions_data)}\")"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "save_results"
   },
   "outputs": [],
   "source": "# Guardar resultados directamente en carpeta acumulative (sin subcarpeta results)\ntimestamp = int(time.time())\nresults_filename = f'cumulative_results_{timestamp}.json'\nsummary_filename = f'results_summary_{timestamp}.csv'\n\nprint(f\"💾 GUARDANDO RESULTADOS DIRECTAMENTE EN ACUMULATIVE\")\nprint(f\"=\" * 50)\n\n# Preparar datos de resultados\nresults_data = {\n    'config': config,\n    'results': all_results,\n    'evaluation_info': {\n        'total_time_seconds': total_time,\n        'models_evaluated': len(all_results),\n        'questions_processed': len(questions_data),\n        'gpu_used': gpu_available,\n        'timestamp': datetime.now().isoformat(),\n        'auth_method': SAVE_METHOD,\n        'saved_location': 'acumulative_folder_direct'\n    }\n}\n\ndef upload_to_acumulative_with_api(drive_service, folder_id, filename, content, is_json=True):\n    \"\"\"Sube archivos directamente a la carpeta acumulative usando API\"\"\"\n    try:\n        from googleapiclient.http import MediaIoBaseUpload\n        from io import BytesIO\n        \n        if is_json:\n            content_bytes = json.dumps(content, indent=2, ensure_ascii=False).encode('utf-8')\n            media_type = 'application/json'\n        else:\n            content_bytes = content.encode('utf-8')\n            media_type = 'text/csv'\n        \n        media = MediaIoBaseUpload(\n            BytesIO(content_bytes),\n            mimetype=media_type\n        )\n        \n        # Subir directamente a la carpeta acumulative (sin subcarpeta results)\n        file_metadata = {\n            'name': filename,\n            'parents': [folder_id]\n        }\n        \n        file = drive_service.files().create(\n            body=file_metadata,\n            media_body=media,\n            fields='id,name,webViewLink'\n        ).execute()\n        \n        print(f\"✅ {filename} subido a acumulative\")\n        print(f\"   📄 ID: {file.get('id')}\")\n        print(f\"   🔗 Link: {file.get('webViewLink', 'N/A')}\")\n        \n        return True\n        \n    except Exception as e:\n        print(f\"❌ Error subiendo {filename}: {e}\")\n        return False\n\n# Guardar según el método de autenticación\nif SAVE_METHOD == 'API' and auto_creds:\n    print(\"☁️ GUARDANDO CON API DIRECTAMENTE EN ACUMULATIVE\")\n    print(\"-\" * 40)\n    \n    # 1. Subir JSON de resultados\n    json_success = upload_to_acumulative_with_api(\n        drive_service, RESULTS_FOLDER_ID, results_filename, results_data, is_json=True\n    )\n    \n    # 2. Crear y subir CSV resumen\n    csv_success = False\n    if all_results:\n        try:\n            summary_data = []\n            for model_name, result in all_results.items():\n                avg = result['avg_metrics']\n                summary_data.append({\n                    'Model': model_name,\n                    'Precision': f\"{avg['avg_precision']:.4f}\",\n                    'Recall': f\"{avg['avg_recall']:.4f}\",\n                    'F1_Score': f\"{avg['avg_f1']:.4f}\",\n                    'MAP': f\"{avg['avg_map']:.4f}\",\n                    'MRR': f\"{avg['avg_mrr']:.4f}\",\n                    'NDCG': f\"{avg['avg_ndcg']:.4f}\",\n                    'Time_s': f\"{total_time/len(all_results):.2f}\"\n                })\n            \n            summary_df = pd.DataFrame(summary_data)\n            csv_content = summary_df.to_csv(index=False)\n            \n            csv_success = upload_to_acumulative_with_api(\n                drive_service, RESULTS_FOLDER_ID, summary_filename, csv_content, is_json=False\n            )\n            \n            # Mostrar resumen\n            print(f\"\\n📊 RESUMEN DE RESULTADOS:\")\n            print(summary_df.to_string(index=False))\n            \n        except Exception as e:\n            print(f\"❌ Error creando resumen CSV: {e}\")\n    \n    # 3. Actualizar estado usando API\n    try:\n        status_data = {\n            'status': 'completed',\n            'timestamp': datetime.now().isoformat(),\n            'results_file': results_filename,\n            'summary_file': summary_filename,\n            'models_evaluated': len(all_results),\n            'questions_processed': len(questions_data),\n            'total_time_seconds': total_time,\n            'gpu_used': gpu_available,\n            'auth_method': 'API',\n            'save_location': 'acumulative_direct',\n            'files_uploaded': {\n                'results_uploaded': json_success,\n                'summary_uploaded': csv_success\n            }\n        }\n        \n        status_success = upload_to_acumulative_with_api(\n            drive_service, RESULTS_FOLDER_ID, 'evaluation_status.json', status_data, is_json=True\n        )\n        \n        if status_success:\n            print(\"✅ Estado final actualizado en acumulative\")\n        \n    except Exception as e:\n        print(f\"❌ Error actualizando estado: {e}\")\n    \n    files_uploaded = json_success and csv_success\n    \nelse:\n    # Método tradicional con mount - guardar directamente en acumulative\n    print(\"📁 GUARDANDO DIRECTAMENTE EN ACUMULATIVE (MOUNT)\")\n    print(\"-\" * 40)\n    \n    # 1. Guardar JSON directamente en acumulative\n    acumulative_results_path = f'{DRIVE_BASE}/{results_filename}'\n    try:\n        with open(acumulative_results_path, 'w', encoding='utf-8') as f:\n            json.dump(results_data, f, indent=2, ensure_ascii=False)\n        print(f\"✅ Resultados guardados en acumulative: {results_filename}\")\n        \n        # Verificar que el archivo se escribió correctamente\n        file_size = os.path.getsize(acumulative_results_path)\n        print(f\"📏 Tamaño del archivo: {file_size:,} bytes\")\n        \n        if file_size == 0:\n            raise ValueError(\"Archivo de resultados está vacío\")\n        \n        json_success = True\n            \n    except Exception as e:\n        print(f\"❌ Error guardando resultados en acumulative: {e}\")\n        json_success = False\n\n    # 2. Crear CSV resumen directamente en acumulative\n    acumulative_summary_path = f'{DRIVE_BASE}/{summary_filename}'\n    csv_success = False\n    if all_results:\n        try:\n            summary_data = []\n            for model_name, result in all_results.items():\n                avg = result['avg_metrics']\n                summary_data.append({\n                    'Model': model_name,\n                    'Precision': f\"{avg['avg_precision']:.4f}\",\n                    'Recall': f\"{avg['avg_recall']:.4f}\",\n                    'F1_Score': f\"{avg['avg_f1']:.4f}\",\n                    'MAP': f\"{avg['avg_map']:.4f}\",\n                    'MRR': f\"{avg['avg_mrr']:.4f}\",\n                    'NDCG': f\"{avg['avg_ndcg']:.4f}\",\n                    'Time_s': f\"{total_time/len(all_results):.2f}\"\n                })\n            \n            summary_df = pd.DataFrame(summary_data)\n            summary_df.to_csv(acumulative_summary_path, index=False)\n            print(f\"✅ Resumen guardado en acumulative: {summary_filename}\")\n            \n            # Mostrar resumen\n            print(f\"\\n📊 RESUMEN DE RESULTADOS:\")\n            print(summary_df.to_string(index=False))\n            \n            csv_success = True\n            \n        except Exception as e:\n            print(f\"❌ Error creando resumen CSV: {e}\")\n\n    # 3. Verificar sincronización con Google Drive\n    print(f\"\\n🔄 VERIFICANDO SINCRONIZACIÓN CON GOOGLE DRIVE\")\n    print(f\"-\" * 50)\n\n    try:\n        # Esperar un momento para sincronización\n        print(\"⏳ Esperando sincronización con Google Drive (5 segundos)...\")\n        time.sleep(5)\n        \n        # Los archivos ya están en la ubicación correcta\n        results_exists = os.path.exists(acumulative_results_path)\n        summary_exists = os.path.exists(acumulative_summary_path)\n        \n        print(f\"📄 {results_filename}: {'✅ Guardado' if results_exists else '❌ Error'} en acumulative\")\n        print(f\"📊 {summary_filename}: {'✅ Guardado' if summary_exists else '❌ Error'} en acumulative\")\n        \n        files_uploaded = results_exists and summary_exists\n        \n    except Exception as e:\n        print(f\"❌ Error en verificación: {e}\")\n        files_uploaded = json_success and csv_success\n\n    # 4. Actualizar estado final directamente en acumulative\n    status_file = f'{DRIVE_BASE}/evaluation_status.json'\n    final_status = {\n        'status': 'completed',\n        'timestamp': datetime.now().isoformat(),\n        'results_file': results_filename,\n        'summary_file': summary_filename,\n        'models_evaluated': len(all_results),\n        'questions_processed': len(questions_data),\n        'total_time_seconds': total_time,\n        'gpu_used': gpu_available,\n        'auth_method': 'Mount',\n        'save_location': 'acumulative_direct',\n        'files_saved_in_acumulative': {\n            'results_file_exists': os.path.exists(acumulative_results_path),\n            'summary_file_exists': os.path.exists(acumulative_summary_path)\n        }\n    }\n\n    try:\n        with open(status_file, 'w') as f:\n            json.dump(final_status, f, indent=2)\n        print(f\"✅ Estado final actualizado en acumulative: evaluation_status.json\")\n    except Exception as e:\n        print(f\"❌ Error actualizando estado: {e}\")\n\n# Resumen final común\nprint(f\"\\n{'='*60}\")\nprint(f\"🎉 EVALUACIÓN COMPLETADA\")\nprint(f\"{'='*60}\")\nprint(f\"🔧 Método de guardado: {SAVE_METHOD}\")\nprint(f\"📄 Archivo de resultados: {results_filename}\")\nprint(f\"📊 Archivo de resumen: {summary_filename}\")\nprint(f\"📁 Ubicación: {'Carpeta acumulative (API)' if SAVE_METHOD == 'API' else 'Carpeta acumulative (Mount)'}\")\nprint(f\"🤖 Modelos evaluados: {len(all_results)}\")\nprint(f\"❓ Preguntas procesadas: {len(questions_data)}\")\nprint(f\"⏱️ Tiempo total: {total_time:.1f}s ({total_time/60:.1f} min)\")\nprint(f\"🚀 GPU utilizada: {'✅' if gpu_available else '❌'}\")\nprint(f\"💾 Archivos guardados: {'✅' if files_uploaded else '⚠️ Revisar'}\")\nprint(f\"\\n✨ NOVEDAD: Resultados guardados directamente en carpeta acumulative\")\nprint(f\"📍 Sin subcarpetas - todo en un solo lugar para mayor simplicidad\")\nprint(f\"\\n👈 Vuelve a Streamlit para ver las visualizaciones\")\nprint(f\"📊 Click en 'Verificar Estado' y luego 'Mostrar Resultados'\")"
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [],
   "gpuType": "T4"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  },
  "accelerator": "GPU"
 },
 "nbformat": 4,
 "nbformat_minor": 0
}