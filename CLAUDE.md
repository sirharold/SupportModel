# CLAUDE.md - An√°lisis y Directrices del Proyecto

## üìã DIRECTRICES PRINCIPALES

### ‚ö†Ô∏è IMPORTANTE - Contracto de Datos
- **NO cambiar el archivo de resultados** generado en el Colab
- El archivo de resultados se usa en la app de Streamlit - es un CONTRATO
- **SIEMPRE usar solo m√©tricas reales**, no aleatorias, simuladas o inventadas
- Si se necesita crear m√©tricas simuladas por alguna raz√≥n, debe estar EXPL√çCITO en la app de Streamlit

### üóÇÔ∏è Estructura del Proyecto
- **ChromaDB Principal**: `/Users/haroldgomez/chromadb2`
- **Archivos Parquet**: `colab_data/docs_[modelo]_with_embeddings_*.parquet`
- **Archivos de Resultados**: `cumulative_results_*.json`
- **Datos de Entrenamiento**: `data/train_set.json`, `data/val_set.json`

## üîç AN√ÅLISIS ACTUAL - PROBLEMA DE M√âTRICAS CERO

### üìä Estado de las Colecciones ChromaDB
**Ubicaci√≥n**: `/Users/haroldgomez/chromadb2`

#### Preguntas:
- `questions_ada`: 13,436 preguntas ‚úÖ
- `questions_mpnet`: 13,436 preguntas ‚úÖ
- `questions_minilm`: 13,436 preguntas ‚úÖ
- `questions_e5large`: 13,436 preguntas ‚úÖ
- **`questions_withlinks`: 2,067 preguntas** ‚úÖ (CON LINKS VALIDADOS)

#### Documentos:
- `docs_ada`: 187,031 documentos ‚úÖ
- `docs_mpnet`: 187,031 documentos ‚úÖ
- `docs_minilm`: 187,031 documentos ‚úÖ
- `docs_e5large`: 187,031 documentos ‚úÖ

### üö® CAUSA RA√çZ DEL PROBLEMA

**PROBLEMA IDENTIFICADO**: Las m√©tricas est√°n en cero NO por falta de datos, sino por **BAJA CALIDAD DE EMBEDDINGS**

#### Evidencia del Debug:
```
Pregunta: "Azure disk encryption with Platform Managed Keys..."
Documento relevante: "Server-side encryption of Azure Disk Storage"

‚ùå PROBLEMA:
- Documentos irrelevantes: Ranks 1-8 (scores 0.85-0.50)
- Documentos RELEVANTES: Ranks 9-10 (scores 0.45-0.40)

üìä M√©tricas resultantes:
- Precision@5: 0.000 (ning√∫n relevante en top-5)
- Precision@10: 0.200 (2 relevantes en top-10 pero mal rankeados)
```

#### Cobertura de Datos:
- **‚úÖ 68.2% de cobertura** entre preguntas y documentos
- **‚úÖ 4,138 preguntas tienen ground truth v√°lido**
- **‚úÖ URLs normalizadas funcionan correctamente**
- **‚ùå Los embeddings no capturan sem√°ntica t√©cnica correctamente**

### üìà Performance por Modelo:
- **Ada**: M√©tricas promedio > 0 (encuentra algunos relevantes)
- **MPNet**: M√©tricas promedio > 0 (encuentra algunos relevantes)
- **E5-large**: Todas las m√©tricas = 0 (no encuentra relevantes)
- **MiniLM**: M√©tricas muy bajas

## üîß SOLUCIONES RECOMENDADAS

### 1. üéØ MODELOS ESPECIALIZADOS
**Reemplazar modelos actuales por:**

#### Premium (Mejores resultados):
- `text-embedding-3-large` (OpenAI) - 3072D
- `text-embedding-3-small` (OpenAI) - 1536D

#### Open Source Especializados:
- `sentence-transformers/multi-qa-mpnet-base-dot-v1` - Q&A especializado
- `microsoft/codebert-base` - Contenido t√©cnico
- `sentence-transformers/all-mpnet-base-v2` - Documentos largos

#### Multiling√ºes:
- `sentence-transformers/paraphrase-multilingual-mpnet-base-v2`
- `intfloat/multilingual-e5-large` (versi√≥n mejorada)

### 2. üéì FINE-TUNING EN DOMINIO MICROSOFT LEARN
- Usar los 2,067 pares de `questions_withlinks`
- Contrastive Learning con pares positivos/negativos
- Herramientas: `sentence-transformers`, OpenAI Fine-tuning API

### 3. üîÑ HYBRID SEARCH (EMBEDDINGS + KEYWORD)
```python
def hybrid_search(query, top_k=10):
    semantic_results = embedding_search(query, top_k=20)
    keyword_results = bm25_search(query, top_k=20)
    return combine_scores(semantic_results, keyword_results, weights=[0.7, 0.3])
```

### 4. üß† RERANKING AVANZADO
- CrossEncoder m√°s grande: `ms-marco-MiniLM-L-12-v2`
- Pipeline multi-etapa: Retrieval ‚Üí Rerank1 ‚Üí Rerank2
- Query expansion y pseudo relevance feedback

## üìÖ PLAN DE IMPLEMENTACI√ìN

### Fase 1 (R√°pida - 1-2 d√≠as):
1. ‚úÖ Cambiar a `multi-qa-mpnet-base-dot-v1`
2. ‚úÖ Implementar CrossEncoder m√°s grande

### Fase 2 (Media - 1 semana):
3. ‚è≥ Implementar hybrid search con BM25
4. ‚è≥ Optimizar pesos de combinaci√≥n

### Fase 3 (Avanzada - 2-3 semanas):
5. ‚è≥ Fine-tuning con datos de Microsoft Learn
6. ‚è≥ Pipeline de reranking multi-etapa

## üîÑ √öLTIMAS ACTUALIZACIONES

### Colab - Versi√≥n 3.5 (Min-Max Normalization):
- ‚úÖ Corregido modelo BERTScore: `distiluse-base-multilingual-cased-v2`
- ‚úÖ Agregada normalizaci√≥n URL completa
- ‚úÖ Expandidas m√©tricas RAG (6 RAGAS + 3 BERTScore)
- ‚úÖ Aplicada normalizaci√≥n Min-Max a CrossEncoder
- ‚úÖ Metodolog√≠a actualizada en Streamlit

### Streamlit Actualizado:
- ‚úÖ L√≥gica de scoring statistics adaptada a datos disponibles
- ‚úÖ Tabla din√°mica sin columnas N/A
- ‚úÖ Metodolog√≠a de evaluaci√≥n actualizada para CrossEncoder Min-Max

## üéØ PR√ìXIMOS PASOS RECOMENDADOS

1. **Implementar modelo especializado**: Cambiar a `multi-qa-mpnet-base-dot-v1`
2. **Mejorar CrossEncoder**: Usar modelo m√°s grande
3. **Validar mejoras**: Ejecutar evaluaci√≥n y comparar m√©tricas
4. **Considerar hybrid search**: Si las mejoras no son suficientes

## üìù NOTAS T√âCNICAS

### Normalizaci√≥n URL:
```python
def normalize_url(url: str) -> str:
    parsed = urlparse(url.strip())
    return urlunparse((parsed.scheme, parsed.netloc, parsed.path, '', '', ''))
```

### CrossEncoder Normalization:
- **Anterior**: Sigmoid
- **Actual**: Min-Max para scores m√°s interpretables
- **Resultado**: Scores en rango [0,1] m√°s altos

### Archivos de Debug Creados:
- `debug_metrics_zero.py` - An√°lisis paso a paso
- `verify_questions_links_match.py` - Verificaci√≥n de cobertura
- `check_chromadb_collections.py` - Estado de colecciones

---

**Fecha de √∫ltimo an√°lisis**: 2025-07-29
**Estado**: Problema identificado, soluciones definidas, listo para implementaci√≥n