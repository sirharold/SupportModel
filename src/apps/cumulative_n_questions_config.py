"""
P√°gina de Configuraci√≥n para An√°lisis Acumulativo de N Preguntas
Permite configurar evaluaciones y enviarlas a Google Colab
"""

import streamlit as st
import time
import json
import os
from typing import List, Dict, Any
from src.config.config import EMBEDDING_MODELS, GENERATIVE_MODELS, CHROMADB_COLLECTION_CONFIG

# Importar utilidades
from src.data.memory_utils import get_memory_usage, cleanup_memory
from src.services.storage.real_gdrive_integration import (
    show_gdrive_status, create_evaluation_config_in_drive,
    check_evaluation_status_in_drive, show_gdrive_authentication_instructions, 
    show_gdrive_debug_info
)
from src.apps.cumulative_metrics_page import display_current_colab_status


def load_questions_for_config(num_questions: int):
    """Cargar N preguntas aleatorias desde ChromaDB."""
    import random
    from src.services.storage.chromadb_utils import ChromaDBConfig, get_chromadb_client
    
    try:
        st.info(f"üîç Buscando {num_questions} preguntas aleatorias en ChromaDB...")
        
        # Conectar a ChromaDB usando la configuraci√≥n del sistema
        try:
            config_chroma = ChromaDBConfig.from_env()
            client = get_chromadb_client(config_chroma)
            st.success(f"‚úÖ Conectado a ChromaDB: {config_chroma.persist_directory}")
            
        except Exception as e:
            st.error(f"‚ùå Error conectando a ChromaDB: {e}")
            return []
        
        # Usar la colecci√≥n seleccionada desde session_state
        try:
            if 'selected_question_collection' not in st.session_state:
                st.error("‚ùå No se ha seleccionado una colecci√≥n de preguntas")
                return []
            
            collection_name = st.session_state.selected_question_collection
            collection = client.get_collection(name=collection_name)
            total_docs = collection.count()
            
            st.info(f"üìä Usando colecci√≥n '{collection_name}' con {total_docs:,} preguntas")
            
        except Exception as e:
            st.error(f"‚ùå Error accediendo a colecci√≥n '{collection_name}': {e}")
            return []
        
        # Obtener preguntas aleatorias
        try:
            # Obtener todos los IDs
            all_results = collection.get()
            all_ids = all_results['ids']
            
            if len(all_ids) < num_questions:
                st.warning(f"‚ö†Ô∏è Solo hay {len(all_ids)} preguntas disponibles, ajustando cantidad...")
                num_questions = len(all_ids)
            
            # Seleccionar IDs aleatorios
            random.seed(42)  # Seed fijo para reproducibilidad
            selected_ids = random.sample(all_ids, num_questions)
            
            # Obtener datos de las preguntas seleccionadas
            selected_results = collection.get(ids=selected_ids)
            
            st.success(f"‚úÖ Obtenidos {len(selected_results['ids'])} documentos aleatorios")
            
        except Exception as e:
            st.error(f"‚ùå Error obteniendo preguntas aleatorias: {e}")
            return []
        
        # Procesar preguntas al formato esperado
        questions_data = []
        
        for i in range(len(selected_results['ids'])):
            try:
                metadata = selected_results['metadatas'][i]
                document = selected_results['documents'][i]
                
                # Extraer informaci√≥n de la pregunta desde metadata o document
                question = {
                    'id': selected_results['ids'][i],
                    'title': metadata.get('title', metadata.get('question', document[:100] if document else 'Sin t√≠tulo')),
                    'question_content': metadata.get('question_content', metadata.get('question', document if document else '')),
                    'accepted_answer': metadata.get('accepted_answer', metadata.get('answer', 'Sin respuesta')),
                    'tags': metadata.get('tags', []),
                    'ms_links': metadata.get('ms_links', metadata.get('links', [])),
                    'metadata': metadata
                }
                
                # Si no hay ms_links, intentar extraer de la respuesta aceptada
                if not question['ms_links'] and question['accepted_answer']:
                    # Buscar enlaces de Microsoft en la respuesta
                    import re
                    ms_patterns = [
                        r'https?://(?:docs\.microsoft\.com|learn\.microsoft\.com)[^\s\)]+',
                        r'https?://[^\s]*microsoft[^\s]*',
                    ]
                    found_links = []
                    for pattern in ms_patterns:
                        found_links.extend(re.findall(pattern, question['accepted_answer'], re.IGNORECASE))
                    question['ms_links'] = list(set(found_links))  # Eliminar duplicados
                
                # Validar que tenga al menos t√≠tulo
                if question['title'] and question['title'] != 'Sin t√≠tulo':
                    questions_data.append(question)
                else:
                    st.warning(f"‚ö†Ô∏è Pregunta {i} sin t√≠tulo v√°lido - omitida")
                    
            except Exception as e:
                st.warning(f"‚ö†Ô∏è Error procesando pregunta {i}: {e}")
                continue
        
        if not questions_data:
            st.error("‚ùå No se pudieron procesar preguntas v√°lidas")
            return []
        
        st.success(f"‚úÖ {len(questions_data)} preguntas v√°lidas extra√≠das desde ChromaDB")
        
        # Mostrar muestra de preguntas
        with st.expander(f"üìã Muestra de preguntas seleccionadas", expanded=False):
            for i, q in enumerate(questions_data[:3]):
                st.write(f"**{i+1}.** {q['title'][:80]}...")
                st.write(f"   - Enlaces MS: {len(q['ms_links'])}")
                st.write(f"   - Tags: {', '.join(q['tags'][:3])}...")
            
            if len(questions_data) > 3:
                st.write(f"... y {len(questions_data)-3} preguntas m√°s")
        
        return questions_data
        
    except Exception as e:
        st.error(f"‚ùå Error general cargando preguntas: {e}")
        st.exception(e)
        return []


def show_cumulative_n_questions_config_page():
    """P√°gina principal para crear configuraciones de an√°lisis acumulativo de N preguntas."""
    
    st.title("‚öôÔ∏è Configuraci√≥n An√°lisis Acumulativo N Preguntas")
    st.markdown("""
    Esta p√°gina permite configurar evaluaciones de m√∫ltiples preguntas con comparaci√≥n pregunta vs respuesta 
    y enviarlas a Google Colab para procesamiento con GPU.
    """)
    
    # Verificar si ChromaDB est√° disponible usando la configuraci√≥n del sistema
    chromadb_available = False
    chromadb_path = None
    total_questions = 0
    available_question_collections = []
    
    try:
        import chromadb
        from chromadb.config import Settings
        from src.services.storage.chromadb_utils import ChromaDBConfig, get_chromadb_client
        
        # Usar la configuraci√≥n del sistema
        config_chroma = ChromaDBConfig.from_env()
        client = get_chromadb_client(config_chroma)
        chromadb_path = config_chroma.persist_directory
        
        # Buscar colecciones de preguntas disponibles
        try:
            collections = client.list_collections()
            collection_names = [c.name for c in collections]
            
            # Filtrar colecciones de preguntas
            for collection_name in collection_names:
                if collection_name.startswith('questions_'):
                    try:
                        collection = client.get_collection(name=collection_name)
                        count = collection.count()
                        if count > 0:
                            available_question_collections.append({
                                'name': collection_name,
                                'count': count,
                                'model': collection_name.replace('questions_', '')
                            })
                    except:
                        continue
            
            if available_question_collections:
                # Usar la colecci√≥n con m√°s preguntas o una espec√≠fica
                best_collection = max(available_question_collections, key=lambda x: x['count'])
                total_questions = best_collection['count']
                chromadb_available = True
                
                # Guardar en session_state para usar en load_questions_for_config
                st.session_state.selected_question_collection = best_collection['name']
                st.session_state.available_question_collections = available_question_collections
                
                st.success(f"‚úÖ ChromaDB disponible: {total_questions:,} preguntas en '{chromadb_path}'")
                st.info(f"üìã Usando colecci√≥n: '{best_collection['name']}' ({best_collection['model']})")
                
                # Mostrar otras colecciones disponibles
                if len(available_question_collections) > 1:
                    other_collections = [f"{c['name']} ({c['count']:,})" for c in available_question_collections]
                    st.info(f"üìã Colecciones de preguntas disponibles: {', '.join(other_collections)}")
            else:
                st.error("‚ùå No se encontraron colecciones de preguntas con datos")
                st.info(f"üìã Colecciones disponibles: {collection_names}")
                st.info("üí° Busca colecciones que empiecen con 'questions_' (ej: questions_mpnet, questions_e5large)")
                return
                
        except Exception as e:
            st.error(f"‚ùå Error listando colecciones: {e}")
            return
            
    except ImportError:
        st.error("‚ùå ChromaDB no est√° instalado. Ejecuta: pip install chromadb")
        return
    except Exception as e:
        st.error(f"‚ùå Error verificando ChromaDB: {e}")
        return
    
    # Mostrar informaci√≥n del flujo
    st.info(f"""
    üìã **Flujo de trabajo:**
    1. **Seleccionar N preguntas** aleatorias desde ChromaDB ({total_questions:,} disponibles)
    2. **Configurar evaluaci√≥n** en esta p√°gina
    3. **Enviar a Google Drive** para Colab (incluye las preguntas seleccionadas)
    4. **Ejecutar en Google Colab** con GPU usando embeddings pre-calculados
    5. **Ver resultados** en la p√°gina de resultados
    """)
    
    # Configuraci√≥n inicial
    col1, col2 = st.columns(2)
    
    with col1:
        st.subheader("üìä Configuraci√≥n de Datos")
        
        # Informaci√≥n sobre la fuente de datos
        st.info(f"üìä Las preguntas se seleccionan aleatoriamente desde ChromaDB ({total_questions:,} disponibles) para comparar recuperaci√≥n usando pregunta vs respuesta.")
        
        # N√∫mero de preguntas
        max_questions = min(total_questions, 3000)  # L√≠mite pr√°ctico
        num_questions = st.number_input(
            "N√∫mero de preguntas a evaluar:",
            min_value=10,
            max_value=max_questions,
            value=min(100, max_questions),
            step=10,
            help=f"Cantidad de preguntas aleatorias para analizar (m√°ximo {max_questions:,} de {total_questions:,} disponibles)"
        )
        
        # Top-k documentos
        top_k = st.number_input(
            "Documentos a recuperar (top-k):",
            min_value=5,
            max_value=50,
            value=10,
            step=1,
            help="Cantidad de documentos a recuperar para cada query"
        )
        
        # Usar reranking
        use_reranking = st.checkbox(
            "üîÑ Usar Reranking",
            value=True,
            help="Aplicar reranking a los documentos recuperados"
        )
    
    with col2:
        st.subheader("ü§ñ Configuraci√≥n de Modelos")
        
        # Selecci√≥n de modelo generativo
        generative_model = st.selectbox(
            "Modelo Generativo:",
            options=list(GENERATIVE_MODELS.keys()),
            index=list(GENERATIVE_MODELS.keys()).index("tinyllama-1.1b"),
            help="Modelo generativo para evaluaci√≥n de calidad y RAG metrics"
        )
        
        # Modelos de embedding a evaluar
        st.markdown("**Modelos de Embedding a evaluar:**")
        
        # Mapeo de nombres cortos
        MODEL_NAME_MAPPING = {
            "mpnet": "multi-qa-mpnet-base-dot-v1",
            "minilm": "all-MiniLM-L6-v2",
            "ada": "ada",
            "e5-large": "e5-large-v2"
        }
        
        selected_models = []
        for short_name, full_name in MODEL_NAME_MAPPING.items():
            if st.checkbox(f"‚úÖ {short_name.upper()}", value=True, key=f"model_{short_name}"):
                selected_models.append(short_name)
        
        if not selected_models:
            st.error("‚ö†Ô∏è Selecciona al menos un modelo de embedding")
    
    # Configuraci√≥n avanzada
    with st.expander("üîß Configuraci√≥n Avanzada", expanded=False):
        col3, col4 = st.columns(2)
        
        with col3:
            # Configuraci√≥n de m√©tricas
            st.markdown("**M√©tricas a calcular:**")
            calculate_traditional_metrics = st.checkbox("üìä M√©tricas IR Tradicionales", value=True, help="Jaccard, nDCG@10, Precision@5")
            calculate_rag_metrics = st.checkbox("üß† M√©tricas RAG", value=True, help="Faithfulness, Relevance, Correctness, Similarity")
            calculate_llm_quality = st.checkbox("‚≠ê Evaluaci√≥n LLM", value=True, help="Calidad de contenido evaluada por LLM")
        
        with col4:
            # Configuraci√≥n de procesamiento
            st.markdown("**Procesamiento:**")
            batch_size = st.number_input("Tama√±o de lote:", min_value=1, max_value=50, value=10, help="Preguntas por lote")
            parallel_processing = st.checkbox("‚ö° Procesamiento paralelo", value=True, help="Usar m√∫ltiples workers")
            save_intermediate = st.checkbox("üíæ Guardar resultados intermedios", value=True, help="Guardar progreso cada N preguntas")
    
    # Secci√≥n de Google Drive
    st.markdown("---")
    st.subheader("‚òÅÔ∏è Integraci√≥n con Google Drive")
    
    # Estado de Google Drive
    show_gdrive_status()
    
    # Configuraci√≥n de archivos
    st.markdown("**üìÅ Configuraci√≥n de archivos:**")
    col5, col6 = st.columns(2)
    
    with col5:
        config_filename = st.text_input(
            "Nombre del archivo de configuraci√≥n:",
            value=f"n_questions_config_{int(time.time())}.json",
            help="Nombre del archivo de configuraci√≥n a subir a Google Drive"
        )
    
    with col6:
        results_filename = st.text_input(
            "Nombre del archivo de resultados:",
            value=f"n_questions_results_{int(time.time())}.json",
            help="Nombre esperado del archivo de resultados"
        )
    
    # Bot√≥n para crear configuraci√≥n
    if st.button("üöÄ Crear y Subir Configuraci√≥n", type="primary", key="create_config"):
        if not selected_models:
            st.error("‚ùå Selecciona al menos un modelo de embedding")
            return
        
        # Crear configuraci√≥n
        config = create_n_questions_evaluation_config(
            num_questions=num_questions,
            top_k=top_k,
            use_reranking=use_reranking,
            generative_model=generative_model,
            selected_models=selected_models,
            calculate_traditional_metrics=calculate_traditional_metrics,
            calculate_rag_metrics=calculate_rag_metrics,
            calculate_llm_quality=calculate_llm_quality,
            batch_size=batch_size,
            parallel_processing=parallel_processing,
            save_intermediate=save_intermediate,
            results_filename=results_filename
        )
        
        # Mostrar configuraci√≥n creada en acorde√≥n colapsado
        with st.expander("üìã Ver Configuraci√≥n JSON (opcional)", expanded=False):
            st.json(config)
            st.caption("üí° Esta es la configuraci√≥n que se enviar√° a Google Colab. Solo necesitas revisarla si quieres verificar los detalles t√©cnicos.")
        
        # Subir a Google Drive usando funci√≥n unificada
        try:
            # Importar funci√≥n unificada
            from src.apps.cumulative_metrics_create import create_config_and_send_to_drive
            
            # Usar la funci√≥n unificada que maneja ambos tipos de configuraci√≥n
            create_config_and_send_to_drive(config)
            
            # Guardar en session state para seguimiento
            if 'n_questions_configs' not in st.session_state:
                st.session_state.n_questions_configs = []
            
            st.session_state.n_questions_configs.append({
                'config_file': config_filename,
                'results_file': results_filename,
                'created_at': time.strftime("%Y-%m-%d %H:%M:%S"),
                'status': 'configurado'
            })
            
            st.info(f"""
            üéØ **Pr√≥ximos pasos espec√≠ficos para N preguntas:**
            1. Ve a Google Colab y ejecuta el notebook de an√°lisis acumulativo N preguntas
            2. Usa el archivo de configuraci√≥n: `{config_filename}`
            3. Los resultados se guardar√°n como: `{results_filename}`
            4. Ve a la p√°gina de resultados para visualizar los datos
            """)
                    
        except Exception as e:
            st.error(f"‚ùå Error: {str(e)}")
    
    # Mostrar configuraciones recientes
    if 'n_questions_configs' in st.session_state and st.session_state.n_questions_configs:
        st.markdown("---")
        st.subheader("üìã Configuraciones Recientes")
        
        for i, config_info in enumerate(reversed(st.session_state.n_questions_configs[-5:])):  # Mostrar √∫ltimas 5
            with st.expander(f"üìÑ {config_info['config_file']} - {config_info['created_at']}"):
                col7, col8, col9 = st.columns(3)
                with col7:
                    st.write(f"**Estado:** {config_info['status']}")
                with col8:
                    st.write(f"**Config:** {config_info['config_file']}")
                with col9:
                    st.write(f"**Resultados:** {config_info['results_file']}")
    
    # Informaci√≥n adicional
    st.markdown("---")
    st.markdown("### üí° Tips y Recomendaciones")
    
    col10, col11 = st.columns(2)
    
    with col10:
        st.info("""
        **üéØ Para mejores resultados:**
        - Usa al menos 50 preguntas para estad√≠sticas confiables
        - Activa el reranking para mejor calidad
        - Incluye m√∫ltiples modelos para comparaci√≥n
        """)
    
    with col11:
        st.warning("""
        **‚ö†Ô∏è Consideraciones:**
        - M√°s preguntas = m√°s tiempo de procesamiento
        - RAG metrics requieren m√°s recursos
        - Guarda los nombres de archivos para referencia
        """)


def create_n_questions_evaluation_config(
    num_questions: int,
    top_k: int,
    use_reranking: bool,
    generative_model: str,
    selected_models: List[str],
    calculate_traditional_metrics: bool,
    calculate_rag_metrics: bool,
    calculate_llm_quality: bool,
    batch_size: int,
    parallel_processing: bool,
    save_intermediate: bool,
    results_filename: str
) -> Dict[str, Any]:
    """Crear configuraci√≥n para evaluaci√≥n de N preguntas."""
    
    # Mapeo de nombres de modelos
    MODEL_NAME_MAPPING = {
        "mpnet": "multi-qa-mpnet-base-dot-v1",
        "minilm": "all-MiniLM-L6-v2",
        "ada": "ada",
        "e5-large": "e5-large-v2"
    }
    
    # Cargar preguntas desde ChromaDB
    questions_data = load_questions_for_config(num_questions)
    if not questions_data:
        st.error("‚ùå No se pudieron cargar preguntas para la configuraci√≥n")
        return None
    
    config = {
        "evaluation_type": "n_questions_cumulative_analysis",
        "version": "1.0",
        "created_at": time.strftime("%Y-%m-%d %H:%M:%S"),
        
        # Configuraci√≥n de datos
        "data_config": {
            "num_questions": len(questions_data),  # Usar cantidad real de preguntas cargadas
            "top_k": top_k,
            "use_reranking": use_reranking,
            "data_source": "chromadb_random_selection"
        },
        
        # Incluir las preguntas en la configuraci√≥n
        "questions_data": questions_data,
        
        # Configuraci√≥n de modelos
        "model_config": {
            "generative_model": generative_model,
            "embedding_models": {
                short_name: MODEL_NAME_MAPPING[short_name] 
                for short_name in selected_models
            }
        },
        
        # Configuraci√≥n de m√©tricas
        "metrics_config": {
            "calculate_traditional_metrics": calculate_traditional_metrics,  # Jaccard, nDCG, Precision
            "calculate_rag_metrics": calculate_rag_metrics,  # RAG metrics
            "calculate_llm_quality": calculate_llm_quality,  # LLM quality evaluation
            "metrics_included": []
        },
        
        # Configuraci√≥n de procesamiento
        "processing_config": {
            "batch_size": batch_size,
            "parallel_processing": parallel_processing,
            "save_intermediate": save_intermediate,
            "max_retries": 3,
            "timeout_per_question": 120  # segundos
        },
        
        # Configuraci√≥n de salida
        "output_config": {
            "results_filename": results_filename,
            "include_individual_results": True,
            "include_consolidated_metrics": True,
            "export_formats": ["json", "csv"]
        },
        
        # Metadatos
        "metadata": {
            "description": f"An√°lisis acumulativo de {len(questions_data)} preguntas con comparaci√≥n pregunta vs respuesta",
            "expected_duration_minutes": estimate_processing_time(len(questions_data), len(selected_models), use_reranking),
            "models_count": len(selected_models),
            "total_comparisons": len(questions_data) * len(selected_models),
            "questions_source": "chromadb_random_selection"
        }
    }
    
    # Agregar m√©tricas espec√≠ficas seg√∫n configuraci√≥n
    if calculate_traditional_metrics:
        config["metrics_config"]["metrics_included"].extend([
            "jaccard_similarity", "ndcg_at_10", "precision_at_5", "common_docs"
        ])
    
    if calculate_rag_metrics:
        config["metrics_config"]["metrics_included"].extend([
            "faithfulness", "answer_relevance", "answer_correctness", "answer_similarity"
        ])
    
    if calculate_llm_quality:
        config["metrics_config"]["metrics_included"].extend([
            "question_quality", "answer_quality", "avg_quality"
        ])
    
    # Agregar score compuesto siempre
    config["metrics_config"]["metrics_included"].append("composite_score")
    
    return config


def estimate_processing_time(actual_questions: int, num_models: int, use_reranking: bool) -> int:
    """Estimar tiempo de procesamiento en minutos."""
    
    # Tiempo base por pregunta y modelo (en segundos)
    base_time_per_question_model = 2.0
    
    # Factor de reranking
    reranking_factor = 1.5 if use_reranking else 1.0
    
    # Factor de RAG metrics (m√°s lento)
    rag_factor = 1.8
    
    # C√°lculo total
    total_seconds = (
        actual_questions * 
        num_models * 
        base_time_per_question_model * 
        reranking_factor * 
        rag_factor
    )
    
    # Convertir a minutos y redondear hacia arriba
    total_minutes = int(total_seconds / 60) + 1
    
    return max(total_minutes, 5)  # M√≠nimo 5 minutos


if __name__ == "__main__":
    show_cumulative_n_questions_config_page()