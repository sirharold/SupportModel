"""
P√°gina de Configuraci√≥n para An√°lisis Acumulativo de N Preguntas
Permite configurar evaluaciones y enviarlas a Google Colab
"""

import streamlit as st
import time
import json
import os
from typing import List, Dict, Any
from src.config.config import EMBEDDING_MODELS, GENERATIVE_MODELS, CHROMADB_COLLECTION_CONFIG

# Importar utilidades
from src.data.memory_utils import get_memory_usage, cleanup_memory
from src.services.storage.real_gdrive_integration import (
    show_gdrive_status, create_evaluation_config_in_drive,
    check_evaluation_status_in_drive, show_gdrive_authentication_instructions, 
    show_gdrive_debug_info
)
from src.apps.cumulative_metrics_page import display_current_colab_status


def load_questions_for_config(num_questions: int):
    """Cargar N preguntas aleatorias desde ChromaDB que tengan links presentes en la colecci√≥n de documentos."""
    import random
    import re
    from urllib.parse import urlparse, urlunparse
    from src.services.storage.chromadb_utils import ChromaDBConfig, get_chromadb_client
    
    try:
        st.info(f"üîç Buscando {num_questions} preguntas con links v√°lidos en ChromaDB...")
        
        # Conectar a ChromaDB usando la configuraci√≥n del sistema
        try:
            config_chroma = ChromaDBConfig.from_env()
            client = get_chromadb_client(config_chroma)
            st.success(f"‚úÖ Conectado a ChromaDB: {config_chroma.persist_directory}")
            
        except Exception as e:
            st.error(f"‚ùå Error conectando a ChromaDB: {e}")
            return []
        
        # Funci√≥n para normalizar URLs (quita query params y anchors)
        def normalize_link(url):
            try:
                parsed = urlparse(url)
                normalized = urlunparse((parsed.scheme, parsed.netloc, parsed.path, '', '', ''))
                return normalized.rstrip('/')
            except:
                return None
        
        # Obtener links de la colecci√≥n de documentos
        try:
            if 'selected_question_collection' not in st.session_state:
                st.error("‚ùå No se ha seleccionado una colecci√≥n de preguntas")
                return []
            
            # Determinar colecci√≥n de documentos basada en la colecci√≥n de preguntas
            collection_name = st.session_state.selected_question_collection
            docs_collection_name = collection_name.replace('questions_', 'docs_')
            
            st.info(f"üìä Obteniendo links de colecci√≥n '{docs_collection_name}'...")
            docs_collection = client.get_collection(name=docs_collection_name)
            docs_metadatas = docs_collection.get(include=["metadatas"], limit=200000)["metadatas"]
            
            doc_links = set()
            for meta in docs_metadatas:
                link = meta.get("link")
                if link:
                    normalized = normalize_link(link)
                    if normalized:
                        doc_links.add(normalized)
            
            st.success(f"‚úÖ Obtenidos {len(doc_links):,} links √∫nicos de documentos")
            
        except Exception as e:
            st.error(f"‚ùå Error accediendo a colecci√≥n de documentos '{docs_collection_name}': {e}")
            return []
        
        # Obtener y filtrar preguntas con links v√°lidos
        try:
            questions_collection = client.get_collection(name=collection_name)
            total_questions = questions_collection.count()
            
            st.info(f"üìä Filtrando preguntas de '{collection_name}' ({total_questions:,} total)...")
            
            # Obtener todas las preguntas para filtrar
            all_questions = questions_collection.get(include=["metadatas"], limit=15000)["metadatas"]
            matched_questions = []
            
            progress_bar = st.progress(0)
            for i, meta in enumerate(all_questions):
                if i % 100 == 0:
                    progress_bar.progress(i / len(all_questions))
                
                accepted_answer = meta.get("accepted_answer", "")
                if "http" in accepted_answer:
                    urls = re.findall(r'https?://[^\s\)\]]+', accepted_answer)
                    for url in urls:
                        norm_url = normalize_link(url)
                        if norm_url and norm_url in doc_links:
                            matched_questions.append(meta)
                            break  # basta con una coincidencia por pregunta
            
            progress_bar.progress(1.0)
            st.success(f"‚úÖ Encontradas {len(matched_questions):,} preguntas con links v√°lidos")
            
            if len(matched_questions) < num_questions:
                st.warning(f"‚ö†Ô∏è Solo hay {len(matched_questions)} preguntas con links v√°lidos, ajustando cantidad...")
                num_questions = len(matched_questions)
            
            # Seleccionar aleatoriamente del subconjunto filtrado
            random.seed(42)  # Seed fijo para reproducibilidad
            selected_questions = random.sample(matched_questions, num_questions)
            
        except Exception as e:
            st.error(f"‚ùå Error filtrando preguntas: {e}")
            return []
        
        # Procesar preguntas al formato esperado
        questions_data = []
        
        for i, metadata in enumerate(selected_questions):
            try:
                # Extraer informaci√≥n de la pregunta desde metadata
                question = {
                    'id': f"filtered_q_{i}",  # ID √∫nico para preguntas filtradas
                    'title': metadata.get('title', metadata.get('question', 'Sin t√≠tulo')),
                    'question_content': metadata.get('question_content', metadata.get('question', '')),
                    'accepted_answer': metadata.get('accepted_answer', metadata.get('answer', 'Sin respuesta')),
                    'tags': metadata.get('tags', []),
                    'ms_links': metadata.get('ms_links', metadata.get('links', [])),
                    'metadata': metadata
                }
                
                # Extraer links v√°lidos de la respuesta aceptada (ya sabemos que tiene al menos uno)
                if question['accepted_answer'] and "http" in question['accepted_answer']:
                    urls = re.findall(r'https?://[^\s\)\]]+', question['accepted_answer'])
                    valid_links = []
                    for url in urls:
                        norm_url = normalize_link(url)
                        if norm_url and norm_url in doc_links:
                            valid_links.append(url)
                    question['ms_links'] = list(set(valid_links))  # Eliminar duplicados
                
                # Validar que tenga al menos t√≠tulo
                if question['title'] and question['title'] != 'Sin t√≠tulo':
                    questions_data.append(question)
                else:
                    st.warning(f"‚ö†Ô∏è Pregunta {i} sin t√≠tulo v√°lido - omitida")
                    
            except Exception as e:
                st.warning(f"‚ö†Ô∏è Error procesando pregunta {i}: {e}")
                continue
        
        if not questions_data:
            st.error("‚ùå No se pudieron procesar preguntas v√°lidas")
            return []
        
        st.success(f"‚úÖ {len(questions_data)} preguntas v√°lidas con links verificados extra√≠das desde ChromaDB")
        
        # Mostrar muestra de preguntas
        with st.expander(f"üìã Muestra de preguntas seleccionadas (con links v√°lidos)", expanded=False):
            for i, q in enumerate(questions_data[:3]):
                st.write(f"**{i+1}.** {q['title'][:80]}...")
                st.write(f"   - Enlaces v√°lidos: {len(q['ms_links'])}")
                st.write(f"   - Tags: {', '.join(q['tags'][:3]) if q['tags'] else 'Sin tags'}...")
                # Mostrar un link de ejemplo para verificaci√≥n
                if q['ms_links']:
                    st.write(f"     Ejemplo: {q['ms_links'][0][:60]}...")
            
            if len(questions_data) > 3:
                st.write(f"... y {len(questions_data)-3} preguntas m√°s")
        
        return questions_data
        
    except Exception as e:
        st.error(f"‚ùå Error general cargando preguntas: {e}")
        st.exception(e)
        return []


def show_cumulative_n_questions_config_page():
    """P√°gina principal para crear configuraciones de an√°lisis acumulativo de N preguntas."""
    
    st.title("‚öôÔ∏è Configuraci√≥n An√°lisis Acumulativo N Preguntas")
    st.markdown("""
    Esta p√°gina permite configurar evaluaciones de m√∫ltiples preguntas con comparaci√≥n pregunta vs respuesta 
    y enviarlas a Google Colab para procesamiento con GPU.
    """)
    
    # Verificar si ChromaDB est√° disponible usando la configuraci√≥n del sistema
    chromadb_available = False
    chromadb_path = None
    total_questions = 0
    available_question_collections = []
    
    try:
        import chromadb
        from chromadb.config import Settings
        from src.services.storage.chromadb_utils import ChromaDBConfig, get_chromadb_client
        
        # Usar la configuraci√≥n del sistema
        config_chroma = ChromaDBConfig.from_env()
        client = get_chromadb_client(config_chroma)
        chromadb_path = config_chroma.persist_directory
        
        # Buscar colecciones de preguntas disponibles
        try:
            collections = client.list_collections()
            collection_names = [c.name for c in collections]
            
            # Filtrar colecciones de preguntas
            for collection_name in collection_names:
                if collection_name.startswith('questions_'):
                    try:
                        collection = client.get_collection(name=collection_name)
                        count = collection.count()
                        if count > 0:
                            available_question_collections.append({
                                'name': collection_name,
                                'count': count,
                                'model': collection_name.replace('questions_', '')
                            })
                    except:
                        continue
            
            if available_question_collections:
                # Usar la colecci√≥n con m√°s preguntas o una espec√≠fica
                best_collection = max(available_question_collections, key=lambda x: x['count'])
                total_questions = best_collection['count']
                chromadb_available = True
                
                # Guardar en session_state para usar en load_questions_for_config
                st.session_state.selected_question_collection = best_collection['name']
                st.session_state.available_question_collections = available_question_collections
                
                st.success(f"‚úÖ ChromaDB disponible: {total_questions:,} preguntas en '{chromadb_path}'")
                st.info(f"üìã Usando colecci√≥n: '{best_collection['name']}' ({best_collection['model']})")
                
                # Mostrar otras colecciones disponibles
                if len(available_question_collections) > 1:
                    other_collections = [f"{c['name']} ({c['count']:,})" for c in available_question_collections]
                    st.info(f"üìã Colecciones de preguntas disponibles: {', '.join(other_collections)}")
            else:
                st.error("‚ùå No se encontraron colecciones de preguntas con datos")
                st.info(f"üìã Colecciones disponibles: {collection_names}")
                st.info("üí° Busca colecciones que empiecen con 'questions_' (ej: questions_mpnet, questions_e5large)")
                return
                
        except Exception as e:
            st.error(f"‚ùå Error listando colecciones: {e}")
            return
            
    except ImportError:
        st.error("‚ùå ChromaDB no est√° instalado. Ejecuta: pip install chromadb")
        return
    except Exception as e:
        st.error(f"‚ùå Error verificando ChromaDB: {e}")
        return
    
    # Mostrar informaci√≥n del flujo
    st.info(f"""
    üìã **Flujo de trabajo:**
    1. **Filtrar preguntas** con links v√°lidos desde ChromaDB ({total_questions:,} total ‚Üí ~2,067 con links v√°lidos)
    2. **Seleccionar N preguntas** aleatoriamente del subconjunto filtrado
    3. **Configurar evaluaci√≥n** en esta p√°gina
    4. **Enviar a Google Drive** para Colab (incluye las preguntas seleccionadas)
    5. **Ejecutar en Google Colab** con GPU usando embeddings pre-calculados
    6. **Ver resultados** en la p√°gina de resultados
    """)
    
    # Configuraci√≥n inicial
    col1, col2 = st.columns(2)
    
    with col1:
        st.subheader("üìä Configuraci√≥n de Datos")
        
        # Informaci√≥n sobre la fuente de datos
        st.info(f"üìä Las preguntas se seleccionan aleatoriamente desde ChromaDB ({total_questions:,} total) pero **solo se usan preguntas con links presentes en la colecci√≥n de documentos** (~2,067 estimadas) para comparar recuperaci√≥n usando pregunta vs respuesta.")
        
        # N√∫mero de preguntas
        max_questions = min(2067, 3000)  # L√≠mite basado en preguntas con links v√°lidos
        num_questions = st.number_input(
            "N√∫mero de preguntas a evaluar:",
            min_value=10,
            max_value=max_questions,
            value=min(100, max_questions),
            step=10,
            help=f"Cantidad de preguntas aleatorias para analizar (m√°ximo {max_questions:,} de ~2,067 con links v√°lidos)"
        )
        
        # Top-k documentos
        top_k = st.number_input(
            "Documentos a recuperar (top-k):",
            min_value=5,
            max_value=50,
            value=10,
            step=1,
            help="Cantidad de documentos a recuperar para cada query"
        )
        
        # M√©todo de reranking
        reranking_method = st.selectbox(
            "üîÑ M√©todo de Reranking:",
            options=["crossencoder", "standard", "none"],
            index=0,  # CrossEncoder por defecto
            format_func=lambda x: {
                "crossencoder": "üß† CrossEncoder (Recomendado)",
                "standard": "üìä Reranking Est√°ndar",
                "none": "‚ùå Sin Reranking"
            }[x],
            help="M√©todo de reranking: CrossEncoder usa ms-marco-MiniLM-L-6-v2 para mejor calidad"
        )
    
    with col2:
        st.subheader("ü§ñ Configuraci√≥n de Modelos")
        
        # Selecci√≥n de modelo generativo
        generative_model = st.selectbox(
            "Modelo Generativo:",
            options=list(GENERATIVE_MODELS.keys()),
            index=list(GENERATIVE_MODELS.keys()).index("tinyllama-1.1b"),
            help="Modelo generativo para evaluaci√≥n de calidad y RAG metrics"
        )
        
        # Modelos de embedding a evaluar
        st.markdown("**Modelos de Embedding a evaluar:**")
        
        # Mapeo de nombres cortos
        MODEL_NAME_MAPPING = {
            "mpnet": "multi-qa-mpnet-base-dot-v1",
            "minilm": "all-MiniLM-L6-v2",
            "ada": "ada",
            "e5-large": "e5-large-v2"
        }
        
        selected_models = []
        for short_name, full_name in MODEL_NAME_MAPPING.items():
            if st.checkbox(f"‚úÖ {short_name.upper()}", value=True, key=f"model_{short_name}"):
                selected_models.append(short_name)
        
        if not selected_models:
            st.error("‚ö†Ô∏è Selecciona al menos un modelo de embedding")
    
    # Configuraci√≥n avanzada
    with st.expander("üîß Configuraci√≥n Avanzada", expanded=False):
        col3, col4 = st.columns(2)
        
        with col3:
            # Configuraci√≥n de m√©tricas
            st.markdown("**M√©tricas a calcular:**")
            calculate_traditional_metrics = st.checkbox("üìä M√©tricas IR Tradicionales", value=True, help="Jaccard, nDCG@10, Precision@5")
            calculate_rag_metrics = st.checkbox("üß† M√©tricas RAG", value=True, help="Faithfulness, Relevance, Correctness, Similarity")
            calculate_llm_quality = st.checkbox("‚≠ê Evaluaci√≥n LLM", value=True, help="Calidad de contenido evaluada por LLM")
        
        with col4:
            # Configuraci√≥n de procesamiento
            st.markdown("**Procesamiento:**")
            batch_size = st.number_input("Tama√±o de lote:", min_value=1, max_value=50, value=10, help="Preguntas por lote")
            parallel_processing = st.checkbox("‚ö° Procesamiento paralelo", value=True, help="Usar m√∫ltiples workers")
            save_intermediate = st.checkbox("üíæ Guardar resultados intermedios", value=True, help="Guardar progreso cada N preguntas")
    
    # Secci√≥n de Google Drive
    st.markdown("---")
    st.subheader("‚òÅÔ∏è Integraci√≥n con Google Drive")
    
    # Estado de Google Drive
    show_gdrive_status()
    
    # Configuraci√≥n de archivos
    st.markdown("**üìÅ Configuraci√≥n de archivos:**")
    col5, col6 = st.columns(2)
    
    with col5:
        config_filename = st.text_input(
            "Nombre del archivo de configuraci√≥n:",
            value=f"n_questions_config_{int(time.time())}.json",
            help="Nombre del archivo de configuraci√≥n a subir a Google Drive"
        )
    
    with col6:
        results_filename = st.text_input(
            "Nombre del archivo de resultados:",
            value=f"n_questions_results_{int(time.time())}.json",
            help="Nombre esperado del archivo de resultados"
        )
    
    # Bot√≥n para crear configuraci√≥n
    if st.button("üöÄ Crear y Subir Configuraci√≥n", type="primary", key="create_config"):
        if not selected_models:
            st.error("‚ùå Selecciona al menos un modelo de embedding")
            return
        
        # Crear configuraci√≥n
        config = create_n_questions_evaluation_config(
            num_questions=num_questions,
            top_k=top_k,
            reranking_method=reranking_method,
            generative_model=generative_model,
            selected_models=selected_models,
            calculate_traditional_metrics=calculate_traditional_metrics,
            calculate_rag_metrics=calculate_rag_metrics,
            calculate_llm_quality=calculate_llm_quality,
            batch_size=batch_size,
            parallel_processing=parallel_processing,
            save_intermediate=save_intermediate,
            results_filename=results_filename
        )
        
        # Mostrar configuraci√≥n creada en acorde√≥n colapsado
        with st.expander("üìã Ver Configuraci√≥n JSON (opcional)", expanded=False):
            st.json(config)
            st.caption("üí° Esta es la configuraci√≥n que se enviar√° a Google Colab. Solo necesitas revisarla si quieres verificar los detalles t√©cnicos.")
        
        # Subir a Google Drive usando funci√≥n unificada
        try:
            # Importar funci√≥n unificada
            from src.apps.cumulative_metrics_create import create_config_and_send_to_drive
            
            # Usar la funci√≥n unificada que maneja ambos tipos de configuraci√≥n
            create_config_and_send_to_drive(config)
            
            # Guardar en session state para seguimiento
            if 'n_questions_configs' not in st.session_state:
                st.session_state.n_questions_configs = []
            
            st.session_state.n_questions_configs.append({
                'config_file': config_filename,
                'results_file': results_filename,
                'created_at': time.strftime("%Y-%m-%d %H:%M:%S"),
                'status': 'configurado'
            })
            
            st.info(f"""
            üéØ **Pr√≥ximos pasos espec√≠ficos para N preguntas:**
            1. Ve a Google Colab y ejecuta el notebook de an√°lisis acumulativo N preguntas
            2. Usa el archivo de configuraci√≥n: `{config_filename}`
            3. Los resultados se guardar√°n como: `{results_filename}`
            4. Ve a la p√°gina de resultados para visualizar los datos
            """)
                    
        except Exception as e:
            st.error(f"‚ùå Error: {str(e)}")
    
    # Mostrar configuraciones recientes
    if 'n_questions_configs' in st.session_state and st.session_state.n_questions_configs:
        st.markdown("---")
        st.subheader("üìã Configuraciones Recientes")
        
        for i, config_info in enumerate(reversed(st.session_state.n_questions_configs[-5:])):  # Mostrar √∫ltimas 5
            with st.expander(f"üìÑ {config_info['config_file']} - {config_info['created_at']}"):
                col7, col8, col9 = st.columns(3)
                with col7:
                    st.write(f"**Estado:** {config_info['status']}")
                with col8:
                    st.write(f"**Config:** {config_info['config_file']}")
                with col9:
                    st.write(f"**Resultados:** {config_info['results_file']}")
    
    # Informaci√≥n adicional
    st.markdown("---")
    st.markdown("### üí° Tips y Recomendaciones")
    
    col10, col11 = st.columns(2)
    
    with col10:
        st.info("""
        **üéØ Para mejores resultados:**
        - Usa al menos 50 preguntas para estad√≠sticas confiables
        - Activa el reranking para mejor calidad
        - Incluye m√∫ltiples modelos para comparaci√≥n
        """)
    
    with col11:
        st.warning("""
        **‚ö†Ô∏è Consideraciones:**
        - M√°s preguntas = m√°s tiempo de procesamiento
        - RAG metrics requieren m√°s recursos
        - Guarda los nombres de archivos para referencia
        """)


def create_n_questions_evaluation_config(
    num_questions: int,
    top_k: int,
    reranking_method: str,
    generative_model: str,
    selected_models: List[str],
    calculate_traditional_metrics: bool,
    calculate_rag_metrics: bool,
    calculate_llm_quality: bool,
    batch_size: int,
    parallel_processing: bool,
    save_intermediate: bool,
    results_filename: str
) -> Dict[str, Any]:
    """Crear configuraci√≥n para evaluaci√≥n de N preguntas."""
    
    # Mapeo de nombres de modelos
    MODEL_NAME_MAPPING = {
        "mpnet": "multi-qa-mpnet-base-dot-v1",
        "minilm": "all-MiniLM-L6-v2",
        "ada": "ada",
        "e5-large": "e5-large-v2"
    }
    
    # Cargar preguntas desde ChromaDB
    questions_data = load_questions_for_config(num_questions)
    if not questions_data:
        st.error("‚ùå No se pudieron cargar preguntas para la configuraci√≥n")
        return None
    
    config = {
        "evaluation_type": "n_questions_cumulative_analysis",
        "version": "1.0",
        "created_at": time.strftime("%Y-%m-%d %H:%M:%S"),
        
        # Configuraci√≥n de datos
        "data_config": {
            "num_questions": len(questions_data),  # Usar cantidad real de preguntas cargadas
            "top_k": top_k,
            "reranking_method": reranking_method,
            "use_reranking": reranking_method != "none",  # Compatibilidad hacia atr√°s
            "data_source": "chromadb_random_selection"
        },
        
        # Incluir las preguntas en la configuraci√≥n
        "questions_data": questions_data,
        
        # Configuraci√≥n de modelos
        "model_config": {
            "generative_model": generative_model,
            "embedding_models": {
                short_name: MODEL_NAME_MAPPING[short_name] 
                for short_name in selected_models
            }
        },
        
        # Configuraci√≥n de m√©tricas
        "metrics_config": {
            "calculate_traditional_metrics": calculate_traditional_metrics,  # Jaccard, nDCG, Precision
            "calculate_rag_metrics": calculate_rag_metrics,  # RAG metrics
            "calculate_llm_quality": calculate_llm_quality,  # LLM quality evaluation
            "metrics_included": []
        },
        
        # Configuraci√≥n de procesamiento
        "processing_config": {
            "batch_size": batch_size,
            "parallel_processing": parallel_processing,
            "save_intermediate": save_intermediate,
            "max_retries": 3,
            "timeout_per_question": 120  # segundos
        },
        
        # Configuraci√≥n de salida
        "output_config": {
            "results_filename": results_filename,
            "include_individual_results": True,
            "include_consolidated_metrics": True,
            "export_formats": ["json", "csv"]
        },
        
        # Metadatos
        "metadata": {
            "description": f"An√°lisis acumulativo de {len(questions_data)} preguntas con comparaci√≥n pregunta vs respuesta",
            "expected_duration_minutes": estimate_processing_time(len(questions_data), len(selected_models), reranking_method != "none"),
            "models_count": len(selected_models),
            "total_comparisons": len(questions_data) * len(selected_models),
            "questions_source": "chromadb_random_selection"
        }
    }
    
    # Agregar m√©tricas espec√≠ficas seg√∫n configuraci√≥n
    if calculate_traditional_metrics:
        config["metrics_config"]["metrics_included"].extend([
            "jaccard_similarity", "ndcg_at_10", "precision_at_5", "common_docs"
        ])
    
    if calculate_rag_metrics:
        config["metrics_config"]["metrics_included"].extend([
            "faithfulness", "answer_relevance", "answer_correctness", "answer_similarity"
        ])
    
    if calculate_llm_quality:
        config["metrics_config"]["metrics_included"].extend([
            "question_quality", "answer_quality", "avg_quality"
        ])
    
    # Agregar score compuesto siempre
    config["metrics_config"]["metrics_included"].append("composite_score")
    
    return config


def estimate_processing_time(actual_questions: int, num_models: int, use_reranking: bool) -> int:
    """Estimar tiempo de procesamiento en minutos."""
    
    # Tiempo base por pregunta y modelo (en segundos)
    base_time_per_question_model = 2.0
    
    # Factor de reranking
    reranking_factor = 1.5 if use_reranking else 1.0
    
    # Factor de RAG metrics (m√°s lento)
    rag_factor = 1.8
    
    # C√°lculo total
    total_seconds = (
        actual_questions * 
        num_models * 
        base_time_per_question_model * 
        reranking_factor * 
        rag_factor
    )
    
    # Convertir a minutos y redondear hacia arriba
    total_minutes = int(total_seconds / 60) + 1
    
    return max(total_minutes, 5)  # M√≠nimo 5 minutos


if __name__ == "__main__":
    show_cumulative_n_questions_config_page()