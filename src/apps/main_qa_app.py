import multiprocessing
import os
import time
import pandas as pd

# Set multiprocessing start method to 'spawn' and disable tokenizers parallelism
# This must be done before any other imports that might initialize multiprocessing
if multiprocessing.get_start_method(allow_none=True) != 'spawn':
    multiprocessing.set_start_method('spawn', force=True)
os.environ["TOKENIZERS_PARALLELISM"] = "false"

import streamlit as st
import plotly.express as px
import json
import numpy as np
from src.core.qa_pipeline import answer_question_documents_only, answer_question_with_rag
from src.services.auth.clients import initialize_clients
from src.services.local_models import preload_tinyllama_model
from src.apps.comparison_page import show_comparison_page
# from src.apps.batch_queries_page import show_batch_queries_page  # Module doesn't exist
from src.apps.data_analysis_page import show_data_analysis_page
from src.apps.cumulative_metrics_create import show_cumulative_metrics_create_page
from src.apps.cumulative_metrics_results import show_cumulative_metrics_results_page
from src.apps.cumulative_metrics_results_matplotlib import show_cumulative_metrics_results_page as show_cumulative_metrics_results_matplotlib_page
from src.apps.question_answer_comparison import show_question_answer_comparison_page
from src.apps.cumulative_comparison import show_cumulative_comparison_page
# from src.apps.cumulative_n_questions_config import show_cumulative_n_questions_config_page  # Module doesn't exist
# from src.apps.cumulative_n_questions_results import show_cumulative_n_questions_results_page  # Module doesn't exist
from src.apps.chapter_4_visualizations import main as show_chapter_4_visualizations
from src.apps.chapter_7_figures import main as show_chapter_7_figures
from src.config.config import EMBEDDING_MODELS, DEFAULT_EMBEDDING_MODEL, CHROMADB_COLLECTION_CONFIG, GENERATIVE_MODELS, DEFAULT_GENERATIVE_MODEL, GENERATIVE_MODEL_DESCRIPTIONS

def _sanitize_json_string(json_string: str) -> str:
    """Sanitiza una cadena JSON eliminando caracteres de control invÃ¡lidos."""
    import re
    
    # MÃ©todo mÃ¡s robusto: usar regex para remover todos los caracteres de control
    # ASCII control characters (0-31) except \t(9), \n(10), \r(13)
    sanitized = re.sub(r'[\x00-\x08\x0B\x0C\x0E-\x1F\x7F]', '', json_string)
    
    # TambiÃ©n remover caracteres Unicode problemÃ¡ticos
    sanitized = re.sub(r'[\u0080-\u009F]', '', sanitized)  # C1 control characters
    sanitized = re.sub(r'[\u2028\u2029]', '', sanitized)   # Line/Paragraph separators
    
    # Remove any remaining non-printable characters
    sanitized = re.sub(r'[^\x20-\x7E\t\n\r]', '', sanitized)
    
    return sanitized

# ConfiguraciÃ³n de pÃ¡gina
st.set_page_config(
    page_title="Azure Q&A Expert System", 
    layout="wide",
    page_icon="â˜ï¸"
)

# CSS personalizado
st.markdown("""
<style>
    .main-header {
        background: linear-gradient(90deg, #0078d4 0%, #00bcf2 100%);
        padding: 1rem;
        border-radius: 10px;
        color: white;
        text-align: center;
        margin-bottom: 2rem;
    }
    .metric-card {
        background: #f8f9fa;
        padding: 1rem;
        border-radius: 8px;
        border-left: 4px solid #0078d4;
    }
    .doc-card {
        background: white;
        padding: 1rem;
        border-radius: 8px;
        border: 1px solid #e1e5e9;
        margin-bottom: 1rem;
        box-shadow: 0 2px 4px rgba(0,0,0,0.1);
    }
    .confidence-high { border-left: 4px solid #28a745; }
    .confidence-medium { border-left: 4px solid #ffc107; }
    .confidence-low { border-left: 4px solid #dc3545; }
</style>
""", unsafe_allow_html=True)

# Cache del modelo TinyLlama para mejor performance
@st.cache_resource
def get_cached_tinyllama_client():
    """Get cached TinyLlama client for better performance."""
    from src.services.local_models import get_tinyllama_client
    return get_tinyllama_client()

@st.cache_resource
def preload_model_on_startup():
    """Preload model when app starts."""
    try:
        success = preload_tinyllama_model()
        return success
    except Exception as e:
        st.error(f"âŒ Error precargando modelo: {e}")
        return False

# Precargar modelo al iniciar la aplicaciÃ³n
with st.spinner("ğŸ”„ Inicializando modelo TinyLlama..."):
    model_preloaded = preload_model_on_startup()

# Header principal
st.markdown("""
<div class="main-header">
    <h1>â˜ï¸ Azure Q&A Expert System</h1>
    <p>Encuentra documentaciÃ³n oficial de Microsoft Azure para tus preguntas tÃ©cnicas</p>
</div>
""", unsafe_allow_html=True)

# Sidebar para navegaciÃ³n y configuraciÃ³n
st.sidebar.title("ğŸ§­ NavegaciÃ³n")
page = st.sidebar.radio(
    "Selecciona una pÃ¡gina:",
    [
        "ğŸ” BÃºsqueda Individual",
        "ğŸ“Š Visualizaciones CapÃ­tulo 4",
        "ğŸ“Š Figuras CapÃ­tulo 7",
        "ğŸ“ˆ AnÃ¡lisis de Datos",
        "ğŸ”¬ ComparaciÃ³n de Modelos",
        "ğŸ”„ Comparador Pregunta vs Respuesta",
        "ğŸ“Š AnÃ¡lisis Acumulativo N Preguntas",
        "âš™ï¸ ConfiguraciÃ³n MÃ©tricas Acumulativas",
        "ğŸ“ˆ Resultados MÃ©tricas Acumulativas",
        "ğŸ“Š Resultados Matplotlib",
    ],
    index=0
)
st.sidebar.markdown("---")

st.sidebar.title("âš™ï¸ ConfiguraciÃ³n")

# SelecciÃ³n de modelo de embedding
model_name = st.sidebar.selectbox(
    "Selecciona el modelo de embedding:",
    options=list(EMBEDDING_MODELS.keys()),
    index=list(EMBEDDING_MODELS.keys()).index(DEFAULT_EMBEDDING_MODEL)
)

# SelecciÃ³n de modelo generativo
generative_model_name = st.sidebar.selectbox(
    "Selecciona el modelo generativo:",
    options=list(GENERATIVE_MODELS.keys()),
    index=list(GENERATIVE_MODELS.keys()).index(DEFAULT_GENERATIVE_MODEL),
    help="TinyLlama es gratuito y funciona sin configuraciÃ³n. Mistral requiere autorizaciÃ³n en Hugging Face."
)

# Mostrar informaciÃ³n del modelo seleccionado
if generative_model_name in GENERATIVE_MODEL_DESCRIPTIONS:
    model_info = GENERATIVE_MODEL_DESCRIPTIONS[generative_model_name]
    st.sidebar.success(f"ğŸ¯ **{model_info['cost']}** - {model_info['description']}")
    st.sidebar.info(f"ğŸ“‹ **Requisitos**: {model_info['requirements']}")
    
    # Advertencia especial para Mistral
    if generative_model_name == "mistral-7b":
        st.sidebar.warning("âš ï¸ **AtenciÃ³n**: Mistral (7B) es muy pesado para laptops. Recomendamos usar TinyLlama (1.1B).")
        st.sidebar.info("ğŸ“ Mistral requiere ~14GB de descarga y 6-8GB RAM.")
        
        # Verificar memoria disponible
        try:
            import psutil
            available_memory = psutil.virtual_memory().available / (1024**3)
            if available_memory < 6:
                st.sidebar.error(f"ğŸš« Memoria insuficiente: {available_memory:.1f}GB disponible, 6GB+ requerido")
            else:
                st.sidebar.info(f"âœ… Memoria disponible: {available_memory:.1f}GB")
        except:
            pass
elif generative_model_name == "llama-4-scout":
    st.sidebar.success("ğŸŒŸ **Modelo de API Gratuito** - Llama-4-Scout via OpenRouter")
    st.sidebar.info("â„¹ï¸ Si el modelo no estÃ¡ disponible temporalmente, intenta con TinyLlama como alternativa local.")
elif generative_model_name == "gemini-1.5-flash":
    st.sidebar.warning("ğŸ’° **Modelo de API** - Incurre en costos por uso")
elif generative_model_name == "gpt-4":
    st.sidebar.warning("ğŸ’° **Modelo de API** - Incurre en costos altos por uso")

# PÃ¡ginas principales
if page == "ğŸ” BÃºsqueda Individual":
    
    # ParÃ¡metros de bÃºsqueda
    search_params = st.sidebar.expander("ğŸ” ParÃ¡metros de BÃºsqueda", expanded=True)
    with search_params:
        top_k = st.slider("Documentos a retornar", 5, 20, 10)
        use_llm_reranker = st.checkbox("Usar Re-Ranking con LLM", value=True, help="Usa CrossEncoder (ms-marco-MiniLM-L-6-v2) para reordenar documentos con mayor precisiÃ³n.")
        diversity_threshold = st.slider(
            "Umbral de diversidad", 0.5, 0.95, 0.85, 0.05,
            help="Controla la diversidad de resultados (mÃ¡s alto = mÃ¡s diverso)"
        )
    
    # ConfiguraciÃ³n RAG
    rag_params = st.sidebar.expander("ğŸ¤– ConfiguraciÃ³n RAG", expanded=False)
    with rag_params:
        enable_rag = st.checkbox("Activar RAG Completo", value=True, 
                                help="Genera respuestas sintetizadas usando los documentos encontrados")
        
        # OpciÃ³n para generar respuesta en bÃºsqueda individual
        if not enable_rag:
            generate_individual_answer = st.checkbox("Generar Respuesta Individual", value=True,
                                                    help="Genera respuesta usando documentos con score â‰¥ 0.8 o mÃ­nimo 3 documentos con el modelo seleccionado")
            if generate_individual_answer:
                st.info("ğŸ¯ Criterio de selecciÃ³n: Documentos con score â‰¥ 0.8 o mÃ­nimo 3 documentos")
                
                # OpciÃ³n de precarga manual
                if generative_model_name == "tinyllama-1.1b":
                    if st.button("ğŸš€ Precargar TinyLlama (MÃ¡s RÃ¡pido)", help="Carga el modelo en memoria para respuestas mÃ¡s rÃ¡pidas"):
                        with st.spinner("ğŸ”„ Precargando modelo..."):
                            client = get_cached_tinyllama_client()
                            success = client.ensure_loaded()
                            if not success:
                                st.error("âŒ Error precargando modelo")
        else:
            generate_individual_answer = False
            
        evaluate_rag_quality = st.checkbox("Evaluar Calidad RAG", value=True,
                                         help="Calcula mÃ©tricas RAGAS (faithfulness, answer relevancy, context precision/recall) y BERTScore usando modelos reales")
        show_rag_metrics = st.checkbox("Mostrar MÃ©tricas RAG", value=True,
                                     help="Muestra mÃ©tricas de confianza y completitud de la respuesta generada")

    # MÃ©tricas de evaluaciÃ³n
    eval_params = st.sidebar.expander("ğŸ“Š EvaluaciÃ³n", expanded=False)
    with eval_params:
        enable_openai_comparison = st.checkbox("Comparar con OpenAI", value=False)
        show_debug_info = st.checkbox("Mostrar informaciÃ³n de debug", value=True)

    chromadb_wrapper, embedding_client, openai_client, gemini_client, local_tinyllama_client, local_mistral_client, openrouter_client, client = initialize_clients(model_name, generative_model_name)
    
    # Ãrea principal
    col1, col2 = st.columns([2, 1])

    with col1:
        st.subheader("ğŸ’¬ Haz tu pregunta sobre Azure")
    
    # Question input with examples
    question_examples = [
        "Â¿CÃ³mo configurar Managed Identity en Azure Functions?",
        "Â¿CÃ³mo conectar Azure Functions a Key Vault sin secrets?",
        "Â¿CuÃ¡les son las mejores prÃ¡cticas para Azure Storage?",
        "Â¿CÃ³mo implementar autenticaciÃ³n en Azure App Service?"
    ]
    
    # Initialize session state for persistence
    if 'last_title' not in st.session_state:
        st.session_state.last_title = ""
    if 'last_question' not in st.session_state:
        st.session_state.last_question = ""
    if 'keyword_search_results' not in st.session_state:
        st.session_state.keyword_search_results = []
    if 'keyword_search_query' not in st.session_state:
        st.session_state.keyword_search_query = ""
    
    # Update inputs if example was selected or random question loaded
    if 'selected_title' in st.session_state:
        title_value = st.session_state.selected_title
        # Update last_title to persist the value
        st.session_state.last_title = st.session_state.selected_title
        del st.session_state.selected_title
    else:
        title_value = st.session_state.last_title
    
    if 'selected_question' in st.session_state:
        selected_question = st.session_state.selected_question
        # Update last_question to persist the value
        st.session_state.last_question = st.session_state.selected_question
        del st.session_state.selected_question
    else:
        selected_question = st.session_state.last_question

    title = st.text_input(
        "ğŸ“ TÃ­tulo (opcional):",
        value=title_value,
        placeholder="e.g., Azure Functions Authentication, Virtual Machine Setup, etc.",
        help="Un tÃ­tulo descriptivo para tu consulta",
        key="title_input"
    )

    question = st.text_area(
        "â“ Tu pregunta:",
        value=selected_question,
        height=120,
        placeholder="Describe tu pregunta tÃ©cnica sobre Azure en detalle...",
        help="SÃ© especÃ­fico sobre el servicio de Azure y lo que quieres lograr",
        key="question_input"
    )

    # BotÃ³n para cargar pregunta aleatoria
    if st.button("ğŸ² Cargar Pregunta Aleatoria", 
                help="Carga una pregunta aleatoria con enlaces vÃ¡lidos de Microsoft Learn",
                key="load_random_question"):
        
        with st.spinner("ğŸ” Cargando pregunta optimizada con enlaces vÃ¡lidos..."):
            try:
                # Importar funciÃ³n optimizada y utilidades necesarias
                from src.data.optimized_questions import get_optimized_random_question
                from src.services.auth.clients import initialize_clients
                
                # Obtener el modelo de embedding desde la configuraciÃ³n actual
                embedding_model = model_name  # Usar directamente el modelo seleccionado en el selectbox
                
                # Inicializar cliente ChromaDB
                chromadb_wrapper, embedding_client, openai_client, gemini_client, local_tinyllama_client, local_mistral_client, openrouter_client, client = initialize_clients(
                    model_name=embedding_model,
                    generative_model_name="gpt-4"  # No se usa para esta operaciÃ³n
                )
                
                # Obtener pregunta aleatoria validada (versiÃ³n optimizada ultra-rÃ¡pida)
                random_question = get_optimized_random_question(
                    chromadb_wrapper=chromadb_wrapper,
                    embedding_model_name=embedding_model,
                    max_attempts=3  # Solo 3 intentos necesarios con la colecciÃ³n optimizada
                )
                
                if random_question:
                    # Actualizar session state para cargar en los campos
                    st.session_state.selected_title = random_question.get('title', '')
                    st.session_state.selected_question = random_question.get('question_content', random_question.get('question', ''))
                    
                    # Mostrar informaciÃ³n de la pregunta cargada
                    st.success("âœ… Pregunta cargada exitosamente! Los campos se actualizarÃ¡n automÃ¡ticamente.")
                    
                    # Mostrar estadÃ­sticas de validaciÃ³n en un expander
                    with st.expander("ğŸ“Š InformaciÃ³n de la pregunta cargada"):
                        st.write(f"**TÃ­tulo:** {random_question.get('title', 'Sin tÃ­tulo')}")
                        st.write(f"**Enlaces totales:** {random_question.get('total_links', 0)}")
                        st.write(f"**Enlaces vÃ¡lidos:** {random_question.get('valid_links', 0)}")
                        if random_question.get('validation_success_rate'):
                            rate = random_question['validation_success_rate'] * 100
                            st.write(f"**Tasa de validaciÃ³n:** {rate:.1f}%")
                        
                        if random_question.get('validated_links'):
                            st.write("**Enlaces validados:**")
                            for link in random_question['validated_links'][:3]:  # Mostrar primeros 3
                                st.write(f"- {link}")
                            if len(random_question['validated_links']) > 3:
                                st.write(f"... y {len(random_question['validated_links']) - 3} mÃ¡s")
                    
                    # Forzar recarga de la pÃ¡gina para actualizar los campos
                    st.rerun()
                    
                else:
                    st.warning("âš ï¸ No se pudo encontrar una pregunta con enlaces vÃ¡lidos. Intenta nuevamente.")
                    st.info("ğŸ’¡ Tip: Las preguntas vÃ¡lidas requieren que los enlaces de Microsoft Learn en la respuesta aceptada existan como documentos en la base de datos.")
                    
            except Exception as e:
                st.error(f"âŒ Error al cargar pregunta aleatoria: {str(e)}")
                st.info("ğŸ”§ Verifica que la base de datos estÃ© disponible y configurada correctamente.")

    with col2:
        st.subheader("ğŸ“ˆ MÃ©tricas de SesiÃ³n")

        # Inicializar mÃ©tricas de sesiÃ³n
        if 'session_metrics' not in st.session_state:
            st.session_state.session_metrics = {
                'queries_made': 0,
                'avg_response_time': 0,
                'total_docs_retrieved': 0
            }

        metrics_container = st.container()

    # BotÃ³n de bÃºsqueda
    if st.button("ğŸ” Buscar DocumentaciÃ³n", type="primary", use_container_width=True):
        # Save current title and question to session state for persistence
        st.session_state.last_title = title
        st.session_state.last_question = question

        # Debug info para entender el problema
        if st.session_state.get('debug_mode', False):
            st.write(f"Debug - question value: '{question}'")
            st.write(f"Debug - title value: '{title}'")
            st.write(f"Debug - last_question: '{st.session_state.get('last_question', '')}'")
        
        # Usar la pregunta del input field o del session state si estÃ¡ vacÃ­a
        actual_question = question.strip() or st.session_state.get('last_question', '').strip()
        
        if not actual_question:
            st.warning("âš ï¸ Por favor ingresa una pregunta.")
            if st.session_state.get('last_question', ''):
                st.info(f"ğŸ’¡ Pregunta detectada en session: {st.session_state['last_question'][:100]}...")
        else:
            # Combine title and question for better search
            full_query = f"{title.strip()} {actual_question}".strip()
            
            # Medir tiempo de respuesta
            start_time = time.time()
            
            with st.spinner("ğŸ” Buscando documentaciÃ³n relevante..." + (" y generando respuesta..." if enable_rag else "")):
                # Ejecutar bÃºsqueda con o sin RAG
                if enable_rag:
                    results, debug_info, generated_answer, rag_metrics = answer_question_with_rag(
                        full_query,
                        chromadb_wrapper,
                        embedding_client,
                        openai_client,
                        gemini_client,
                        local_tinyllama_client,
                        local_mistral_client,
                        top_k=top_k,
                        diversity_threshold=diversity_threshold,
                        use_llm_reranker=use_llm_reranker,
                        use_questions_collection=False,
                        evaluate_quality=evaluate_rag_quality,
                        documents_class=CHROMADB_COLLECTION_CONFIG[model_name]["documents"],
                        questions_class=CHROMADB_COLLECTION_CONFIG[model_name]["questions"],
                        generative_model_name=generative_model_name
                    )
                    # For RAG mode, we don't have pre-reranking docs, so use results
                    pre_reranking_docs = results
                else:
                    # Get both pre and post reranking results for comparison
                    from src.core.qa_pipeline import answer_question_documents_with_comparison
                    pre_reranking_docs, post_reranking_docs, debug_info = answer_question_documents_with_comparison(
                        full_query,
                        chromadb_wrapper,
                        embedding_client,
                        openai_client,
                        top_k=top_k,
                        diversity_threshold=diversity_threshold,
                        use_llm_reranker=use_llm_reranker,
                        use_questions_collection=False,
                        documents_class=CHROMADB_COLLECTION_CONFIG[model_name]["documents"],
                        questions_class=CHROMADB_COLLECTION_CONFIG[model_name]["questions"]
                    )
                    # Use post-reranking results as main results for compatibility
                    results = post_reranking_docs
                    
                    # Generate final answer using local model for individual search
                    generated_answer = None
                    rag_metrics = {}
                    
                    if results and generate_individual_answer:
                        # Filter documents with score >= 0.8 or take at least 3 documents
                        high_score_docs = [doc for doc in results if doc.get('score', 0) >= 0.8]
                        
                        if len(high_score_docs) >= 3:
                            selected_docs = high_score_docs
                        else:
                            # Take at least 3 documents (or all if less than 3)
                            selected_docs = results[:max(3, len(high_score_docs))]
                        
                        # Generate answer using selected model
                        if generative_model_name == "llama-4-scout" and openrouter_client:
                            # Use OpenRouter client for Llama-4-Scout
                            try:
                                # Prepare context from selected documents with links
                                context_parts = []
                                for i, doc in enumerate(selected_docs):
                                    title = doc.get('title', f'Documento {i+1}')
                                    content = doc.get('content', '')
                                    link = doc.get('link', '')
                                    
                                    context_part = f"Documento {i+1}:\nTÃ­tulo: {title}\n"
                                    if link:
                                        context_part += f"Enlace: {link}\n"
                                    context_part += f"Contenido: {content}"
                                    context_parts.append(context_part)
                                
                                context = "\n\n".join(context_parts)
                                
                                generated_answer = openrouter_client.generate_answer(
                                    question=full_query,
                                    context=context,
                                    max_length=512
                                )
                                
                                # Add Microsoft Learn links to the response
                                if generated_answer and not generated_answer.startswith("Error"):
                                    ms_links = []
                                    for doc in selected_docs[:6]:
                                        link = doc.get('link', '')
                                        title = doc.get('title', 'Documento')
                                        if link and 'learn.microsoft.com' in link:
                                            ms_links.append(f"- **{title}**  \n  {link}")
                                    
                                    if ms_links:
                                        generated_answer += "\n\n## Enlaces y Referencias\n\n"
                                        generated_answer += "\n\n".join(ms_links[:max(3, len(ms_links))])
                                        generated_answer += "\n\n*Consulta la documentaciÃ³n oficial de Microsoft Learn para informaciÃ³n mÃ¡s detallada.*"
                                
                                # Calculate real RAGAS and BERTScore metrics
                                rag_metrics = {
                                    'docs_used': len(selected_docs),
                                    'high_score_docs': len(high_score_docs),
                                    'min_score': min([doc.get('score', 0) for doc in selected_docs]),
                                    'max_score': max([doc.get('score', 0) for doc in selected_docs]),
                                    'model_provider': 'OpenRouter'
                                }
                                
                                # Calculate RAGAS + BERTScore if enabled
                                if evaluate_rag_quality and show_rag_metrics:
                                    try:
                                        from src.services.answer_generation.ragas_evaluation import evaluate_answer_with_ragas_and_bertscore
                                        ragas_bert_metrics = evaluate_answer_with_ragas_and_bertscore(
                                            question=full_query,
                                            answer=generated_answer,
                                            source_docs=selected_docs,
                                            openai_client=openai_client
                                        )
                                        rag_metrics.update(ragas_bert_metrics)
                                    except Exception as e:
                                        st.warning(f"âš ï¸ Error calculando mÃ©tricas RAGAS/BERTScore: {e}")
                                        # Add default values
                                        rag_metrics.update({
                                            'faithfulness': 0.0,
                                            'answer_relevancy': 0.0,
                                            'context_precision': 0.0,
                                            'context_recall': 0.0,
                                            'bert_precision': 0.0,
                                            'bert_recall': 0.0,
                                            'bert_f1': 0.0
                                        })
                            except Exception as e:
                                st.error(f"Error generando respuesta con OpenRouter: {e}")
                                generated_answer = None
                                rag_metrics = {}
                        elif (generative_model_name == "tinyllama-1.1b" and local_tinyllama_client) or \
                           (generative_model_name == "mistral-7b" and local_mistral_client):
                            from src.services.answer_generation.local import generate_final_answer_local
                            
                            try:
                                # Optimized length for faster generation
                                max_len = 256 if generative_model_name == "tinyllama-1.1b" else 512
                                
                                generated_answer, generation_info = generate_final_answer_local(
                                    question=full_query,
                                    retrieved_docs=selected_docs,
                                    model_name=generative_model_name,
                                    max_length=max_len
                                )
                                
                                # Calculate real RAGAS and BERTScore metrics
                                rag_metrics = {
                                    'docs_used': len(selected_docs),
                                    'high_score_docs': len(high_score_docs),
                                    'min_score': min([doc.get('score', 0) for doc in selected_docs]),
                                    'max_score': max([doc.get('score', 0) for doc in selected_docs]),
                                    'model_provider': 'Local'
                                }
                                
                                # Calculate RAGAS + BERTScore if enabled
                                if evaluate_rag_quality and show_rag_metrics:
                                    try:
                                        from src.services.answer_generation.ragas_evaluation import evaluate_answer_with_ragas_and_bertscore
                                        ragas_bert_metrics = evaluate_answer_with_ragas_and_bertscore(
                                            question=full_query,
                                            answer=generated_answer,
                                            source_docs=selected_docs,
                                            openai_client=openai_client
                                        )
                                        rag_metrics.update(ragas_bert_metrics)
                                    except Exception as e:
                                        st.warning(f"âš ï¸ Error calculando mÃ©tricas RAGAS/BERTScore: {e}")
                                        # Add default values
                                        rag_metrics.update({
                                            'faithfulness': 0.0,
                                            'answer_relevancy': 0.0,
                                            'context_precision': 0.0,
                                            'context_recall': 0.0,
                                            'bert_precision': 0.0,
                                            'bert_recall': 0.0,
                                            'bert_f1': 0.0
                                        })
                            except Exception as e:
                                st.error(f"Error generando respuesta con {generative_model_name}: {e}")
                                generated_answer = None
                                rag_metrics = {}
                        else:
                            # No hay cliente disponible para el modelo seleccionado
                            if generative_model_name == "llama-4-scout":
                                st.warning(f"âš ï¸ OpenRouter client no estÃ¡ disponible. Verifica tu API key OPEN_ROUTER_KEY.")
                            elif generative_model_name in ["tinyllama-1.1b", "mistral-7b"]:
                                st.warning(f"âš ï¸ Modelo local {generative_model_name} no estÃ¡ disponible. AsegÃºrate de que estÃ© configurado correctamente.")
                            else:
                                st.warning(f"âš ï¸ Modelo {generative_model_name} no soportado para respuesta individual.")
                            generated_answer = None
                            rag_metrics = {}
            
            # Actualizar mÃ©tricas de sesiÃ³n
            response_time = time.time() - start_time
            st.session_state.session_metrics['queries_made'] += 1
            st.session_state.session_metrics['total_docs_retrieved'] += len(results) if 'results' in locals() and results else 0
            st.session_state.session_metrics['avg_response_time'] = (
                (st.session_state.session_metrics['avg_response_time'] * 
                 (st.session_state.session_metrics['queries_made'] - 1) + response_time) /
                st.session_state.session_metrics['queries_made']
            )

            # Mostrar resultados
            if 'results' in locals() and results:
                if generated_answer:
                    st.success(f"âœ… Respuesta generada con {len(results)} documentos en {response_time:.2f}s")
                else:
                    st.success(f"âœ… Encontrados {len(results)} documentos relevantes en {response_time:.2f}s")
                
                # Mostrar respuesta generada (RAG o bÃºsqueda individual)
                if generated_answer:
                    st.markdown("---")
                    if enable_rag:
                        st.markdown("### ğŸ¤– **Respuesta Generada (RAG)**")
                    else:
                        st.markdown("### ğŸ¤– **Respuesta Generada (BÃºsqueda Individual)**")
                        high_score_count = rag_metrics.get('high_score_docs', 0)
                        docs_used = rag_metrics.get('docs_used', 0)
                        min_score = rag_metrics.get('min_score', 0)
                        max_score = rag_metrics.get('max_score', 0)
                        
                        if high_score_count >= 3:
                            st.info(f"ğŸ¯ Usando {docs_used} documentos con score â‰¥ 0.8 (rango: {min_score:.3f} - {max_score:.3f})")
                        else:
                            st.info(f"ğŸ¯ Usando {docs_used} documentos ({high_score_count} con score â‰¥ 0.8, completado con top documentos)")
                            st.warning(f"âš ï¸ Pocos documentos de alta calidad encontrados. Rango de scores: {min_score:.3f} - {max_score:.3f}")
                    
                    # Mostrar mÃ©tricas RAGAS + BERTScore si estÃ¡n habilitadas
                    if show_rag_metrics and rag_metrics:
                        # Primera fila: MÃ©tricas RAGAS
                        st.markdown("##### ğŸ“Š MÃ©tricas RAGAS")
                        ragas_col1, ragas_col2, ragas_col3, ragas_col4 = st.columns(4)
                        
                        with ragas_col1:
                            faithfulness = rag_metrics.get('faithfulness', 0)
                            st.metric("ğŸ¯ Faithfulness", f"{faithfulness:.3f}", 
                                    help="Â¿La respuesta es fiel al contexto? (0-1)")
                        
                        with ragas_col2:
                            answer_relevancy = rag_metrics.get('answer_relevancy', 0)
                            st.metric("ğŸ” Answer Relevancy", f"{answer_relevancy:.3f}", 
                                    help="Â¿La respuesta aborda la pregunta? (0-1)")
                        
                        with ragas_col3:
                            context_precision = rag_metrics.get('context_precision', 0)
                            st.metric("ğŸ“ˆ Context Precision", f"{context_precision:.3f}", 
                                    help="Â¿Los docs estÃ¡n bien ordenados por relevancia? (0-1)")
                        
                        with ragas_col4:
                            context_recall = rag_metrics.get('context_recall', 0)
                            st.metric("ğŸ“š Context Recall", f"{context_recall:.3f}", 
                                    help="Â¿El contexto contiene toda la info necesaria? (0-1)")
                        
                        # Segunda fila: BERTScore
                        st.markdown("##### ğŸ¤– BERTScore")
                        bert_col1, bert_col2, bert_col3, bert_col4 = st.columns(4)
                        
                        with bert_col1:
                            bert_precision = rag_metrics.get('bert_precision', 0)
                            st.metric("ğŸ¯ BERT Precision", f"{bert_precision:.3f}", 
                                    help="PrecisiÃ³n semÃ¡ntica respecto a los documentos")
                        
                        with bert_col2:
                            bert_recall = rag_metrics.get('bert_recall', 0)
                            st.metric("ğŸ“Š BERT Recall", f"{bert_recall:.3f}", 
                                    help="Cobertura semÃ¡ntica de los documentos")
                        
                        with bert_col3:
                            bert_f1 = rag_metrics.get('bert_f1', 0)
                            st.metric("âš¡ BERT F1", f"{bert_f1:.3f}", 
                                    help="Balance entre precisiÃ³n y recall semÃ¡ntico")
                        
                        with bert_col4:
                            docs_used = rag_metrics.get('docs_used', 0)
                            st.metric("ğŸ“š Docs Usados", f"{docs_used}/{len(results)}", 
                                    help="Documentos utilizados para generar la respuesta")
                    
                    # Mostrar la respuesta generada
                    st.markdown("#### ğŸ’¬ Respuesta:")
                    st.markdown(generated_answer)
                    
                
                st.markdown("---")
                st.markdown("### ğŸ“„ **Documentos de Referencia**")
                
                # Create side-by-side comparison
                if enable_openai_comparison:
                    st.warning("âš ï¸ ComparaciÃ³n con OpenAI temporalmente deshabilitada debido a problemas de indentaciÃ³n")
                    openai_docs = []
                    openai_links = []
                    # TODO: Fix indentation issues in OpenAI comparison block
                    """
                    # Get OpenAI results first
                    with st.spinner("ğŸ¤– Consultando OpenAI para comparaciÃ³n..."):
                        try:
                            import json

                            tools = [
                                {
                                    "type": "function",
                                    "function": {
                                        "name": "list_azure_documentation",
                                        "description": "Provides a list of relevant Azure documentation links for a given user question.",
                                        "parameters": {
                                            "type": "object",
                                            "properties": {
                                                "documents": {
                                                    "type": "array",
                                                    "items": {
                                                        "type": "object",
                                                        "properties": {
                                                            "score": {"type": "number", "description": "Relevance score between 0 and 1, where 1 represents maximum relevance to the question."},
                                                            "title": {"type": "string", "description": "The official title of the documentation page."},
                                                            "link": {"type": "string", "description": "The full URL to the documentation page, must be from learn.microsoft.com."},
                                                        },
                                                        "required": ["score", "title", "link"]
                                                    }
                                                }
                                            },
                                            "required": ["documents"]
                                        }
                                    }
                                }
                            ]
                            
                            response = openai_client.chat.completions.create(
                                model="gpt-4",
                                messages=[
                                    {"role": "system", "content": "ActÃºa como un experto en documentaciÃ³n de Azure. Tu tarea es buscar y listar exactamente 10 documentos desde el dominio oficial https://learn.microsoft.com que puedan ayudar a responder una pregunta tÃ©cnica. Para cada documento, entrega un score (entre 0 y 1), donde 1 representa mÃ¡xima relevancia respecto a la pregunta, el tÃ­tulo del documento y el link completo. Los documentos deben estar ordenados de mayor a menor segÃºn el score. No inventes contenido ni enlaces. Solo acepta resultados del dominio learn.microsoft.com."},
                                    {"role": "user", "content": f"AquÃ­ estÃ¡ la pregunta a responder: {full_query}"}
                                ],
                                tools=tools,
                                tool_choice={"type": "function", "function": {"name": "list_azure_documentation"}},
                                temperature=0.1
                            )
                            
                            message = response.choices[0].message
                            openai_docs = []
                            openai_links = []

                            if message.tool_calls:
                                tool_call = message.tool_calls[0]
                                if tool_call.function.name == "list_azure_documentation":
                                    # Sanitize JSON arguments to handle control characters
                                    raw_arguments = tool_call.function.arguments
                                sanitized_arguments = _sanitize_json_string(raw_arguments)
                                
                                try:
                                    tool_args = json.loads(sanitized_arguments)
                                except json.JSONDecodeError as e:
                                    st.error(f"Error procesando respuesta de OpenAI: {e}")
                                    tool_args = {"documents": []}
                                documents_data = tool_args.get("documents", [])
                                
                                for doc in documents_data:
                                    # Use the score provided by OpenAI
                                    openai_docs.append({
                                        'title': doc.get('title'),
                                        'link': doc.get('link'),
                                        'score': doc.get('score', 1.0),  # Default to 1.0 if score not provided
                                        'source': 'OpenAI GPT-4'
                                    })
                                
                                openai_links = [doc.get("link") for doc in documents_data if doc.get("link")]
                        else:
                            st.warning("OpenAI did not return documentation in the expected format.")

                    except Exception as e:
                        st.error(f"Error consultando OpenAI: {e}")
                        openai_docs = []
                        openai_links = []
                    """
                else:
                    openai_docs = []
                    openai_links = []
            
            # Show before/after reranking comparison
            st.markdown("### ğŸ”„ ComparaciÃ³n: Antes vs DespuÃ©s del Reranking")
            
            col_before, col_after = st.columns(2)
            
            with col_before:
                st.subheader("ğŸ“¥ ANTES del Reranking")
                st.markdown(f"*Top {len(pre_reranking_docs)} documentos (bÃºsqueda inicial)*")
                
                # Show pre-reranking results
                for i, doc in enumerate(pre_reranking_docs[:10], 1):
                    score = doc.get('score', doc.get('original_score', 0))
                    score_color = "#28a745" if score > 0.8 else "#ffc107" if score > 0.6 else "#dc3545"
                    
                    st.markdown(f"""
                    <div class="doc-card" style="border-left: 4px solid {score_color};">
                        <p><strong>#{i}</strong> {doc.get('title', 'Sin tÃ­tulo')[:50]}...</p>
                        <p><strong>ğŸ“Š Score:</strong> <span style="color: {score_color}; font-weight: bold;">{score:.4f}</span></p>
                        <p style="font-size: 0.8em; color: #666;">ğŸ”— {doc.get('link', 'N/A')[:60]}...</p>
                    </div>
                    """, unsafe_allow_html=True)
                
                # Stats for pre-reranking
                with st.expander("ğŸ“Š EstadÃ­sticas Pre-Reranking"):
                    if pre_reranking_docs:
                        scores = [doc.get('score', doc.get('original_score', 0)) for doc in pre_reranking_docs]
                        st.write(f"- **Score Promedio:** {np.mean(scores):.3f}")
                        st.write(f"- **Score MÃ¡ximo:** {max(scores):.3f}")
                        st.write(f"- **Score MÃ­nimo:** {min(scores):.3f}")
                        st.write(f"- **Desv. EstÃ¡ndar:** {np.std(scores):.3f}")
            
            with col_after:
                st.subheader("ğŸ“¤ DESPUÃ‰S del Reranking")
                reranking_method = "CrossEncoder" if use_llm_reranker else "Embedding Similarity"
                st.markdown(f"*Top {len(results)} documentos ({reranking_method})*")
                
                # Show post-reranking results  
                for i, doc in enumerate(results[:10], 1):
                    score = doc.get('score', 0)
                    pre_score = doc.get('pre_rerank_score', None)
                    score_color = "#28a745" if score > 0.8 else "#ffc107" if score > 0.6 else "#dc3545"
                    
                    # Check position change
                    original_pos = None
                    for j, pre_doc in enumerate(pre_reranking_docs):
                        if pre_doc.get('link') == doc.get('link'):
                            original_pos = j + 1
                            break
                    
                    position_change = ""
                    if original_pos:
                        if original_pos > i:
                            position_change = f"<span style='color: green;'>â†‘ {original_pos - i}</span>"
                        elif original_pos < i:
                            position_change = f"<span style='color: red;'>â†“ {i - original_pos}</span>"
                        else:
                            position_change = "<span style='color: gray;'>â†’</span>"
                    else:
                        position_change = "<span style='color: blue;'>NEW</span>"
                    
                    st.markdown(f"""
                    <div class="doc-card" style="border-left: 4px solid {score_color};">
                        <p><strong>#{i}</strong> {doc.get('title', 'Sin tÃ­tulo')[:50]}... {position_change}</p>
                        <p><strong>ğŸ“Š Score:</strong> <span style="color: {score_color}; font-weight: bold;">{score:.4f}</span></p>
                        <p style="font-size: 0.8em; color: #666;">ğŸ”— {doc.get('link', 'N/A')[:60]}...</p>
                    </div>
                    """, unsafe_allow_html=True)
                
                # Stats for post-reranking
                with st.expander("ğŸ“Š EstadÃ­sticas Post-Reranking"):
                    if results:
                        scores = [doc.get('score', 0) for doc in results]
                        st.write(f"- **Score Promedio:** {np.mean(scores):.3f}")
                        st.write(f"- **Score MÃ¡ximo:** {max(scores):.3f}")
                        st.write(f"- **Score MÃ­nimo:** {min(scores):.3f}")
                        st.write(f"- **Desv. EstÃ¡ndar:** {np.std(scores):.3f}")
                        st.write(f"- **MÃ©todo:** {reranking_method}")
            
            # Add comparison metrics
            st.markdown("---")
            st.markdown("### ğŸ“Š AnÃ¡lisis del Impacto del Reranking")
            
            # Calculate reranking impact metrics
            if pre_reranking_docs and results:
                # Position changes analysis
                position_changes = []
                for i, post_doc in enumerate(results[:10]):
                    for j, pre_doc in enumerate(pre_reranking_docs[:10]):
                        if post_doc.get('link') == pre_doc.get('link'):
                            position_changes.append(j - i)  # Positive = moved up
                            break
                
                if position_changes:
                    avg_position_change = np.mean(np.abs(position_changes))
                    docs_moved_up = sum(1 for change in position_changes if change > 0)
                    docs_moved_down = sum(1 for change in position_changes if change < 0)
                    docs_unchanged = sum(1 for change in position_changes if change == 0)
                    
                    col1, col2, col3, col4 = st.columns(4)
                    with col1:
                        st.metric("ğŸ“ˆ Docs que Subieron", docs_moved_up,
                                help="Documentos que mejoraron su posiciÃ³n")
                    with col2:
                        st.metric("ğŸ“‰ Docs que Bajaron", docs_moved_down,
                                help="Documentos que bajaron de posiciÃ³n")
                    with col3:
                        st.metric("â¡ï¸ Sin Cambios", docs_unchanged,
                                help="Documentos que mantuvieron su posiciÃ³n")
                    with col4:
                        st.metric("ğŸ”„ Cambio Promedio", f"{avg_position_change:.1f} pos",
                                help="Cambio promedio de posiciÃ³n (absoluto)")
            
            # Add retrieval query info
            with st.expander("ğŸ“ Pregunta utilizada para el retrieval"):
                st.code(full_query, language="text")
                st.info("Esta es la pregunta procesada que se utilizÃ³ para buscar documentos relevantes.")
            
            # Keep OpenAI comparison if enabled
            if enable_openai_comparison:
                st.markdown("---")
                st.markdown("### ğŸ¤– ComparaciÃ³n con OpenAI GPT-4")
                col_our_final, col_openai = st.columns(2)
                
                with col_our_final:
                    st.subheader("ğŸ” Nuestro Sistema (Final)")
                    st.markdown(f"*Top 5 despuÃ©s de reranking*")
                    
                    for i, doc in enumerate(results[:5], 1):
                        score = doc.get('score', 0)
                        score_color = "#28a745" if score > 0.8 else "#ffc107" if score > 0.6 else "#dc3545"
                        
                        st.markdown(f"""
                        <div class="doc-card" style="border-left: 4px solid {score_color};">
                            <p>#{i} {doc.get('title', 'Sin tÃ­tulo')}</p>
                            <p><strong>ğŸ“Š Score:</strong> <span style="color: {score_color}; font-weight: bold;">{score:.4f}</span></p>
                            <p><strong>ğŸ”— Link:</strong> <a href="{doc.get('link', '#')}" target="_blank" style="color: #0078d4;">{doc.get('link', 'N/A')}</a></p>
                        </div>
                        """, unsafe_allow_html=True)
                
                with col_openai:
                    if enable_openai_comparison and openai_docs:
                        st.subheader("ğŸ¤– OpenAI GPT-4 Expert")
                        st.markdown(f"*{len(openai_docs)} documentos recomendados*")
                    
                        # Show OpenAI results with same styling
                        for i, doc in enumerate(openai_docs[:10], 1):
                            score = doc.get('score', 1.0)
                            score_color = "#0078d4"
                            
                            st.markdown(f"""
                            <div class="doc-card" style="border-left: 4px solid {score_color};">
                                <h4>#{i} {doc.get('title', 'Sin tÃ­tulo')}</h4>
                                <p><strong>ğŸ“Š Score:</strong> <span style="color: {score_color}; font-weight: bold;">{score:.4f}</span></p>
                                <p><strong>ğŸ”— Link:</strong> <a href="{doc.get('link', '#')}" target="_blank" style="color: #0078d4;">{doc.get('link', 'N/A')}</a></p>
                            </div>
                            """, unsafe_allow_html=True)
                        
                        # Add accordion with retrieval query for OpenAI system
                        with st.expander("ğŸ“ Prompt completo utilizado para OpenAI"):
                            st.markdown("**System Prompt:**")
                            st.code("ActÃºa como un experto en documentaciÃ³n de Azure. Tu tarea es buscar y listar exactamente 10 documentos desde el dominio oficial https://learn.microsoft.com que puedan ayudar a responder una pregunta tÃ©cnica. Para cada documento, entrega un score (entre 0 y 1), donde 1 representa mÃ¡xima relevancia respecto a la pregunta, el tÃ­tulo del documento y el link completo. Los documentos deben estar ordenados de mayor a menor segÃºn el score. No inventes contenido ni enlaces. Solo acepta resultados del dominio learn.microsoft.com.", language="text")
                            st.markdown("**User Prompt:**")
                            st.code(f"AquÃ­ estÃ¡ la pregunta a responder: {full_query}", language="text")
                            st.info("Este es el prompt completo enviado a OpenAI GPT-4 para obtener recomendaciones de documentaciÃ³n con scores de relevancia.")
                    else:
                        st.subheader("ğŸ¤– OpenAI GPT-4 Expert")
                        if enable_openai_comparison:
                            st.warning("No se pudieron obtener resultados de OpenAI")
                        else:
                            st.info("ğŸ’¡ Habilita la comparaciÃ³n con OpenAI en la configuraciÃ³n para ver resultados paralelos")
            
            # Comparison metrics at the bottom
            if enable_openai_comparison and openai_links:
                st.markdown("---")
                st.subheader("ğŸ“Š AnÃ¡lisis Comparativo")
                
                our_links = [doc["link"] for doc in results if "learn.microsoft.com" in doc.get("link", "")]
                matches = len(set(openai_links) & set(our_links))
                
                metric_col1, metric_col2, metric_col3, metric_col4 = st.columns(4)
                with metric_col1:
                    st.metric("ğŸ” Nuestros resultados", len(our_links))
                with metric_col2:
                    st.metric("ğŸ¤– OpenAI resultados", len(openai_links))
                with metric_col3:
                    st.metric("âœ… Coincidencias", matches)
                with metric_col4:
                    st.metric("ğŸ“ˆ % Coincidencia", f"{(matches/len(openai_links)*100) if openai_links else 0:.1f}%")
            
            # Additional tabs for analysis and debug
            st.markdown("---")
            tab1, tab2 = st.tabs(["ğŸ“Š AnÃ¡lisis Detallado", "ğŸ”§ Debug Info"])
            
            with tab1:
                if results:
                    # Performance analysis chart
                    df_results = pd.DataFrame([
                        {
                            'Documento': f"Doc {i+1}",
                            'Score': doc.get('score', 0),
                            'TÃ­tulo': doc.get('title', 'Sin tÃ­tulo')[:30] + "..."
                        }
                        for i, doc in enumerate(results[:10])
                    ])
                    
                    fig = px.bar(
                        df_results, 
                        x='Documento', 
                        y='Score',
                        hover_data=['TÃ­tulo'],
                        title="ğŸ“ˆ Scores de Relevancia - Nuestro Sistema",
                        color='Score',
                        color_continuous_scale='viridis'
                    )
                    st.plotly_chart(fig, use_container_width=True)

                    # Always show quality metrics (work with or without OpenAI)
                    st.markdown("---")
                    st.subheader("ğŸ“Š MÃ©tricas de Calidad del Sistema")
                    
                    # Calculate intrinsic quality metrics
                    scores = [doc.get('score', 0) for doc in results if doc.get('score', 0) > 0]
                    
                    if scores:
                        import numpy as np
                        
                        # Quality metrics
                        top5_scores = scores[:5] if len(scores) >= 5 else scores
                        top3_scores = scores[:3] if len(scores) >= 3 else scores
                        
                        score_avg_top5 = np.mean(top5_scores)
                        score_min_top3 = min(top3_scores) if top3_scores else 0
                        score_max = max(scores)
                        score_std = np.std(scores)
                        
                        # Coverage metrics
                        high_quality_docs = len([s for s in scores if s >= 0.8])
                        medium_quality_docs = len([s for s in scores if 0.6 <= s < 0.8])
                        coverage_quality = high_quality_docs / len(scores) * 100
                        
                        # Diversity metrics (simple entropy-based)
                        score_entropy = -np.sum([(s/sum(scores)) * np.log2(s/sum(scores)) for s in scores if s > 0])
                        
                        # Display primary quality metrics
                        st.markdown("**ğŸ¯ MÃ©tricas Principales de Calidad**")
                        q_col1, q_col2, q_col3, q_col4 = st.columns(4)
                        
                        with q_col1:
                            st.metric("ğŸ“ˆ Score Promedio Top-5", f"{score_avg_top5:.3f}", 
                                    help="FÃ³rmula: Î£(scores_top5) / 5\n\nMide la calidad promedio de los 5 mejores documentos encontrados. Un valor alto (>0.8) indica que los primeros resultados son muy relevantes. Valores bajos (<0.6) sugieren que incluso los mejores documentos tienen poca relevancia.")
                        with q_col2:
                            st.metric("ğŸ¯ Score MÃ­nimo Top-3", f"{score_min_top3:.3f}", 
                                    help="FÃ³rmula: min(scores_top3)\n\nRepresenta el peor documento entre los 3 primeros resultados. Es un umbral de calidad mÃ­nima: si este valor es alto (>0.7), garantiza que incluso el tercer mejor documento es muy relevante. Si es bajo (<0.5), indica inconsistencia en la calidad.")
                        with q_col3:
                            st.metric("â­ Score MÃ¡ximo", f"{score_max:.3f}", 
                                    help="FÃ³rmula: max(scores)\n\nScore del documento mÃ¡s relevante encontrado. Indica el mejor match posible en la base de datos. Valores cercanos a 1.0 significan una coincidencia casi perfecta. Valores bajos (<0.8) sugieren que no hay documentos altamente relevantes.")
                        with q_col4:
                            st.metric("ğŸ“Š Cobertura Alta Calidad", f"{coverage_quality:.1f}%", 
                                    help="FÃ³rmula: (docs_scoreâ‰¥0.8 / total_docs) Ã— 100\n\nPorcentaje de documentos con alta relevancia (score â‰¥ 0.8). Alta cobertura (>60%) indica que la mayorÃ­a de resultados son Ãºtiles. Baja cobertura (<30%) sugiere que solo pocos documentos son realmente relevantes.")
                        
                        # User experience metrics
                        st.markdown("**ğŸš€ MÃ©tricas de Experiencia del Usuario**")
                        ux_col1, ux_col2, ux_col3, ux_col4 = st.columns(4)
                        
                        with ux_col1:
                            st.metric("âš¡ Tiempo de Respuesta", f"{response_time:.2f}s", 
                                    help="FÃ³rmula: tiempo_fin - tiempo_inicio\n\nTiempo total desde que se envÃ­a la consulta hasta que se obtienen los resultados. Incluye bÃºsqueda vectorial, re-ranking y generaciÃ³n de respuesta. Tiempos <2s son excelentes, 2-5s son buenos, >5s pueden afectar la experiencia del usuario.")
                        with ux_col2:
                            st.metric("ğŸ“š Documentos Encontrados", f"{len(results)}", 
                                    help="FÃ³rmula: count(retrieved_docs)\n\nNÃºmero total de documentos recuperados por el sistema. MÃ¡s documentos ofrecen mayor cobertura pero pueden incluir resultados menos relevantes. El valor Ã³ptimo depende del parÃ¡metro top_k configurado.")
                        with ux_col3:
                            st.metric("ğŸ² Diversidad (EntropÃ­a)", f"{score_entropy:.2f}", 
                                    help="FÃ³rmula: -Î£(pi Ã— log2(pi)) donde pi = score_i/Î£(scores)\n\nMide la variedad en los scores de relevancia. Alta entropÃ­a (>3) indica scores diversos, sugiriendo documentos con diferentes niveles de relevancia. Baja entropÃ­a (<1) indica scores similares, ya sea todos altos o todos bajos.")
                        with ux_col4:
                            variability = "Alta" if score_std > 0.1 else "Media" if score_std > 0.05 else "Baja"
                            st.metric("ğŸ“ˆ Variabilidad", f"{variability}", 
                                    help=f"FÃ³rmula: âˆš(Î£(score_i - media)Â² / n)\n\nDesviaciÃ³n estÃ¡ndar: {score_std:.3f}\n\nMide la dispersiÃ³n de los scores. Alta variabilidad (>0.1) indica mezcla de documentos muy relevantes y poco relevantes. Baja variabilidad (<0.05) indica consistencia en la calidad de resultados.")
                        
                        # Content quality breakdown
                        st.markdown("**ğŸ“„ AnÃ¡lisis de Contenido**")
                        content_col1, content_col2 = st.columns(2)
                        
                        with content_col1:
                            st.metric("ğŸ”¥ Docs Alta Calidad", f"{high_quality_docs}", 
                                    help="FÃ³rmula: count(docs where score â‰¥ 0.8)\n\nCantidad absoluta de documentos con alta relevancia. Complementa el porcentaje de cobertura mostrando el nÃºmero exacto. Valores altos (>5) indican buena cantidad de contenido relevante disponible.")
                        with content_col2:
                            st.metric("âš¡ Docs Calidad Media", f"{medium_quality_docs}", 
                                    help="FÃ³rmula: count(docs where 0.6 â‰¤ score < 0.8)\n\nCantidad de documentos con relevancia moderada. Estos documentos pueden ser Ãºtiles como informaciÃ³n complementaria. Un balance entre docs de alta y media calidad indica un sistema robusto.")
                    
                    # OpenAI comparison section (only if enabled)
                    if enable_openai_comparison and openai_links:
                        st.markdown("---")
                        st.subheader("ğŸ¤– ComparaciÃ³n con OpenAI")
                        
                        # Simple overlap metrics (more reliable than complex ranking metrics)
                        our_links = [doc.get("link", "") for doc in results]
                        
                        # Normalize URLs for better comparison
                        def normalize_url(url):
                            return url.lower().replace('/en-us/', '/').replace('///', '//').rstrip('/')
                        
                        our_links_normalized = [normalize_url(link) for link in our_links if link]
                        openai_links_normalized = [normalize_url(link) for link in openai_links if link]
                        
                        # Calculate overlap
                        common_links = set(our_links_normalized) & set(openai_links_normalized)
                        overlap_percentage = len(common_links) / len(openai_links_normalized) * 100 if openai_links_normalized else 0
                        
                        # Jaccard similarity of titles
                        def get_keywords(text):
                            return set(text.lower().split()) if text else set()
                        
                        our_titles = " ".join([doc.get('title', '') for doc in results[:5]])
                        openai_titles = " ".join([doc.get('title', '') for doc in openai_docs[:5]])
                        
                        our_keywords = get_keywords(our_titles)
                        openai_keywords = get_keywords(openai_titles)
                        
                        jaccard_similarity = len(our_keywords & openai_keywords) / len(our_keywords | openai_keywords) * 100 if (our_keywords | openai_keywords) else 0
                        
                        # Display comparison metrics
                        comp_col1, comp_col2, comp_col3, comp_col4 = st.columns(4)
                        
                        with comp_col1:
                            st.metric("ğŸ” Nuestros Resultados", len(our_links))
                        with comp_col2:
                            st.metric("ğŸ¤– OpenAI Resultados", len(openai_links))
                        with comp_col3:
                            st.metric("âœ… Overlap de Enlaces", f"{overlap_percentage:.1f}%", 
                                    help="FÃ³rmula: (|nuestros_links âˆ© openai_links| / |openai_links|) Ã— 100\n\nPorcentaje de enlaces que ambos sistemas encontraron en comÃºn. URLs normalizadas para evitar diferencias de formato. Alto overlap (>70%) indica concordancia, bajo overlap (<30%) sugiere enfoques diferentes.")
                        with comp_col4:
                            st.metric("ğŸ“ Similitud de TÃ­tulos", f"{jaccard_similarity:.1f}%", 
                                    help="FÃ³rmula: (|palabras_comunes| / |palabras_totales|) Ã— 100\n\nSimilitud Jaccard entre las palabras de los tÃ­tulos top-5 de ambos sistemas. Mide si ambos sistemas encuentran documentos sobre temas similares, independientemente de los URLs exactos. Alta similitud (>50%) indica coherencia temÃ¡tica.")
                    else:
                        st.info("No hay scores disponibles para calcular mÃ©tricas de calidad.")
            
            with tab2:
                if show_debug_info:
                    st.subheader("ğŸ”§ InformaciÃ³n de Debug")
                    st.text(debug_info)
                else:
                    st.info("â„¹ï¸ Debug info deshabilitado en configuraciÃ³n")

            # Removed tab3 (Database Inspection) as requested

                # Display results if any are stored in session state
                if st.session_state.keyword_search_results:
                    st.success(f"Encontrados {len(st.session_state.keyword_search_results)} documentos para '{st.session_state.keyword_search_query}':")
                    for i, doc in enumerate(st.session_state.keyword_search_results):
                        content_preview = doc.get('content', '')[:500]
                        st.markdown(f"**{i+1}. {doc.get('title', 'Sin tÃ­tulo')}**")
                        st.markdown(f"ğŸ”— [{doc.get('link', '#')}]({doc.get('link', '#')})")
                        st.markdown(f"""
```
{content_preview}
```
""")
                        st.markdown("--- ")
                elif st.session_state.keyword_search_query and not st.session_state.keyword_search_results:
                    st.info(f"No se encontraron documentos para '{st.session_state.keyword_search_query}'.")
                else:
                    st.warning("âš ï¸ No se encontraron documentos relevantes. Intenta reformular tu pregunta.")

    # Actualizar mÃ©tricas en sidebar
    with metrics_container:
        st.metric("Consultas realizadas", st.session_state.session_metrics['queries_made'])
        st.metric("Tiempo promedio", f"{st.session_state.session_metrics['avg_response_time']:.2f}s")
        st.metric("Docs recuperados", st.session_state.session_metrics['total_docs_retrieved'])

# elif page == "ğŸ“Š Consultas en Lote":
#     show_batch_queries_page()  # Module doesn't exist

elif page == "ğŸ”¬ ComparaciÃ³n de Modelos":
    show_comparison_page()

elif page == "ğŸ”„ Comparador Pregunta vs Respuesta":
    show_question_answer_comparison_page()

elif page == "ğŸ“Š AnÃ¡lisis Acumulativo N Preguntas":
    show_cumulative_comparison_page()

elif page == "ğŸ“Š Visualizaciones CapÃ­tulo 4":
    show_chapter_4_visualizations()

elif page == "ğŸ“Š Figuras CapÃ­tulo 7":
    show_chapter_7_figures()

elif page == "ğŸ“ˆ AnÃ¡lisis de Datos":
    show_data_analysis_page()

elif page == "âš™ï¸ ConfiguraciÃ³n MÃ©tricas Acumulativas":
    show_cumulative_metrics_create_page()
elif page == "ğŸ“ˆ Resultados MÃ©tricas Acumulativas":
    show_cumulative_metrics_results_page()
elif page == "ğŸ“Š Resultados Matplotlib":
    show_cumulative_metrics_results_matplotlib_page()

# elif page == "ğŸ”§ ConfiguraciÃ³n AnÃ¡lisis N Preguntas (Colab)":
#     show_cumulative_n_questions_config_page()  # Module doesn't exist
# elif page == "ğŸ“Š Resultados AnÃ¡lisis N Preguntas (Colab)":
#     show_cumulative_n_questions_results_page()  # Module doesn't exist

# Footer comÃºn
st.markdown("---")
st.markdown("""
<div style="text-align: center; color: #666;">
    <p>ğŸ’¡ <strong>Tip:</strong> Para mejores resultados, sÃ© especÃ­fico en tus preguntas e incluye el servicio de Azure de interÃ©s.</p>
    <p>ğŸ”§ Sistema desarrollado con ChromaDB + sentence-transformers</p>
</div>
""", unsafe_allow_html=True)
