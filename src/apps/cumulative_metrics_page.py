"""
P√°gina de M√©tricas Acumulativas - Eval√∫a m√∫ltiples preguntas y calcula promedios (REFACTORIZADA)
"""

import streamlit as st
import time
import json
import os
from typing import List, Dict, Any
from src.config.config import EMBEDDING_MODELS, GENERATIVE_MODELS, CHROMADB_COLLECTION_CONFIG

# Importar utilidades refactorizadas
from src.data.memory_utils import get_memory_usage, cleanup_memory
# from utils.data_processing import filter_questions_with_links  # No longer needed
from src.ui.metrics_display import display_cumulative_metrics, display_models_comparison
from src.ui.enhanced_metrics_display import display_enhanced_cumulative_metrics, display_enhanced_models_comparison
from src.evaluation.cumulative import run_cumulative_metrics_evaluation, run_cumulative_metrics_for_models
from src.data.file_utils import display_download_section
from src.evaluation.metrics import validate_data_integrity
from src.services.storage.real_gdrive_integration import (
    show_gdrive_status, create_evaluation_config_in_drive,
    check_evaluation_status_in_drive, get_evaluation_results_from_drive,
    show_gdrive_authentication_instructions, show_gdrive_debug_info,
    get_all_results_files_from_drive, get_specific_results_file_from_drive
)


def show_cumulative_metrics_page():
    """Funci√≥n principal que muestra la p√°gina de m√©tricas acumulativas."""
    st.title("üìä M√©tricas Acumulativas de Recuperaci√≥n RAG")
    st.markdown("""
    Esta p√°gina permite evaluar m√∫ltiples preguntas y calcular m√©tricas promedio para diferentes modelos de embedding.
    """)
    
    # Variables globales para la configuraci√≥n
    use_colab_processing = False
    
    # Configuraci√≥n inicial
    col1, col2 = st.columns(2)
    
    with col1:
        st.subheader("‚öôÔ∏è Configuraci√≥n de Evaluaci√≥n")
        
        # Informaci√≥n sobre la fuente de datos
        st.info("üìä Las preguntas se extraen aleatoriamente desde ChromaDB, filtrando solo aquellas que tienen enlaces de Microsoft Learn en la respuesta aceptada.")
        
        # N√∫mero de preguntas
        num_questions = st.number_input(
            "üî¢ N√∫mero de preguntas a evaluar:",
            min_value=1,
            max_value=2000,
            value=600,
            step=1,
            help="N√∫mero total de preguntas para la evaluaci√≥n"
        )
        
        # Configuraci√≥n de modelo generativo
        generative_model_name = st.selectbox(
            "ü§ñ Modelo Generativo:",
            list(GENERATIVE_MODELS.keys()),
            index=list(GENERATIVE_MODELS.keys()).index("gpt-4") if "gpt-4" in GENERATIVE_MODELS else 0,
            help="Modelo usado para reranking LLM"
        )
        
        # Configuraci√≥n de recuperaci√≥n
        top_k = st.number_input(
            "üîù Top-K documentos:",
            min_value=1,
            max_value=50,
            value=10,
            step=1,
            help="N√∫mero m√°ximo de documentos a recuperar"
        )
        
        # Usar reranking LLM
        use_llm_reranker = st.checkbox(
            "ü§ñ Usar Reranking LLM",
            value=True,
            help="Activar reordenamiento de documentos con LLM"
        )
        
        # NUEVO: Opci√≥n de usar Google Colab
        use_colab_processing = st.checkbox(
            "‚òÅÔ∏è Procesamiento en Google Colab",
            value=True,
            help="Exportar evaluaci√≥n a Google Colab para procesamiento con GPU (m√°s r√°pido)"
        )
        
        if use_colab_processing:
            st.info("üìã Con Colab: Se exportar√°n los datos, obtendr√°s un notebook para ejecutar en Colab con GPU, y luego importar√°s los resultados.")
            
            # Mostrar estado de Google Drive
            gdrive_ok = show_gdrive_status()
            
            if not gdrive_ok:
                show_gdrive_authentication_instructions()
                st.stop()
        
        # Tama√±o de lote
        batch_size = st.number_input(
            "üì¶ Tama√±o de lote:",
            min_value=10,
            max_value=100,
            value=50,
            step=10,
            help="N√∫mero de preguntas a procesar por lote (para gesti√≥n de memoria)"
        )
    
    with col2:
        st.subheader("üéØ Selecci√≥n de Modelos")
        
        # Opci√≥n para evaluar todos los modelos
        evaluate_all_models = st.checkbox(
            "üîÑ Evaluar todos los modelos",
            value=True,
            help="Evaluar todos los modelos de embedding disponibles"
        )
        
        if evaluate_all_models:
            selected_models = list(EMBEDDING_MODELS.keys())
            st.info(f"üìã Se evaluar√°n {len(selected_models)} modelos")
            for model in selected_models:
                st.markdown(f"‚Ä¢ {model}")
        else:
            # Selecci√≥n manual de modelos
            selected_models = st.multiselect(
                "üéõÔ∏è Modelos de Embedding:",
                list(EMBEDDING_MODELS.keys()),
                default=[list(EMBEDDING_MODELS.keys())[0]],
                help="Selecciona uno o m√°s modelos para evaluar"
            )
        
        if not selected_models:
            st.error("‚ùå Debes seleccionar al menos un modelo")
            return
        
        # Informaci√≥n sobre memoria estimada
        estimated_memory = len(selected_models) * num_questions * 0.1  # MB aproximados
        st.info(f"üíæ Memoria estimada: ~{estimated_memory:.1f} MB")
    
    # Botones de control
    st.markdown("---")
    
    if use_colab_processing:
        # Interfaz para flujo Google Colab
        show_colab_workflow(
            num_questions, selected_models, 
            generative_model_name, top_k, use_llm_reranker, 
            batch_size, evaluate_all_models
        )
    else:
        # Interfaz para evaluaci√≥n local
        col1, col2, col3 = st.columns(3)
        
        with col1:
            if st.button("üöÄ Ejecutar Evaluaci√≥n", type="primary"):
                run_evaluation(
                    num_questions, selected_models, 
                    generative_model_name, top_k, use_llm_reranker, 
                    batch_size, evaluate_all_models
                )
        
        with col2:
            if st.button("üßπ Limpiar Cach√©"):
                for key in list(st.session_state.keys()):
                    if 'cumulative_results' in key:
                        del st.session_state[key]
                st.success("‚úÖ Cach√© limpiado")
                st.rerun()
        
        with col3:
            if st.button("üìä Mostrar Estad√≠sticas de Memoria"):
                show_memory_stats()
    
    # Mostrar resultados si existen
    show_cached_results()


def run_evaluation(num_questions: int, selected_models: List[str],
                  generative_model_name: str, top_k: int, use_llm_reranker: bool,
                  batch_size: int, evaluate_all_models: bool):
    """Ejecuta la evaluaci√≥n de m√©tricas."""
    
    # Mostrar informaci√≥n de memoria inicial
    initial_memory = get_memory_usage()
    st.info(f"üíæ Memoria inicial: {initial_memory:.1f} MB")
    
    # Ejecutar evaluaci√≥n
    start_time = time.time()
    evaluation_time = time.strftime("%Y-%m-%d %H:%M:%S")
    
    try:
        if evaluate_all_models:
            with st.spinner(f"üîÑ Evaluando {len(selected_models)} modelos..."):
                results = run_cumulative_metrics_for_models(
                    num_questions=num_questions,
                    model_names=selected_models,
                    generative_model_name=generative_model_name,
                    top_k=top_k,
                    use_llm_reranker=use_llm_reranker,
                    batch_size=batch_size
                )
        else:
            # Evaluaci√≥n de modelo √∫nico
            model_name = selected_models[0]
            with st.spinner(f"‚öôÔ∏è Evaluando {model_name}..."):
                single_result = run_cumulative_metrics_evaluation(
                    num_questions=num_questions,
                    model_name=model_name,
                    generative_model_name=generative_model_name,
                    top_k=top_k,
                    use_llm_reranker=use_llm_reranker,
                    batch_size=batch_size
                )
                results = {model_name: single_result}
        
        # Validar integridad de datos
        for model_name, result in results.items():
            results[model_name] = validate_data_integrity(result)
        
        end_time = time.time()
        execution_time = end_time - start_time
        
        # Guardar en cach√©
        cache_key = f"cumulative_results_{len(selected_models)}_{num_questions}_{int(start_time)}"
        st.session_state[cache_key] = {
            'results': results,
            'evaluation_time': evaluation_time,
            'execution_time': execution_time,
            'evaluate_all_models': evaluate_all_models,
            'params': {
                'num_questions': num_questions,
                'selected_models': selected_models,
                'embedding_model_name': selected_models[0] if len(selected_models) == 1 else 'Multi-Model',
                'generative_model_name': generative_model_name,
                'top_k': top_k,
                'use_llm_reranker': use_llm_reranker,
                'batch_size': batch_size
            }
        }
        
        st.success(f"‚úÖ Evaluaci√≥n completada en {execution_time:.2f} segundos")
        
        # Mostrar memoria final
        final_memory = get_memory_usage()
        memory_increase = final_memory - initial_memory
        st.info(f"üíæ Memoria final: {final_memory:.1f} MB (+{memory_increase:.1f} MB)")
        
        # Forzar rerun para mostrar resultados
        st.rerun()
        
    except Exception as e:
        st.error(f"‚ùå Error durante la evaluaci√≥n: {str(e)}")
        st.exception(e)
    finally:
        # Limpieza de memoria
        cleanup_memory()


def show_colab_workflow(num_questions: int, selected_models: List[str],
                       generative_model_name: str, top_k: int, use_llm_reranker: bool,
                       batch_size: int, evaluate_all_models: bool):
    """Muestra la interfaz completa del flujo Google Colab"""
    
    st.subheader("‚òÅÔ∏è Flujo de Evaluaci√≥n con Google Colab")
    
    # Configuraci√≥n de la evaluaci√≥n
    evaluation_config = {
        'num_questions': num_questions,
        'selected_models': selected_models,
        'generative_model_name': generative_model_name,
        'top_k': top_k,
        'use_llm_reranker': use_llm_reranker,
        'batch_size': batch_size,
        'evaluate_all_models': evaluate_all_models,
        'evaluation_type': 'cumulative_metrics'
    }
    
    # Columnas para configuraci√≥n
    col1, col2 = st.columns(2)
    
    with col1:
        st.markdown("**üìä Configuraci√≥n de Evaluaci√≥n:**")
        st.json({k: v for k, v in evaluation_config.items()})
    
    with col2:
        st.markdown("**üìÅ Google Drive:**")
        st.info("Carpeta: `/TesisMagister/acumulative/`")
    
    # Secci√≥n de resultados disponibles (mostrar siempre)
    st.markdown("---")
    show_available_results_section()
    
    # Botones del flujo
    col1, col2, col3, col4 = st.columns(4)
    
    with col1:
        if st.button("üöÄ Crear Configuraci√≥n y Enviar a Google Drive", type="primary"):
            create_config_and_send_to_drive(evaluation_config)
    
    with col2:
        if st.button("üîÑ Verificar Estado"):
            check_colab_evaluation_status()
    
    with col3:
        if st.button("üîç Debug Google Drive"):
            st.markdown("---")
            show_gdrive_debug_info()
    
    with col4:
        # Placeholder para futuro bot√≥n
        st.empty()
    
    # Mostrar estado actual
    st.markdown("---")
    display_current_colab_status()


def create_config_and_send_to_drive(evaluation_config: Dict):
    """Crea y env√≠a configuraci√≥n a Google Drive real"""
    
    st.info("üì§ Creando configuraci√≥n y enviando a Google Drive...")
    
    try:
        # Obtener preguntas reales de la base de datos
        with st.spinner("üì• Obteniendo preguntas de ChromaDB..."):
            from src.data.processing import fetch_random_questions_from_chromadb
            from src.services.auth.clients import initialize_clients
            
            try:
                # Usar el primer modelo seleccionado para obtener las preguntas
                first_model = evaluation_config['selected_models'][0]
                chromadb_wrapper, embedding_client, openai_client, gemini_client, local_tinyllama_client, local_mistral_client, openrouter_client, client = initialize_clients(
                    model_name=first_model,
                    generative_model_name=evaluation_config['generative_model_name']
                )
                questions = fetch_random_questions_from_chromadb(
                    chromadb_wrapper=chromadb_wrapper,
                    embedding_model_name=first_model,
                    num_questions=evaluation_config['num_questions']
                )
                
                if questions:
                    evaluation_config['questions_data'] = questions
                    st.success(f"‚úÖ Obtenidas {len(questions)} preguntas con enlaces MS Learn")
                else:
                    st.warning("‚ö†Ô∏è No se encontraron preguntas, usando configuraci√≥n sin datos")
                    evaluation_config['questions_data'] = None
                    
            except Exception as e:
                st.warning(f"‚ö†Ô∏è Error obteniendo preguntas: {e}")
                evaluation_config['questions_data'] = None
        
        # Enviar a Google Drive real
        with st.spinner("‚òÅÔ∏è Enviando configuraci√≥n a Google Drive..."):
            result = create_evaluation_config_in_drive(evaluation_config)
            
            if result['success']:
                st.success("‚úÖ ¬°Configuraci√≥n enviada exitosamente a Google Drive!")
                
                col1, col2 = st.columns(2)
                
                with col1:
                    st.markdown("**üìÑ Archivo creado:**")
                    st.code(result['config_filename'])
                    st.markdown(f"[üîó Ver en Drive]({result['web_link']})")
                
                with col2:
                    st.markdown("**üìã Pr√≥ximos pasos:**")
                    st.markdown("""
                    1. Abrir Google Colab
                    2. Subir `Universal_Colab_Evaluator.ipynb` 
                    3. Activar GPU (T4)
                    4. Ejecutar todas las celdas
                    5. Volver aqu√≠ para ver resultados
                    """)
                
                # Bot√≥n para descargar notebook
                try:
                    with open('Universal_Colab_Evaluator.ipynb', 'r') as f:
                        notebook_content = f.read()
                    
                    st.download_button(
                        label="üìì Descargar Notebook Universal",
                        data=notebook_content,
                        file_name="Universal_Colab_Evaluator.ipynb",
                        mime="application/json"
                    )
                except FileNotFoundError:
                    st.warning("‚ö†Ô∏è Notebook no encontrado localmente")
                
                st.rerun()
                
            else:
                st.error(f"‚ùå Error enviando a Google Drive: {result['error']}")
                
    except Exception as e:
        st.error(f"‚ùå Error: {e}")


def check_colab_evaluation_status():
    """Verifica el estado de la evaluaci√≥n en Google Drive"""
    
    with st.spinner("üîç Verificando estado en Google Drive..."):
        result = check_evaluation_status_in_drive()
        
        if result['success']:
            status = result['status']
            
            if status == 'completed':
                st.success("‚úÖ ¬°Evaluaci√≥n completada en Google Colab!")
                
                data = result['data']
                col1, col2 = st.columns(2)
                
                with col1:
                    st.markdown("**üìä Resumen:**")
                    st.write(f"ü§ñ Modelos: {data.get('models_evaluated', 'N/A')}")
                    st.write(f"‚ùì Preguntas: {data.get('questions_processed', 'N/A')}")
                    st.write(f"‚è±Ô∏è Tiempo: {data.get('total_time_seconds', 0):.1f}s")
                    st.write(f"üöÄ GPU: {'‚úÖ' if data.get('gpu_used') else '‚ùå'}")
                
                with col2:
                    st.markdown("**üìÅ Archivos:**")
                    st.write(f"üìÑ {data.get('results_file', 'N/A')}")
                    st.write(f"üìä {data.get('summary_file', 'N/A')}")
                    
            elif status == 'config_created':
                st.info("üìã Configuraci√≥n creada. Esperando ejecuci√≥n en Colab...")
            elif status == 'no_status_file':
                st.warning("‚ö†Ô∏è No se encontr√≥ informaci√≥n de estado")
            else:
                st.info(f"üîÑ Estado actual: {status}")
                
        else:
            st.error(f"‚ùå Error verificando estado: {result['error']}")


def show_available_results_section():
    """Muestra la secci√≥n de resultados disponibles con dropdown de selecci√≥n"""
    
    st.subheader("üìä Resultados Disponibles")
    
    # Obtener archivos disponibles sin spinner para no interferir con la UI
    try:
        files_result = get_all_results_files_from_drive()
        
        if not files_result['success']:
            st.warning(f"üîç No se pudieron obtener archivos de resultados")
            st.error(f"‚ùå {files_result['error']}")
            
            # Mostrar informaci√≥n de debug si est√° disponible
            if 'debug_info' in files_result:
                debug_info = files_result['debug_info']
                with st.expander("üîç Informaci√≥n de Debug", expanded=True):
                    col1, col2 = st.columns(2)
                    with col1:
                        st.write(f"üìÅ **Folder ID:** `{debug_info.get('folder_id', 'N/A')}`")
                        st.write(f"üîç **B√∫squeda:** `{debug_info.get('search_query', 'N/A')}`")
                    with col2:
                        st.write(f"üìÑ **Total archivos:** {debug_info.get('total_files', 0)}")
                        st.write(f"üìã **Archivos JSON:** {debug_info.get('json_files', 0)}")
                    
                    # Mostrar algunos nombres de archivos para ayudar con debug
                    if 'all_files' in debug_info and debug_info['all_files']:
                        st.write("üìã **Algunos archivos JSON encontrados:**")
                        for filename in debug_info['all_files']:
                            st.write(f"- `{filename}`")
            
            # Bot√≥n para ejecutar debug completo
            if st.button("üîç Ejecutar Debug Completo de Google Drive"):
                st.markdown("---")
                show_gdrive_debug_info()
            
            st.info("üí° **Posibles soluciones:**")
            st.markdown("""
            1. üöÄ **Ejecuta una evaluaci√≥n en Colab** para generar archivos de resultados
            2. üîß **Verifica la conexi√≥n** con el bot√≥n 'Debug Google Drive' 
            3. üìÅ **Confirma la carpeta** acumulative en Google Drive
            4. üìã **Revisa el formato** de archivos (deben ser `cumulative_results_*.json`)
            """)
            return
        
        available_files = files_result['files']
        
        if not available_files:
            st.info("üì≠ No hay archivos de resultados disponibles")
            st.info("üí° Ejecuta una evaluaci√≥n en Colab para generar resultados")
            return
            
    except Exception as e:
        st.warning(f"‚ö†Ô∏è Error verificando archivos: {str(e)}")
        return
    
    # Mostrar informaci√≥n de archivos disponibles
    st.success(f"‚úÖ Encontrados {len(available_files)} archivos de resultados")
    
    # Crear dropdown con todos los archivos disponibles
    file_options = {file_info['display_name']: file_info['file_id'] for file_info in available_files}
    
    # Usar session_state para mantener la selecci√≥n
    if 'selected_results_file' not in st.session_state:
        st.session_state.selected_results_file = list(file_options.keys())[0]
    
    selected_display_name = st.selectbox(
        "üìÅ Seleccionar archivo de resultados:",
        options=list(file_options.keys()),
        index=list(file_options.keys()).index(st.session_state.selected_results_file) if st.session_state.selected_results_file in file_options else 0,
        help="Los archivos est√°n ordenados por fecha de modificaci√≥n (m√°s reciente primero)",
        key="results_file_selector"
    )
    
    # Actualizar session_state
    st.session_state.selected_results_file = selected_display_name
    selected_file_id = file_options[selected_display_name]
    
    # Bot√≥n para mostrar resultados del archivo seleccionado
    col1, col2 = st.columns([3, 1])
    
    with col1:
        if st.button("üìä Mostrar Resultados del Archivo Seleccionado", type="primary", use_container_width=True):
            load_and_display_selected_results(selected_file_id)
    
    with col2:
        if st.button("üîÑ Actualizar Lista", help="Buscar nuevos archivos de resultados"):
            st.rerun()


def load_and_display_selected_results(file_id: str):
    """Carga y muestra los resultados del archivo seleccionado"""
    
    with st.spinner("üìä Cargando resultados del archivo seleccionado..."):
        result = get_specific_results_file_from_drive(file_id)
        
        if not result['success']:
            st.error(f"‚ùå Error cargando archivo: {result['error']}")
            return
        
        st.success("‚úÖ Resultados cargados exitosamente!")
        
        # Usar un expander para los resultados para que no ocupen todo el espacio
        with st.expander("üìä Resultados de Evaluaci√≥n", expanded=True):
            display_results_content(result['results'])


def display_results_content(results_data):
    """Muestra el contenido de los resultados de evaluaci√≥n"""
    
    # Debug: Mostrar estructura de datos si est√° habilitado
    if st.checkbox("üîç Mostrar estructura de datos (debug)", value=False):
        with st.expander("üìã Estructura de datos completa"):
            st.json(results_data)
    
    # Convertir formato para compatibilidad con funciones existentes
    processed_results = {}
    for model_name, model_data in results_data['results'].items():
        processed_results[model_name] = model_data
    
    # Generar visualizaciones usando funciones existentes
    # Extraer informaci√≥n de tiempo y configuraci√≥n
    evaluation_info = results_data.get('evaluation_info', {})
    config_info = results_data.get('config', {})
    
    # Formatear fecha y hora
    timestamp = evaluation_info.get('timestamp')
    if timestamp:
        try:
            from datetime import datetime
            if isinstance(timestamp, str):
                # Intentar parsear diferentes formatos de timestamp
                if 'T' in timestamp:
                    dt = datetime.fromisoformat(timestamp.replace('Z', '+00:00'))
                else:
                    dt = datetime.strptime(timestamp, '%Y-%m-%d %H:%M:%S')
            else:
                dt = datetime.fromtimestamp(timestamp)
            
            formatted_date = dt.strftime('%d/%m/%Y %H:%M:%S')
        except:
            formatted_date = str(timestamp)
    else:
        formatted_date = "Fecha no disponible"
    
    # Obtener n√∫mero de preguntas
    num_questions = config_info.get('num_questions', evaluation_info.get('questions_processed', 'N/A'))
    
    # Obtener m√©todo de autenticaci√≥n si est√° disponible
    auth_method = evaluation_info.get('auth_method', '')
    auth_icon = 'üîê' if auth_method == 'API' else 'üìÅ' if auth_method == 'Mount' else ''
    
    # Mostrar header con informaci√≥n detallada
    st.subheader(f"üìä Resultados de Evaluaci√≥n Acumulativa")
    
    # Informaci√≥n de la evaluaci√≥n en una caja destacada
    col1, col2, col3 = st.columns(3)
    
    with col1:
        st.info(f"üìÖ **Fecha:** {formatted_date}")
    
    with col2:
        st.info(f"‚ùì **Preguntas:** {num_questions}")
    
    with col3:
        models_count = len(results_data['results'])
        st.info(f"ü§ñ **Modelos:** {models_count}")
    
    # Informaci√≥n adicional si est√° disponible
    if evaluation_info.get('total_time_seconds'):
        total_time = evaluation_info['total_time_seconds']
        if total_time >= 60:
            time_str = f"{total_time/60:.1f} min"
        else:
            time_str = f"{total_time:.1f}s"
        
        gpu_used = evaluation_info.get('gpu_used', False)
        gpu_icon = 'üöÄ' if gpu_used else 'üíª'
        
        st.caption(f"‚è±Ô∏è Tiempo de ejecuci√≥n: {time_str} {gpu_icon} | {auth_icon} {auth_method}")
    
    st.markdown("---")
    
    # Determinar si usar reranker desde config
    use_llm_reranker = results_data['config'].get('use_llm_reranker', True)
    
    if len(processed_results) == 1:
        # Para un solo modelo, usar display_enhanced_cumulative_metrics
        model_name = list(processed_results.keys())[0]
        model_results = processed_results[model_name]
        
        # Adaptar formato seg√∫n la estructura de datos disponible
        # Nuevo formato del notebook actualizado tiene avg_before_metrics y avg_after_metrics
        if 'avg_before_metrics' in model_results and 'avg_after_metrics' in model_results:
            # Formato actualizado con before/after metrics
            adapted_results = {
                'num_questions_evaluated': model_results.get('num_questions_evaluated', results_data['config']['num_questions']),
                'avg_before_metrics': model_results['avg_before_metrics'],
                'avg_after_metrics': model_results['avg_after_metrics'],
                'individual_metrics': model_results.get('individual_before_metrics', [])
            }
            
            # Verificar si realmente hay m√©tricas after
            if use_llm_reranker and not model_results['avg_after_metrics']:
                st.info("‚ÑπÔ∏è Reranking LLM estaba habilitado en configuraci√≥n, pero no se generaron m√©tricas after. Mostrando solo m√©tricas before.")
                use_llm_reranker = False
                
        else:
            # Formato anterior - solo avg_metrics (mantener compatibilidad)
            avg_metrics = model_results.get('avg_metrics', {})
            
            adapted_results = {
                'num_questions_evaluated': results_data['config']['num_questions'],
                'avg_before_metrics': avg_metrics,  # Usar las m√©tricas como "before" 
                'avg_after_metrics': {},  # Vac√≠o porque no hay reranking
                'individual_metrics': model_results.get('individual_metrics', [])
            }
            
            if use_llm_reranker:
                st.warning("‚ö†Ô∏è Reranking LLM estaba habilitado pero estos resultados usan formato antiguo. Mostrando solo m√©tricas base.")
                use_llm_reranker = False
        
        # Use enhanced display with cleaner before/after LLM separation
        display_enhanced_cumulative_metrics(adapted_results, model_name, use_llm_reranker)
        
    else:
        # Para m√∫ltiples modelos, usar display_enhanced_models_comparison
        st.markdown("### üèÜ Comparaci√≥n de Modelos")
        
        # Adaptar formato para m√∫ltiples modelos seg√∫n estructura disponible
        adapted_multi_results = {}
        has_new_format = False
        
        for model_name, model_data in processed_results.items():
            # Verificar si usa el nuevo formato con before/after metrics
            if 'avg_before_metrics' in model_data and 'avg_after_metrics' in model_data:
                # Formato actualizado
                adapted_multi_results[model_name] = {
                    'num_questions_evaluated': model_data.get('num_questions_evaluated', results_data['config']['num_questions']),
                    'avg_before_metrics': model_data['avg_before_metrics'],
                    'avg_after_metrics': model_data['avg_after_metrics'],
                    'individual_metrics': model_data.get('individual_before_metrics', [])
                }
                has_new_format = True
            else:
                # Formato anterior - compatibilidad
                avg_metrics = model_data.get('avg_metrics', {})
                adapted_multi_results[model_name] = {
                    'num_questions_evaluated': results_data['config']['num_questions'],
                    'avg_before_metrics': avg_metrics,  # Usar m√©tricas como "before"
                    'avg_after_metrics': {},  # Vac√≠o porque no hay reranking
                    'individual_metrics': model_data.get('individual_metrics', [])
                }
        
        # Verificar si hay m√©tricas after disponibles para LLM reranking
        if use_llm_reranker:
            has_after_metrics = any(adapted_multi_results[model]['avg_after_metrics'] for model in adapted_multi_results)
            if not has_after_metrics:
                if has_new_format:
                    st.info("‚ÑπÔ∏è Reranking LLM estaba habilitado pero no se generaron m√©tricas after. Mostrando solo m√©tricas before.")
                else:
                    st.warning("‚ö†Ô∏è Reranking LLM estaba habilitado pero estos resultados usan formato antiguo. Mostrando solo m√©tricas base.")
                use_llm_reranker = False
        
        # Use enhanced display for cleaner multi-model comparison
        display_enhanced_models_comparison(adapted_multi_results, use_llm_reranker)
    
    # Secci√≥n de descarga
    st.markdown("---")
    
    # Preparar datos para la secci√≥n de descarga en el formato esperado
    cached_results = {
        'results': processed_results,
        'evaluation_time': results_data['evaluation_info'].get('timestamp'),
        'execution_time': results_data['evaluation_info'].get('total_time_seconds'),
        'evaluate_all_models': len(processed_results) > 1,
        'params': {
            'num_questions': results_data['config']['num_questions'],
            'selected_models': list(processed_results.keys()),
            'embedding_model_name': list(processed_results.keys())[0] if len(processed_results) == 1 else 'Multi-Model',
            'generative_model_name': results_data['config']['generative_model_name'],
            'top_k': results_data['config']['top_k'],
            'use_llm_reranker': results_data['config']['use_llm_reranker'],
            'batch_size': results_data['config']['batch_size']
        }
    }
    
    display_download_section(cached_results)




def display_current_colab_status():
    """Muestra el estado actual del sistema Google Colab"""
    
    st.subheader("üìä Estado Actual")
    
    # Verificar estado sin spinner (solo para mostrar info)
    try:
        result = check_evaluation_status_in_drive()
        
        if result['success']:
            status = result['status']
            
            if status == 'completed':
                st.success("‚úÖ Evaluaci√≥n completada - Lista para mostrar resultados")
            elif status == 'config_created':
                st.info("üìã Configuraci√≥n lista - Ejecutar en Google Colab")
            elif status == 'no_status_file':
                st.info("‚ö™ Sin evaluaciones pendientes")
            else:
                st.info(f"üîÑ Estado: {status}")
        else:
            st.warning("‚ö†Ô∏è No se pudo verificar estado")
            
    except Exception as e:
        st.warning(f"‚ö†Ô∏è Error verificando estado: {e}")


def create_drive_config_file(evaluation_config: Dict) -> bool:
    """Crea archivo de configuraci√≥n simulando Google Drive"""
    
    try:
        import os
        
        # Simular la estructura de carpetas de Google Drive localmente
        # En producci√≥n, esto se conectar√≠a realmente a Google Drive
        local_drive_path = "/Users/haroldgomez/Documents/ProyectoTituloMAgister/SupportModel/simulated_drive"
        config_path = f"{local_drive_path}/evaluation_config.json"
        
        # Crear directorio si no existe
        os.makedirs(local_drive_path, exist_ok=True)
        
        # Guardar configuraci√≥n
        with open(config_path, 'w', encoding='utf-8') as f:
            json.dump(evaluation_config, f, indent=2, ensure_ascii=False)
        
        st.success(f"‚úÖ Configuraci√≥n guardada en: {config_path}")
        st.info("üìù En producci√≥n, esto se guardar√≠a en `/content/drive/MyDrive/TesisMagister/acumulative/evaluation_config.json`")
        
        return True
        
    except Exception as e:
        st.error(f"‚ùå Error creando archivo de configuraci√≥n: {e}")
        return False


def show_colab_status_and_results():
    """Muestra el estado de la evaluaci√≥n en Colab y bot√≥n para mostrar resultados"""
    
    st.subheader("üìä Estado de la Evaluaci√≥n en Colab")
    
    # Verificar si existe archivo de status
    status_file = "/Users/haroldgomez/Documents/ProyectoTituloMAgister/SupportModel/simulated_drive/evaluation_status.json"
    results_dir = "/Users/haroldgomez/Documents/ProyectoTituloMAgister/SupportModel/simulated_drive/results"
    
    if os.path.exists(status_file):
        try:
            with open(status_file, 'r', encoding='utf-8') as f:
                status_data = json.load(f)
            
            if status_data.get('status') == 'completed':
                st.success("‚úÖ ¬°Evaluaci√≥n completada en Google Colab!")
                
                col1, col2 = st.columns(2)
                
                with col1:
                    st.markdown("**üìä Resumen de Resultados:**")
                    st.write(f"ü§ñ Modelos evaluados: {status_data.get('models_evaluated', 'N/A')}")
                    st.write(f"‚ùì Preguntas procesadas: {status_data.get('questions_processed', 'N/A')}")
                    st.write(f"‚è±Ô∏è Tiempo total: {status_data.get('total_time_seconds', 0):.2f}s")
                    st.write(f"üöÄ GPU utilizada: {'‚úÖ' if status_data.get('gpu_used') else '‚ùå'}")
                
                with col2:
                    st.markdown("**üìÅ Archivos generados:**")
                    st.write(f"üìÑ {status_data.get('results_file', 'N/A')}")
                    st.write(f"üìä {status_data.get('summary_file', 'N/A')}")
                
                # Bot√≥n para mostrar resultados
                if st.button("üìä Mostrar Resultados y Generar Visualizaciones", type="primary"):
                    show_colab_results_and_generate_visuals(status_data)
                
            else:
                st.info(f"üîÑ Estado: {status_data.get('status', 'unknown')}")
        
        except Exception as e:
            st.error(f"‚ùå Error leyendo estado: {e}")
    
    else:
        st.info("‚è≥ Esperando resultados de Google Colab...")
        st.markdown("**üìã Para que aparezcan los resultados:**")
        st.markdown("1. Ejecuta el notebook en Google Colab")
        st.markdown("2. Espera a que termine la evaluaci√≥n")
        st.markdown("3. Los resultados aparecer√°n autom√°ticamente aqu√≠")
        
        # Bot√≥n para refrescar estado
        if st.button("üîÑ Verificar Estado"):
            st.rerun()




def generate_colab_notebook_code(config: Dict) -> str:
    """Genera el c√≥digo completo para ejecutar en Colab."""
    
    models_str = ', '.join([f'"{m}"' for m in config['selected_models']])
    
    code = f'''# üöÄ Evaluaci√≥n de Embeddings en Google Colab con GPU
# Generado autom√°ticamente el {config['timestamp']}
# Configuraci√≥n: {config['num_questions']} preguntas, {len(config['selected_models'])} modelos

import os
import time
import json
import random
import pandas as pd
import numpy as np
from datetime import datetime
from typing import Dict, List, Any

# üìä Configuraci√≥n de la evaluaci√≥n
EVALUATION_CONFIG = {{
    'num_questions': {config['num_questions']},
    'selected_models': [{models_str}],
    'generative_model_name': "{config['generative_model_name']}",
    'top_k': {config['top_k']},
    'use_llm_reranker': {config['use_llm_reranker']},
    'batch_size': {config['batch_size']},
    'evaluate_all_models': {config['evaluate_all_models']},
    'timestamp': "{config['timestamp']}"
}}

print("üöÄ Iniciando evaluaci√≥n de embeddings en Google Colab")
print("üìä Configuraci√≥n:")
for key, value in EVALUATION_CONFIG.items():
    print(f"   {{key}}: {{value}}")

# ‚úÖ 1. Verificar GPU
print("\\nüîß Verificando hardware disponible...")
try:
    import torch
    print(f"CUDA disponible: {{torch.cuda.is_available()}}")
    if torch.cuda.is_available():
        print(f"GPU: {{torch.cuda.get_device_name(0)}}")
        print(f"Memoria GPU: {{torch.cuda.get_device_properties(0).total_memory / 1e9:.1f}} GB")
        print("‚úÖ GPU T4 detectada - procesamiento acelerado habilitado!")
    else:
        print("‚ö†Ô∏è  GPU no disponible - usando CPU (m√°s lento)")
except ImportError:
    print("‚ö†Ô∏è  PyTorch no instalado - se instalar√° en el siguiente paso")

# üì¶ 2. Instalar dependencias necesarias
print("\\nüì¶ Instalando dependencias...")
!pip install -q sentence-transformers pandas numpy scikit-learn openai python-dotenv tqdm

# üìö 3. Importar librer√≠as
print("\\nüìö Importando librer√≠as...")
try:
    from sentence_transformers import SentenceTransformer
    from sklearn.metrics.pairwise import cosine_similarity
    from tqdm.auto import tqdm
    import warnings
    warnings.filterwarnings('ignore')
    print("‚úÖ Librer√≠as importadas correctamente")
except ImportError as e:
    print(f"‚ùå Error importando librer√≠as: {{e}}")
    print("üí° Reinicia el runtime y vuelve a ejecutar")

# üé≤ 4. Generar datos de prueba (simulando tu base de datos)
print("\\nüé≤ Generando datos de prueba para demostraci√≥n...")

def generate_sample_questions(num_questions: int) -> List[Dict]:
    """Genera preguntas de ejemplo que simulan tu base de datos"""
    
    sample_questions = [
        "¬øC√≥mo configurar Azure Storage Blob?",
        "¬øCu√°l es la diferencia entre SQL Database y Cosmos DB?",
        "¬øC√≥mo implementar autenticaci√≥n en Azure Functions?",
        "¬øQu√© es Azure Container Instances?",
        "¬øC√≥mo usar Azure DevOps para CI/CD?",
        "¬øCu√°les son las mejores pr√°cticas para Azure Security?",
        "¬øC√≥mo configurar Application Insights?",
        "¬øQu√© es Azure Service Bus?",
        "¬øC√≥mo usar Azure Logic Apps?",
        "¬øCu√°l es la diferencia entre VM y App Service?",
        "¬øC√≥mo configurar Azure Active Directory?",
        "¬øQu√© es Azure Kubernetes Service?",
        "¬øC√≥mo usar Azure Key Vault?",
        "¬øCu√°les son los tipos de Azure Storage?",
        "¬øC√≥mo implementar Azure API Management?"
    ]
    
    # Expandir la lista repitiendo y variando
    questions = []
    for i in range(num_questions):
        base_question = sample_questions[i % len(sample_questions)]
        # A√±adir variaci√≥n
        variations = [
            f"{{base_question}}",
            f"Tutorial: {{base_question}}",
            f"Gu√≠a paso a paso: {{base_question}}",
            f"Mejores pr√°cticas: {{base_question}}",
            f"Soluci√≥n de problemas: {{base_question}}"
        ]
        
        question_text = variations[i % len(variations)]
        
        questions.append({{'question': question_text, 'id': f'q_{{i+1}}'}})
    
    return questions

# Generar preguntas de prueba
test_questions = generate_sample_questions(EVALUATION_CONFIG['num_questions'])
print(f"‚úÖ Generadas {{len(test_questions)}} preguntas de prueba")

# ü§ñ 5. Funci√≥n de evaluaci√≥n acelerada con GPU
def run_gpu_accelerated_evaluation(questions: List[Dict], models: List[str]) -> Dict:
    """Ejecuta evaluaci√≥n usando GPU para m√°ximo rendimiento"""
    
    print(f"\\nüöÄ Iniciando evaluaci√≥n acelerada...")
    print(f"üìä Preguntas: {{len(questions)}}")
    print(f"ü§ñ Modelos: {{models}}")
    
    results = {{}}
    
    for model_name in tqdm(models, desc="Evaluando modelos"):
        print(f"\\n‚öôÔ∏è Procesando modelo: {{model_name}}")
        
        # Simular carga del modelo
        model_start = time.time()
        
        # En una implementaci√≥n real, aqu√≠ cargar√≠as el modelo:
        # model = SentenceTransformer(model_name)
        # if torch.cuda.is_available():
        #     model = model.to('cuda')
        
        print(f"   üì• Modelo cargado en {{time.time() - model_start:.2f}}s")
        
        # Simular procesamiento por lotes
        batch_size = EVALUATION_CONFIG['batch_size']
        batch_results = []
        
        for i in tqdm(range(0, len(questions), batch_size), desc=f"Lotes {{model_name}}", leave=False):
            batch_questions = questions[i:i+batch_size]
            
            # Simular embeddings y m√©tricas (en implementaci√≥n real usar√≠as el modelo)
            batch_metrics = {{
                'precision': random.uniform(0.65, 0.95),
                'recall': random.uniform(0.60, 0.90),
                'f1': random.uniform(0.62, 0.92),
                'map': random.uniform(0.55, 0.88),
                'mrr': random.uniform(0.60, 0.92),
                'ndcg': random.uniform(0.65, 0.95)
            }}
            
            batch_results.append(batch_metrics)
            
            # Simular tiempo de procesamiento GPU
            time.sleep(0.01)  # Simular procesamiento r√°pido con GPU
        
        # Calcular m√©tricas promedio
        avg_metrics = {{}}
        for metric in ['precision', 'recall', 'f1', 'map', 'mrr', 'ndcg']:
            values = [br[metric] for br in batch_results]
            avg_metrics[f'avg_{{metric}}'] = np.mean(values)
        
        # Guardar resultados del modelo
        model_results = {{
            'model_name': model_name,
            'avg_metrics': avg_metrics,
            'total_questions': len(questions),
            'batch_size': batch_size,
            'processing_time_seconds': time.time() - model_start,
            'evaluation_time': datetime.now().isoformat(),
            'gpu_accelerated': torch.cuda.is_available() if 'torch' in globals() else False
        }}
        
        results[model_name] = model_results
        
        print(f"   ‚úÖ {{model_name}} completado - F1: {{avg_metrics['avg_f1']:.3f}}")
    
    return results

# üöÄ 6. Ejecutar evaluaci√≥n completa
print("\\n" + "="*60)
print("üöÄ EJECUTANDO EVALUACI√ìN COMPLETA")
print("="*60)

start_time = time.time()

try:
    # Ejecutar evaluaci√≥n
    evaluation_results = run_gpu_accelerated_evaluation(
        test_questions, 
        EVALUATION_CONFIG['selected_models']
    )
    
    total_time = time.time() - start_time
    
    # üíæ 7. Guardar resultados
    print(f"\\nüíæ Guardando resultados...")
    
    # Archivo JSON completo
    output_file = f"cumulative_results_colab_{{int(time.time())}}.json"
    
    final_results = {{
        'config': EVALUATION_CONFIG,
        'results': evaluation_results,
        'execution_summary': {{
            'total_time_seconds': total_time,
            'questions_processed': len(test_questions),
            'models_evaluated': len(EVALUATION_CONFIG['selected_models']),
            'gpu_used': torch.cuda.is_available() if 'torch' in globals() else False,
            'timestamp': datetime.now().isoformat()
        }}
    }}
    
    with open(output_file, 'w', encoding='utf-8') as f:
        json.dump(final_results, f, indent=2, ensure_ascii=False)
    
    # Archivo CSV resumen
    csv_data = []
    for model_name, results in evaluation_results.items():
        metrics = results['avg_metrics']
        row = {{'Model': model_name}}
        row.update({{k.replace('avg_', '').upper(): f"{{v:.4f}}" for k, v in metrics.items()}})
        row['Processing_Time'] = f"{{results['processing_time_seconds']:.2f}}s"
        csv_data.append(row)
    
    df_summary = pd.DataFrame(csv_data)
    csv_file = f"results_summary_{{int(time.time())}}.csv"
    df_summary.to_csv(csv_file, index=False)
    
    # üìä 8. Mostrar resumen final
    print("\\n" + "="*60)
    print("üìä RESUMEN DE RESULTADOS")
    print("="*60)
    
    print(f"‚è±Ô∏è  Tiempo total: {{total_time:.2f}} segundos")
    print(f"üìä Preguntas procesadas: {{len(test_questions):,}}")
    print(f"ü§ñ Modelos evaluados: {{len(EVALUATION_CONFIG['selected_models'])}}")
    print(f"üöÄ GPU acelerado: {{'‚úÖ S√≠' if torch.cuda.is_available() if 'torch' in globals() else False else '‚ùå No'}}")
    
    print(f"\\nüèÜ RANKING DE MODELOS:")
    print("-"*50)
    
    # Ordenar modelos por F1-Score
    model_ranking = sorted(
        evaluation_results.items(),
        key=lambda x: x[1]['avg_metrics']['avg_f1'],
        reverse=True
    )
    
    for i, (model_name, results) in enumerate(model_ranking, 1):
        metrics = results['avg_metrics']
        print(f"{{i}}. {{model_name}}")
        print(f"   F1: {{metrics['avg_f1']:.4f}} | MAP: {{metrics['avg_map']:.4f}} | MRR: {{metrics['avg_mrr']:.4f}}")
    
    print(f"\\nüìÅ ARCHIVOS GENERADOS:")
    print("-"*30)
    print(f"üìÑ {{output_file}} (resultados completos)")
    print(f"üìä {{csv_file}} (resumen CSV)")
    
    print(f"\\nüíæ Para descargar:")
    print("1. Haz clic en la carpeta üìÅ en el panel izquierdo")
    print("2. Busca los archivos generados")
    print("3. Haz clic derecho ‚Üí Download")
    
    print(f"\\nüéâ ¬°EVALUACI√ìN COMPLETADA EXITOSAMENTE!")
    print("‚úÖ Importa estos archivos en tu aplicaci√≥n Streamlit local")
    
except Exception as e:
    print(f"\\n‚ùå Error durante la evaluaci√≥n: {{e}}")
    import traceback
    traceback.print_exc()
    print("\\nüí° Revisa los errores y vuelve a ejecutar")

print("\\n" + "="*60)
print("üéØ PROCESO FINALIZADO")
print("="*60)
'''

    return code


def show_cached_results():
    """Muestra los resultados cacheados si existen."""
    cached_keys = [k for k in st.session_state.keys() if 'cumulative_results' in k]
    
    if not cached_keys:
        st.info("‚ÑπÔ∏è No hay resultados de evaluaci√≥n. Ejecuta una evaluaci√≥n para ver los resultados.")
        return
    
    # Seleccionar qu√© resultados mostrar
    if len(cached_keys) > 1:
        st.subheader("üìã Resultados Disponibles")
        selected_key = st.selectbox(
            "Selecciona resultados:",
            cached_keys,
            format_func=lambda x: f"Evaluaci√≥n {x.split('_')[-1]} ({st.session_state[x]['params']['embedding_model_name']})"
        )
    else:
        selected_key = cached_keys[0]
    
    if selected_key not in st.session_state:
        return
    
    cached_results = st.session_state[selected_key]
    results = cached_results['results']
    evaluate_all_models = cached_results['evaluate_all_models']
    params = cached_results['params']
    
    st.markdown("---")
    
    # Mostrar resultados
    if evaluate_all_models:
        st.subheader("üìà Comparaci√≥n Multi-Modelo")
        display_models_comparison(results, params['use_llm_reranker'])
    else:
        st.subheader(f"üìä Resultados para {params['embedding_model_name']}")
        model_name = list(results.keys())[0]
        display_cumulative_metrics(results[model_name], model_name, params['use_llm_reranker'])
    
    # Secci√≥n de descarga
    st.markdown("---")
    display_download_section(cached_results)


def show_memory_stats():
    """Muestra estad√≠sticas detalladas de memoria."""
    current_memory = get_memory_usage()
    
    st.subheader("üìä Estad√≠sticas de Memoria")
    st.metric("Memoria Actual", f"{current_memory:.1f} MB")
    
    # Contar elementos en cach√©
    cache_count = len([k for k in st.session_state.keys() if 'cumulative_results' in k])
    st.metric("Resultados en Cach√©", cache_count)
    
    if cache_count > 0:
        st.info("üí° Usa 'Limpiar Cach√©' para liberar memoria")


if __name__ == "__main__":
    show_cumulative_metrics_page()