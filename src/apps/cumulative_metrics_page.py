"""
PÃ¡gina de MÃ©tricas Acumulativas - EvalÃºa mÃºltiples preguntas y calcula promedios (REFACTORIZADA)
"""

import streamlit as st
import time
import json
import os
from typing import List, Dict, Any
from src.config.config import EMBEDDING_MODELS, GENERATIVE_MODELS, CHROMADB_COLLECTION_CONFIG

# Importar utilidades refactorizadas
from src.data.memory_utils import get_memory_usage, cleanup_memory
# from utils.data_processing import filter_questions_with_links  # No longer needed
from src.ui.metrics_display import display_cumulative_metrics, display_models_comparison
from src.ui.enhanced_metrics_display import display_enhanced_cumulative_metrics, display_enhanced_models_comparison
from src.evaluation.cumulative import run_cumulative_metrics_evaluation, run_cumulative_metrics_for_models
from src.data.file_utils import display_download_section
from src.evaluation.metrics import validate_data_integrity
from src.services.storage.real_gdrive_integration import (
    show_gdrive_status, create_evaluation_config_in_drive,
    check_evaluation_status_in_drive, get_evaluation_results_from_drive,
    show_gdrive_authentication_instructions, show_gdrive_debug_info,
    get_all_results_files_from_drive, get_specific_results_file_from_drive
)


def show_cumulative_metrics_page():
    """FunciÃ³n principal que muestra la pÃ¡gina de mÃ©tricas acumulativas."""
    st.title("ğŸ“Š MÃ©tricas Acumulativas de RecuperaciÃ³n RAG")
    st.markdown("""
    Esta pÃ¡gina permite evaluar mÃºltiples preguntas y calcular mÃ©tricas promedio para diferentes modelos de embedding.
    """)
    
    # Variables globales para la configuraciÃ³n
    use_colab_processing = False
    
    # ConfiguraciÃ³n inicial
    col1, col2 = st.columns(2)
    
    with col1:
        st.subheader("âš™ï¸ ConfiguraciÃ³n de EvaluaciÃ³n")
        
        # InformaciÃ³n sobre la fuente de datos
        st.info("ğŸ“Š Las preguntas se extraen aleatoriamente desde ChromaDB, filtrando solo aquellas que tienen enlaces de Microsoft Learn en la respuesta aceptada.")
        
        # NÃºmero de preguntas
        num_questions = st.number_input(
            "ğŸ”¢ NÃºmero de preguntas a evaluar:",
            min_value=1,
            max_value=2000,
            value=600,
            step=1,
            help="NÃºmero total de preguntas para la evaluaciÃ³n"
        )
        
        # ConfiguraciÃ³n de modelo generativo
        generative_model_name = st.selectbox(
            "ğŸ¤– Modelo Generativo:",
            list(GENERATIVE_MODELS.keys()),
            index=list(GENERATIVE_MODELS.keys()).index("gpt-4") if "gpt-4" in GENERATIVE_MODELS else 0,
            help="Modelo usado para reranking LLM"
        )
        
        # ConfiguraciÃ³n de recuperaciÃ³n
        top_k = st.number_input(
            "ğŸ” Top-K documentos:",
            min_value=1,
            max_value=50,
            value=10,
            step=1,
            help="NÃºmero mÃ¡ximo de documentos a recuperar"
        )
        
        # Usar reranking LLM
        use_llm_reranker = st.checkbox(
            "ğŸ¤– Usar Reranking LLM",
            value=True,
            help="Activar reordenamiento de documentos con LLM"
        )
        
        # NUEVO: OpciÃ³n de usar Google Colab
        use_colab_processing = st.checkbox(
            "â˜ï¸ Procesamiento en Google Colab",
            value=True,
            help="Exportar evaluaciÃ³n a Google Colab para procesamiento con GPU (mÃ¡s rÃ¡pido)"
        )
        
        if use_colab_processing:
            st.info("ğŸ“‹ Con Colab: Se exportarÃ¡n los datos, obtendrÃ¡s un notebook para ejecutar en Colab con GPU, y luego importarÃ¡s los resultados.")
            
            # Mostrar estado de Google Drive
            gdrive_ok = show_gdrive_status()
            
            if not gdrive_ok:
                show_gdrive_authentication_instructions()
                st.stop()
        
        # TamaÃ±o de lote
        batch_size = st.number_input(
            "ğŸ“¦ TamaÃ±o de lote:",
            min_value=10,
            max_value=100,
            value=50,
            step=10,
            help="NÃºmero de preguntas a procesar por lote (para gestiÃ³n de memoria)"
        )
    
    with col2:
        st.subheader("ğŸ¯ SelecciÃ³n de Modelos")
        
        # OpciÃ³n para evaluar todos los modelos
        evaluate_all_models = st.checkbox(
            "ğŸ”„ Evaluar todos los modelos",
            value=True,
            help="Evaluar todos los modelos de embedding disponibles"
        )
        
        if evaluate_all_models:
            selected_models = list(EMBEDDING_MODELS.keys())
            st.info(f"ğŸ“‹ Se evaluarÃ¡n {len(selected_models)} modelos")
            for model in selected_models:
                st.markdown(f"â€¢ {model}")
        else:
            # SelecciÃ³n manual de modelos
            selected_models = st.multiselect(
                "ğŸ›ï¸ Modelos de Embedding:",
                list(EMBEDDING_MODELS.keys()),
                default=[list(EMBEDDING_MODELS.keys())[0]],
                help="Selecciona uno o mÃ¡s modelos para evaluar"
            )
        
        if not selected_models:
            st.error("âŒ Debes seleccionar al menos un modelo")
            return
        
        # InformaciÃ³n sobre memoria estimada
        estimated_memory = len(selected_models) * num_questions * 0.1  # MB aproximados
        st.info(f"ğŸ’¾ Memoria estimada: ~{estimated_memory:.1f} MB")
    
    # Botones de control
    st.markdown("---")
    
    if use_colab_processing:
        # Interfaz para flujo Google Colab
        show_colab_workflow(
            num_questions, selected_models, 
            generative_model_name, top_k, use_llm_reranker, 
            batch_size, evaluate_all_models
        )
    else:
        # Interfaz para evaluaciÃ³n local
        col1, col2, col3 = st.columns(3)
        
        with col1:
            if st.button("ğŸš€ Ejecutar EvaluaciÃ³n", type="primary"):
                run_evaluation(
                    num_questions, selected_models, 
                    generative_model_name, top_k, use_llm_reranker, 
                    batch_size, evaluate_all_models
                )
        
        with col2:
            if st.button("ğŸ§¹ Limpiar CachÃ©"):
                for key in list(st.session_state.keys()):
                    if 'cumulative_results' in key:
                        del st.session_state[key]
                st.success("âœ… CachÃ© limpiado")
                st.rerun()
        
        with col3:
            if st.button("ğŸ“Š Mostrar EstadÃ­sticas de Memoria"):
                show_memory_stats()
    
    # Mostrar resultados si existen
    show_cached_results()


def run_evaluation(num_questions: int, selected_models: List[str],
                  generative_model_name: str, top_k: int, use_llm_reranker: bool,
                  batch_size: int, evaluate_all_models: bool):
    """Ejecuta la evaluaciÃ³n de mÃ©tricas."""
    
    # Mostrar informaciÃ³n de memoria inicial
    initial_memory = get_memory_usage()
    st.info(f"ğŸ’¾ Memoria inicial: {initial_memory:.1f} MB")
    
    # Ejecutar evaluaciÃ³n
    start_time = time.time()
    evaluation_time = time.strftime("%Y-%m-%d %H:%M:%S")
    
    try:
        if evaluate_all_models:
            with st.spinner(f"ğŸ”„ Evaluando {len(selected_models)} modelos..."):
                results = run_cumulative_metrics_for_models(
                    num_questions=num_questions,
                    model_names=selected_models,
                    generative_model_name=generative_model_name,
                    top_k=top_k,
                    use_llm_reranker=use_llm_reranker,
                    batch_size=batch_size
                )
        else:
            # EvaluaciÃ³n de modelo Ãºnico
            model_name = selected_models[0]
            with st.spinner(f"âš™ï¸ Evaluando {model_name}..."):
                single_result = run_cumulative_metrics_evaluation(
                    num_questions=num_questions,
                    model_name=model_name,
                    generative_model_name=generative_model_name,
                    top_k=top_k,
                    use_llm_reranker=use_llm_reranker,
                    batch_size=batch_size
                )
                results = {model_name: single_result}
        
        # Validar integridad de datos
        for model_name, result in results.items():
            results[model_name] = validate_data_integrity(result)
        
        end_time = time.time()
        execution_time = end_time - start_time
        
        # Guardar en cachÃ©
        cache_key = f"cumulative_results_{len(selected_models)}_{num_questions}_{int(start_time)}"
        st.session_state[cache_key] = {
            'results': results,
            'evaluation_time': evaluation_time,
            'execution_time': execution_time,
            'evaluate_all_models': evaluate_all_models,
            'params': {
                'num_questions': num_questions,
                'selected_models': selected_models,
                'embedding_model_name': selected_models[0] if len(selected_models) == 1 else 'Multi-Model',
                'generative_model_name': generative_model_name,
                'top_k': top_k,
                'use_llm_reranker': use_llm_reranker,
                'batch_size': batch_size
            }
        }
        
        st.success(f"âœ… EvaluaciÃ³n completada en {execution_time:.2f} segundos")
        
        # Mostrar memoria final
        final_memory = get_memory_usage()
        memory_increase = final_memory - initial_memory
        st.info(f"ğŸ’¾ Memoria final: {final_memory:.1f} MB (+{memory_increase:.1f} MB)")
        
        # Forzar rerun para mostrar resultados
        st.rerun()
        
    except Exception as e:
        st.error(f"âŒ Error durante la evaluaciÃ³n: {str(e)}")
        st.exception(e)
    finally:
        # Limpieza de memoria
        cleanup_memory()


def show_colab_workflow(num_questions: int, selected_models: List[str],
                       generative_model_name: str, top_k: int, use_llm_reranker: bool,
                       batch_size: int, evaluate_all_models: bool):
    """Muestra la interfaz completa del flujo Google Colab"""
    
    st.subheader("â˜ï¸ Flujo de EvaluaciÃ³n con Google Colab")
    
    # ConfiguraciÃ³n de la evaluaciÃ³n
    evaluation_config = {
        'num_questions': num_questions,
        'selected_models': selected_models,
        'generative_model_name': generative_model_name,
        'top_k': top_k,
        'use_llm_reranker': use_llm_reranker,
        'batch_size': batch_size,
        'evaluate_all_models': evaluate_all_models,
        'evaluation_type': 'cumulative_metrics'
    }
    
    # Columnas para configuraciÃ³n
    col1, col2 = st.columns(2)
    
    with col1:
        st.markdown("**ğŸ“Š ConfiguraciÃ³n de EvaluaciÃ³n:**")
        st.json({k: v for k, v in evaluation_config.items()})
    
    with col2:
        st.markdown("**ğŸ“ Google Drive:**")
        st.info("Carpeta: `/TesisMagister/acumulative/`")
    
    # SecciÃ³n de resultados disponibles (mostrar siempre)
    st.markdown("---")
    show_available_results_section()
    
    # Botones del flujo
    col1, col2, col3, col4 = st.columns(4)
    
    with col1:
        if st.button("ğŸš€ Crear ConfiguraciÃ³n y Enviar a Google Drive", type="primary"):
            create_config_and_send_to_drive(evaluation_config)
    
    with col2:
        if st.button("ğŸ”„ Verificar Estado"):
            check_colab_evaluation_status()
    
    with col3:
        if st.button("ğŸ” Debug Google Drive"):
            st.markdown("---")
            show_gdrive_debug_info()
    
    with col4:
        # Placeholder para futuro botÃ³n
        st.empty()
    
    # Mostrar estado actual
    st.markdown("---")
    display_current_colab_status()


def create_config_and_send_to_drive(evaluation_config: Dict):
    """Crea y envÃ­a configuraciÃ³n a Google Drive real"""
    
    st.info("ğŸ“¤ Creando configuraciÃ³n y enviando a Google Drive...")
    
    try:
        # Obtener preguntas reales de la base de datos
        with st.spinner("ğŸ“¥ Obteniendo preguntas de ChromaDB..."):
            from src.data.processing import fetch_random_questions_from_chromadb
            from src.services.auth.clients import initialize_clients
            
            try:
                # Usar el primer modelo seleccionado para obtener las preguntas
                first_model = evaluation_config['selected_models'][0]
                chromadb_wrapper, embedding_client, openai_client, gemini_client, local_tinyllama_client, local_mistral_client, openrouter_client, client = initialize_clients(
                    model_name=first_model,
                    generative_model_name=evaluation_config['generative_model_name']
                )
                questions = fetch_random_questions_from_chromadb(
                    chromadb_wrapper=chromadb_wrapper,
                    embedding_model_name=first_model,
                    num_questions=evaluation_config['num_questions']
                )
                
                if questions:
                    evaluation_config['questions_data'] = questions
                    st.success(f"âœ… Obtenidas {len(questions)} preguntas con enlaces MS Learn")
                else:
                    st.warning("âš ï¸ No se encontraron preguntas, usando configuraciÃ³n sin datos")
                    evaluation_config['questions_data'] = None
                    
            except Exception as e:
                st.warning(f"âš ï¸ Error obteniendo preguntas: {e}")
                evaluation_config['questions_data'] = None
        
        # Enviar a Google Drive real
        with st.spinner("â˜ï¸ Enviando configuraciÃ³n a Google Drive..."):
            result = create_evaluation_config_in_drive(evaluation_config)
            
            if result['success']:
                st.success("âœ… Â¡ConfiguraciÃ³n enviada exitosamente a Google Drive!")
                
                col1, col2 = st.columns(2)
                
                with col1:
                    st.markdown("**ğŸ“„ Archivo creado:**")
                    st.code(result['config_filename'])
                    st.markdown(f"[ğŸ”— Ver en Drive]({result['web_link']})")
                
                with col2:
                    st.markdown("**ğŸ“‹ PrÃ³ximos pasos:**")
                    st.markdown("""
                    1. Abrir Google Colab
                    2. Subir `Universal_Colab_Evaluator.ipynb` 
                    3. Activar GPU (T4)
                    4. Ejecutar todas las celdas
                    5. Volver aquÃ­ para ver resultados
                    """)
                
                # BotÃ³n para descargar notebook
                try:
                    with open('Universal_Colab_Evaluator.ipynb', 'r') as f:
                        notebook_content = f.read()
                    
                    st.download_button(
                        label="ğŸ““ Descargar Notebook Universal",
                        data=notebook_content,
                        file_name="Universal_Colab_Evaluator.ipynb",
                        mime="application/json"
                    )
                except FileNotFoundError:
                    st.warning("âš ï¸ Notebook no encontrado localmente")
                
                st.rerun()
                
            else:
                st.error(f"âŒ Error enviando a Google Drive: {result['error']}")
                
    except Exception as e:
        st.error(f"âŒ Error: {e}")


def check_colab_evaluation_status():
    """Verifica el estado de la evaluaciÃ³n en Google Drive"""
    
    with st.spinner("ğŸ” Verificando estado en Google Drive..."):
        result = check_evaluation_status_in_drive()
        
        if result['success']:
            status = result['status']
            
            if status == 'completed':
                st.success("âœ… Â¡EvaluaciÃ³n completada en Google Colab!")
                
                data = result['data']
                col1, col2 = st.columns(2)
                
                with col1:
                    st.markdown("**ğŸ“Š Resumen:**")
                    st.write(f"ğŸ¤– Modelos: {data.get('models_evaluated', 'N/A')}")
                    st.write(f"â“ Preguntas: {data.get('questions_processed', 'N/A')}")
                    st.write(f"â±ï¸ Tiempo: {data.get('total_time_seconds', 0):.1f}s")
                    st.write(f"ğŸš€ GPU: {'âœ…' if data.get('gpu_used') else 'âŒ'}")
                
                with col2:
                    st.markdown("**ğŸ“ Archivos:**")
                    st.write(f"ğŸ“„ {data.get('results_file', 'N/A')}")
                    st.write(f"ğŸ“Š {data.get('summary_file', 'N/A')}")
                    
            elif status == 'config_created':
                st.info("ğŸ“‹ ConfiguraciÃ³n creada. Esperando ejecuciÃ³n en Colab...")
            elif status == 'no_status_file':
                st.warning("âš ï¸ No se encontrÃ³ informaciÃ³n de estado")
            else:
                st.info(f"ğŸ”„ Estado actual: {status}")
                
        else:
            st.error(f"âŒ Error verificando estado: {result['error']}")


def show_available_results_section():
    """Muestra la secciÃ³n de resultados disponibles con dropdown de selecciÃ³n"""
    
    st.subheader("ğŸ“Š Resultados Disponibles")
    
    # Obtener archivos disponibles sin spinner para no interferir con la UI
    try:
        files_result = get_all_results_files_from_drive()
        
        if not files_result['success']:
            st.warning(f"ğŸ” No se pudieron obtener archivos de resultados")
            st.error(f"âŒ {files_result['error']}")
            
            # Mostrar informaciÃ³n de debug si estÃ¡ disponible
            if 'debug_info' in files_result:
                debug_info = files_result['debug_info']
                with st.expander("ğŸ” InformaciÃ³n de Debug", expanded=True):
                    col1, col2 = st.columns(2)
                    with col1:
                        st.write(f"ğŸ“ **Folder ID:** `{debug_info.get('folder_id', 'N/A')}`")
                        st.write(f"ğŸ” **BÃºsqueda:** `{debug_info.get('search_query', 'N/A')}`")
                    with col2:
                        st.write(f"ğŸ“„ **Total archivos:** {debug_info.get('total_files', 0)}")
                        st.write(f"ğŸ“‹ **Archivos JSON:** {debug_info.get('json_files', 0)}")
                    
                    # Mostrar algunos nombres de archivos para ayudar con debug
                    if 'all_files' in debug_info and debug_info['all_files']:
                        st.write("ğŸ“‹ **Algunos archivos JSON encontrados:**")
                        for filename in debug_info['all_files']:
                            st.write(f"- `{filename}`")
            
            # BotÃ³n para ejecutar debug completo
            if st.button("ğŸ” Ejecutar Debug Completo de Google Drive"):
                st.markdown("---")
                show_gdrive_debug_info()
            
            st.info("ğŸ’¡ **Posibles soluciones:**")
            st.markdown("""
            1. ğŸš€ **Ejecuta una evaluaciÃ³n en Colab** para generar archivos de resultados
            2. ğŸ”§ **Verifica la conexiÃ³n** con el botÃ³n 'Debug Google Drive' 
            3. ğŸ“ **Confirma la carpeta** acumulative en Google Drive
            4. ğŸ“‹ **Revisa el formato** de archivos (deben ser `cumulative_results_*.json`)
            """)
            return
        
        available_files = files_result['files']
        
        if not available_files:
            st.info("ğŸ“­ No hay archivos de resultados disponibles")
            st.info("ğŸ’¡ Ejecuta una evaluaciÃ³n en Colab para generar resultados")
            return
            
    except Exception as e:
        st.warning(f"âš ï¸ Error verificando archivos: {str(e)}")
        return
    
    # Mostrar informaciÃ³n de archivos disponibles
    st.success(f"âœ… Encontrados {len(available_files)} archivos de resultados")
    
    # Crear dropdown con todos los archivos disponibles
    file_options = {file_info['display_name']: file_info['file_id'] for file_info in available_files}
    
    # Usar session_state para mantener la selecciÃ³n
    if 'selected_results_file' not in st.session_state:
        st.session_state.selected_results_file = list(file_options.keys())[0]
    
    selected_display_name = st.selectbox(
        "ğŸ“ Seleccionar archivo de resultados:",
        options=list(file_options.keys()),
        index=list(file_options.keys()).index(st.session_state.selected_results_file) if st.session_state.selected_results_file in file_options else 0,
        help="Los archivos estÃ¡n ordenados por fecha de modificaciÃ³n (mÃ¡s reciente primero)",
        key="results_file_selector"
    )
    
    # Actualizar session_state
    st.session_state.selected_results_file = selected_display_name
    selected_file_id = file_options[selected_display_name]
    
    # BotÃ³n para mostrar resultados del archivo seleccionado
    col1, col2 = st.columns([3, 1])
    
    with col1:
        if st.button("ğŸ“Š Mostrar Resultados del Archivo Seleccionado", type="primary", use_container_width=True):
            load_and_display_selected_results(selected_file_id)
    
    with col2:
        if st.button("ğŸ”„ Actualizar Lista", help="Buscar nuevos archivos de resultados"):
            st.rerun()


def load_and_display_selected_results(file_id: str):
    """Carga y muestra los resultados del archivo seleccionado"""
    
    with st.spinner("ğŸ“Š Cargando resultados del archivo seleccionado..."):
        result = get_specific_results_file_from_drive(file_id)
        
        if not result['success']:
            st.error(f"âŒ Error cargando archivo: {result['error']}")
            return
        
        st.success("âœ… Resultados cargados exitosamente!")
        
        # Usar un expander para los resultados para que no ocupen todo el espacio
        with st.expander("ğŸ“Š Resultados de EvaluaciÃ³n", expanded=True):
            display_results_content(result['results'])


def display_results_content(results_data):
    """Muestra el contenido de los resultados de evaluaciÃ³n"""
    
    # Debug: Mostrar estructura de datos si estÃ¡ habilitado
    if st.checkbox("ğŸ” Mostrar estructura de datos (debug)", value=False):
        with st.expander("ğŸ“‹ Estructura de datos completa"):
            st.json(results_data)
    
    # Convertir formato para compatibilidad con funciones existentes
    processed_results = {}
    for model_name, model_data in results_data['results'].items():
        processed_results[model_name] = model_data
    
    # Generar visualizaciones usando funciones existentes
    # Extraer informaciÃ³n de tiempo y configuraciÃ³n
    evaluation_info = results_data.get('evaluation_info', {})
    config_info = results_data.get('config', {})
    
    # Formatear fecha y hora
    timestamp = evaluation_info.get('timestamp')
    if timestamp:
        try:
            from datetime import datetime
            if isinstance(timestamp, str):
                # Intentar parsear diferentes formatos de timestamp
                if 'T' in timestamp:
                    dt = datetime.fromisoformat(timestamp.replace('Z', '+00:00'))
                else:
                    dt = datetime.strptime(timestamp, '%Y-%m-%d %H:%M:%S')
            else:
                dt = datetime.fromtimestamp(timestamp)
            
            formatted_date = dt.strftime('%d/%m/%Y %H:%M:%S')
        except:
            formatted_date = str(timestamp)
    else:
        formatted_date = "Fecha no disponible"
    
    # Obtener nÃºmero de preguntas
    num_questions = config_info.get('num_questions', evaluation_info.get('questions_processed', 'N/A'))
    
    # Obtener mÃ©todo de autenticaciÃ³n si estÃ¡ disponible
    auth_method = evaluation_info.get('auth_method', '')
    auth_icon = 'ğŸ”' if auth_method == 'API' else 'ğŸ“' if auth_method == 'Mount' else ''
    
    # Mostrar header con informaciÃ³n detallada
    st.subheader(f"ğŸ“Š Resultados de EvaluaciÃ³n Acumulativa")
    
    # InformaciÃ³n de la evaluaciÃ³n en una caja destacada
    col1, col2, col3 = st.columns(3)
    
    with col1:
        st.info(f"ğŸ“… **Fecha:** {formatted_date}")
    
    with col2:
        st.info(f"â“ **Preguntas:** {num_questions}")
    
    with col3:
        models_count = len(results_data['results'])
        st.info(f"ğŸ¤– **Modelos:** {models_count}")
    
    # InformaciÃ³n adicional si estÃ¡ disponible
    if evaluation_info.get('total_time_seconds'):
        total_time = evaluation_info['total_time_seconds']
        if total_time >= 60:
            time_str = f"{total_time/60:.1f} min"
        else:
            time_str = f"{total_time:.1f}s"
        
        gpu_used = evaluation_info.get('gpu_used', False)
        gpu_icon = 'ğŸš€' if gpu_used else 'ğŸ’»'
        
        st.caption(f"â±ï¸ Tiempo de ejecuciÃ³n: {time_str} {gpu_icon} | {auth_icon} {auth_method}")
    
    st.markdown("---")
    
    # Determinar si usar reranker desde config
    use_llm_reranker = results_data['config'].get('use_llm_reranker', True)
    
    if len(processed_results) == 1:
        # Para un solo modelo, usar display_enhanced_cumulative_metrics
        model_name = list(processed_results.keys())[0]
        model_results = processed_results[model_name]
        
        # Adaptar formato segÃºn la estructura de datos disponible
        # Nuevo formato del notebook actualizado tiene avg_before_metrics y avg_after_metrics
        if 'avg_before_metrics' in model_results and 'avg_after_metrics' in model_results:
            # Formato actualizado con before/after metrics
            adapted_results = {
                'num_questions_evaluated': model_results.get('num_questions_evaluated', results_data['config']['num_questions']),
                'avg_before_metrics': model_results['avg_before_metrics'],
                'avg_after_metrics': model_results['avg_after_metrics'],
                'individual_metrics': model_results.get('individual_before_metrics', [])
            }
            
            # Verificar si realmente hay mÃ©tricas after
            if use_llm_reranker and not model_results['avg_after_metrics']:
                st.info("â„¹ï¸ Reranking LLM estaba habilitado en configuraciÃ³n, pero no se generaron mÃ©tricas after. Mostrando solo mÃ©tricas before.")
                use_llm_reranker = False
                
        else:
            # Formato anterior - solo avg_metrics (mantener compatibilidad)
            avg_metrics = model_results.get('avg_metrics', {})
            
            adapted_results = {
                'num_questions_evaluated': results_data['config']['num_questions'],
                'avg_before_metrics': avg_metrics,  # Usar las mÃ©tricas como "before" 
                'avg_after_metrics': {},  # VacÃ­o porque no hay reranking
                'individual_metrics': model_results.get('individual_metrics', [])
            }
            
            if use_llm_reranker:
                st.warning("âš ï¸ Reranking LLM estaba habilitado pero estos resultados usan formato antiguo. Mostrando solo mÃ©tricas base.")
                use_llm_reranker = False
        
        # Use enhanced display with cleaner before/after LLM separation
        display_enhanced_cumulative_metrics(adapted_results, model_name, use_llm_reranker)
        
    else:
        # Para mÃºltiples modelos, usar display_enhanced_models_comparison
        st.markdown("### ğŸ† ComparaciÃ³n de Modelos")
        
        # Adaptar formato para mÃºltiples modelos segÃºn estructura disponible
        adapted_multi_results = {}
        has_new_format = False
        
        for model_name, model_data in processed_results.items():
            # Verificar si usa el nuevo formato con before/after metrics
            if 'avg_before_metrics' in model_data and 'avg_after_metrics' in model_data:
                # Formato actualizado
                adapted_multi_results[model_name] = {
                    'num_questions_evaluated': model_data.get('num_questions_evaluated', results_data['config']['num_questions']),
                    'avg_before_metrics': model_data['avg_before_metrics'],
                    'avg_after_metrics': model_data['avg_after_metrics'],
                    'individual_metrics': model_data.get('individual_before_metrics', [])
                }
                has_new_format = True
            else:
                # Formato anterior - compatibilidad
                avg_metrics = model_data.get('avg_metrics', {})
                adapted_multi_results[model_name] = {
                    'num_questions_evaluated': results_data['config']['num_questions'],
                    'avg_before_metrics': avg_metrics,  # Usar mÃ©tricas como "before"
                    'avg_after_metrics': {},  # VacÃ­o porque no hay reranking
                    'individual_metrics': model_data.get('individual_metrics', [])
                }
        
        # Verificar si hay mÃ©tricas after disponibles para LLM reranking
        if use_llm_reranker:
            has_after_metrics = any(adapted_multi_results[model]['avg_after_metrics'] for model in adapted_multi_results)
            if not has_after_metrics:
                if has_new_format:
                    st.info("â„¹ï¸ Reranking LLM estaba habilitado pero no se generaron mÃ©tricas after. Mostrando solo mÃ©tricas before.")
                else:
                    st.warning("âš ï¸ Reranking LLM estaba habilitado pero estos resultados usan formato antiguo. Mostrando solo mÃ©tricas base.")
                use_llm_reranker = False
        
        # Use enhanced display for cleaner multi-model comparison
        display_enhanced_models_comparison(adapted_multi_results, use_llm_reranker)
    
    # SecciÃ³n de descarga
    st.markdown("---")
    
    # Preparar datos para la secciÃ³n de descarga en el formato esperado
    cached_results = {
        'results': processed_results,
        'evaluation_time': results_data['evaluation_info'].get('timestamp'),
        'execution_time': results_data['evaluation_info'].get('total_time_seconds'),
        'evaluate_all_models': len(processed_results) > 1,
        'params': {
            'num_questions': results_data['config']['num_questions'],
            'selected_models': list(processed_results.keys()),
            'embedding_model_name': list(processed_results.keys())[0] if len(processed_results) == 1 else 'Multi-Model',
            'generative_model_name': results_data['config']['generative_model_name'],
            'top_k': results_data['config']['top_k'],
            'use_llm_reranker': results_data['config']['use_llm_reranker'],
            'batch_size': results_data['config']['batch_size']
        }
    }
    
    display_download_section(cached_results)




def display_current_colab_status():
    """Muestra el estado actual del sistema Google Colab"""
    
    st.subheader("ğŸ“Š Estado Actual")
    
    # Verificar estado sin spinner (solo para mostrar info)
    try:
        result = check_evaluation_status_in_drive()
        
        if result['success']:
            status = result['status']
            
            if status == 'completed':
                st.success("âœ… EvaluaciÃ³n completada - Lista para mostrar resultados")
            elif status == 'config_created':
                st.info("ğŸ“‹ ConfiguraciÃ³n lista - Ejecutar en Google Colab")
            elif status == 'no_status_file':
                st.info("âšª Sin evaluaciones pendientes")
            else:
                st.info(f"ğŸ”„ Estado: {status}")
        else:
            st.warning("âš ï¸ No se pudo verificar estado")
            
    except Exception as e:
        st.warning(f"âš ï¸ Error verificando estado: {e}")


def create_drive_config_file(evaluation_config: Dict) -> bool:
    """Crea archivo de configuraciÃ³n simulando Google Drive"""
    
    try:
        import os
        
        # Simular la estructura de carpetas de Google Drive localmente
        # En producciÃ³n, esto se conectarÃ­a realmente a Google Drive
        local_drive_path = "/Users/haroldgomez/Documents/ProyectoTituloMAgister/SupportModel/simulated_drive"
        config_path = f"{local_drive_path}/evaluation_config.json"
        
        # Crear directorio si no existe
        os.makedirs(local_drive_path, exist_ok=True)
        
        # Guardar configuraciÃ³n
        with open(config_path, 'w', encoding='utf-8') as f:
            json.dump(evaluation_config, f, indent=2, ensure_ascii=False)
        
        st.success(f"âœ… ConfiguraciÃ³n guardada en: {config_path}")
        st.info("ğŸ“ En producciÃ³n, esto se guardarÃ­a en `/content/drive/MyDrive/TesisMagister/acumulative/evaluation_config.json`")
        
        return True
        
    except Exception as e:
        st.error(f"âŒ Error creando archivo de configuraciÃ³n: {e}")
        return False


def show_colab_status_and_results():
    """Muestra el estado de la evaluaciÃ³n en Colab y botÃ³n para mostrar resultados"""
    
    st.subheader("ğŸ“Š Estado de la EvaluaciÃ³n en Colab")
    
    # Verificar si existe archivo de status
    status_file = "/Users/haroldgomez/Documents/ProyectoTituloMAgister/SupportModel/simulated_drive/evaluation_status.json"
    results_dir = "/Users/haroldgomez/Documents/ProyectoTituloMAgister/SupportModel/simulated_drive/results"
    
    if os.path.exists(status_file):
        try:
            with open(status_file, 'r', encoding='utf-8') as f:
                status_data = json.load(f)
            
            if status_data.get('status') == 'completed':
                st.success("âœ… Â¡EvaluaciÃ³n completada en Google Colab!")
                
                col1, col2 = st.columns(2)
                
                with col1:
                    st.markdown("**ğŸ“Š Resumen de Resultados:**")
                    st.write(f"ğŸ¤– Modelos evaluados: {status_data.get('models_evaluated', 'N/A')}")
                    st.write(f"â“ Preguntas procesadas: {status_data.get('questions_processed', 'N/A')}")
                    st.write(f"â±ï¸ Tiempo total: {status_data.get('total_time_seconds', 0):.2f}s")
                    st.write(f"ğŸš€ GPU utilizada: {'âœ…' if status_data.get('gpu_used') else 'âŒ'}")
                
                with col2:
                    st.markdown("**ğŸ“ Archivos generados:**")
                    st.write(f"ğŸ“„ {status_data.get('results_file', 'N/A')}")
                    st.write(f"ğŸ“Š {status_data.get('summary_file', 'N/A')}")
                
                # BotÃ³n para mostrar resultados
                if st.button("ğŸ“Š Mostrar Resultados y Generar Visualizaciones", type="primary"):
                    show_colab_results_and_generate_visuals(status_data)
                
            else:
                st.info(f"ğŸ”„ Estado: {status_data.get('status', 'unknown')}")
        
        except Exception as e:
            st.error(f"âŒ Error leyendo estado: {e}")
    
    else:
        st.info("â³ Esperando resultados de Google Colab...")
        st.markdown("**ğŸ“‹ Para que aparezcan los resultados:**")
        st.markdown("1. Ejecuta el notebook en Google Colab")
        st.markdown("2. Espera a que termine la evaluaciÃ³n")
        st.markdown("3. Los resultados aparecerÃ¡n automÃ¡ticamente aquÃ­")
        
        # BotÃ³n para refrescar estado
        if st.button("ğŸ”„ Verificar Estado"):
            st.rerun()




def generate_colab_notebook_code(config: Dict) -> str:
    """Genera el cÃ³digo completo para ejecutar en Colab."""
    
    models_str = ', '.join([f'"{m}"' for m in config['selected_models']])
    
    code = f'''# ğŸš€ EvaluaciÃ³n de Embeddings en Google Colab con GPU
# Generado automÃ¡ticamente el {config['timestamp']}
# ConfiguraciÃ³n: {config['num_questions']} preguntas, {len(config['selected_models'])} modelos

import os
import time
import json
import random
import pandas as pd
import numpy as np
from datetime import datetime
from typing import Dict, List, Any

# ğŸ“Š ConfiguraciÃ³n de la evaluaciÃ³n
EVALUATION_CONFIG = {{
    'num_questions': {config['num_questions']},
    'selected_models': [{models_str}],
    'generative_model_name': "{config['generative_model_name']}",
    'top_k': {config['top_k']},
    'use_llm_reranker': {config['use_llm_reranker']},
    'batch_size': {config['batch_size']},
    'evaluate_all_models': {config['evaluate_all_models']},
    'timestamp': "{config['timestamp']}"
}}

print("ğŸš€ Iniciando evaluaciÃ³n de embeddings en Google Colab")
print("ğŸ“Š ConfiguraciÃ³n:")
for key, value in EVALUATION_CONFIG.items():
    print(f"   {{key}}: {{value}}")

# âœ… 1. Verificar GPU
print("\\nğŸ”§ Verificando hardware disponible...")
try:
    import torch
    print(f"CUDA disponible: {{torch.cuda.is_available()}}")
    if torch.cuda.is_available():
        print(f"GPU: {{torch.cuda.get_device_name(0)}}")
        print(f"Memoria GPU: {{torch.cuda.get_device_properties(0).total_memory / 1e9:.1f}} GB")
        print("âœ… GPU T4 detectada - procesamiento acelerado habilitado!")
    else:
        print("âš ï¸  GPU no disponible - usando CPU (mÃ¡s lento)")
except ImportError:
    print("âš ï¸  PyTorch no instalado - se instalarÃ¡ en el siguiente paso")

# ğŸ“¦ 2. Instalar dependencias necesarias
print("\\nğŸ“¦ Instalando dependencias...")
!pip install -q sentence-transformers pandas numpy scikit-learn openai python-dotenv tqdm

# ğŸ“š 3. Importar librerÃ­as
print("\\nğŸ“š Importando librerÃ­as...")
try:
    from sentence_transformers import SentenceTransformer
    from sklearn.metrics.pairwise import cosine_similarity
    from tqdm.auto import tqdm
    import warnings
    warnings.filterwarnings('ignore')
    print("âœ… LibrerÃ­as importadas correctamente")
except ImportError as e:
    print(f"âŒ Error importando librerÃ­as: {{e}}")
    print("ğŸ’¡ Reinicia el runtime y vuelve a ejecutar")

# ğŸ² 4. Generar datos de prueba (simulando tu base de datos)
print("\\nğŸ² Generando datos de prueba para demostraciÃ³n...")

def generate_sample_questions(num_questions: int) -> List[Dict]:
    """Genera preguntas de ejemplo que simulan tu base de datos"""
    
    sample_questions = [
        "Â¿CÃ³mo configurar Azure Storage Blob?",
        "Â¿CuÃ¡l es la diferencia entre SQL Database y Cosmos DB?",
        "Â¿CÃ³mo implementar autenticaciÃ³n en Azure Functions?",
        "Â¿QuÃ© es Azure Container Instances?",
        "Â¿CÃ³mo usar Azure DevOps para CI/CD?",
        "Â¿CuÃ¡les son las mejores prÃ¡cticas para Azure Security?",
        "Â¿CÃ³mo configurar Application Insights?",
        "Â¿QuÃ© es Azure Service Bus?",
        "Â¿CÃ³mo usar Azure Logic Apps?",
        "Â¿CuÃ¡l es la diferencia entre VM y App Service?",
        "Â¿CÃ³mo configurar Azure Active Directory?",
        "Â¿QuÃ© es Azure Kubernetes Service?",
        "Â¿CÃ³mo usar Azure Key Vault?",
        "Â¿CuÃ¡les son los tipos de Azure Storage?",
        "Â¿CÃ³mo implementar Azure API Management?"
    ]
    
    # Expandir la lista repitiendo y variando
    questions = []
    for i in range(num_questions):
        base_question = sample_questions[i % len(sample_questions)]
        # AÃ±adir variaciÃ³n
        variations = [
            f"{{base_question}}",
            f"Tutorial: {{base_question}}",
            f"GuÃ­a paso a paso: {{base_question}}",
            f"Mejores prÃ¡cticas: {{base_question}}",
            f"SoluciÃ³n de problemas: {{base_question}}"
        ]
        
        question_text = variations[i % len(variations)]
        
        questions.append({{'question': question_text, 'id': f'q_{{i+1}}'}})
    
    return questions

# Generar preguntas de prueba
test_questions = generate_sample_questions(EVALUATION_CONFIG['num_questions'])
print(f"âœ… Generadas {{len(test_questions)}} preguntas de prueba")

# ğŸ¤– 5. FunciÃ³n de evaluaciÃ³n acelerada con GPU
def run_gpu_accelerated_evaluation(questions: List[Dict], models: List[str]) -> Dict:
    """Ejecuta evaluaciÃ³n usando GPU para mÃ¡ximo rendimiento"""
    
    print(f"\\nğŸš€ Iniciando evaluaciÃ³n acelerada...")
    print(f"ğŸ“Š Preguntas: {{len(questions)}}")
    print(f"ğŸ¤– Modelos: {{models}}")
    
    results = {{}}
    
    for model_name in tqdm(models, desc="Evaluando modelos"):
        print(f"\\nâš™ï¸ Procesando modelo: {{model_name}}")
        
        # Simular carga del modelo
        model_start = time.time()
        
        # En una implementaciÃ³n real, aquÃ­ cargarÃ­as el modelo:
        # model = SentenceTransformer(model_name)
        # if torch.cuda.is_available():
        #     model = model.to('cuda')
        
        print(f"   ğŸ“¥ Modelo cargado en {{time.time() - model_start:.2f}}s")
        
        # Simular procesamiento por lotes
        batch_size = EVALUATION_CONFIG['batch_size']
        batch_results = []
        
        for i in tqdm(range(0, len(questions), batch_size), desc=f"Lotes {{model_name}}", leave=False):
            batch_questions = questions[i:i+batch_size]
            
            # Simular embeddings y mÃ©tricas (en implementaciÃ³n real usarÃ­as el modelo)
            batch_metrics = {{
                'precision': random.uniform(0.65, 0.95),
                'recall': random.uniform(0.60, 0.90),
                'f1': random.uniform(0.62, 0.92),
                'map': random.uniform(0.55, 0.88),
                'mrr': random.uniform(0.60, 0.92),
                'ndcg': random.uniform(0.65, 0.95)
            }}
            
            batch_results.append(batch_metrics)
            
            # Simular tiempo de procesamiento GPU
            time.sleep(0.01)  # Simular procesamiento rÃ¡pido con GPU
        
        # Calcular mÃ©tricas promedio
        avg_metrics = {{}}
        for metric in ['precision', 'recall', 'f1', 'map', 'mrr', 'ndcg']:
            values = [br[metric] for br in batch_results]
            avg_metrics[f'avg_{{metric}}'] = np.mean(values)
        
        # Guardar resultados del modelo
        model_results = {{
            'model_name': model_name,
            'avg_metrics': avg_metrics,
            'total_questions': len(questions),
            'batch_size': batch_size,
            'processing_time_seconds': time.time() - model_start,
            'evaluation_time': datetime.now().isoformat(),
            'gpu_accelerated': torch.cuda.is_available() if 'torch' in globals() else False
        }}
        
        results[model_name] = model_results
        
        print(f"   âœ… {{model_name}} completado - F1: {{avg_metrics['avg_f1']:.3f}}")
    
    return results

# ğŸš€ 6. Ejecutar evaluaciÃ³n completa
print("\\n" + "="*60)
print("ğŸš€ EJECUTANDO EVALUACIÃ“N COMPLETA")
print("="*60)

start_time = time.time()

try:
    # Ejecutar evaluaciÃ³n
    evaluation_results = run_gpu_accelerated_evaluation(
        test_questions, 
        EVALUATION_CONFIG['selected_models']
    )
    
    total_time = time.time() - start_time
    
    # ğŸ’¾ 7. Guardar resultados
    print(f"\\nğŸ’¾ Guardando resultados...")
    
    # Archivo JSON completo
    output_file = f"cumulative_results_colab_{{int(time.time())}}.json"
    
    final_results = {{
        'config': EVALUATION_CONFIG,
        'results': evaluation_results,
        'execution_summary': {{
            'total_time_seconds': total_time,
            'questions_processed': len(test_questions),
            'models_evaluated': len(EVALUATION_CONFIG['selected_models']),
            'gpu_used': torch.cuda.is_available() if 'torch' in globals() else False,
            'timestamp': datetime.now().isoformat()
        }}
    }}
    
    with open(output_file, 'w', encoding='utf-8') as f:
        json.dump(final_results, f, indent=2, ensure_ascii=False)
    
    # Archivo CSV resumen
    csv_data = []
    for model_name, results in evaluation_results.items():
        metrics = results['avg_metrics']
        row = {{'Model': model_name}}
        row.update({{k.replace('avg_', '').upper(): f"{{v:.4f}}" for k, v in metrics.items()}})
        row['Processing_Time'] = f"{{results['processing_time_seconds']:.2f}}s"
        csv_data.append(row)
    
    df_summary = pd.DataFrame(csv_data)
    csv_file = f"results_summary_{{int(time.time())}}.csv"
    df_summary.to_csv(csv_file, index=False)
    
    # ğŸ“Š 8. Mostrar resumen final
    print("\\n" + "="*60)
    print("ğŸ“Š RESUMEN DE RESULTADOS")
    print("="*60)
    
    print(f"â±ï¸  Tiempo total: {{total_time:.2f}} segundos")
    print(f"ğŸ“Š Preguntas procesadas: {{len(test_questions):,}}")
    print(f"ğŸ¤– Modelos evaluados: {{len(EVALUATION_CONFIG['selected_models'])}}")
    print(f"ğŸš€ GPU acelerado: {{'âœ… SÃ­' if torch.cuda.is_available() if 'torch' in globals() else False else 'âŒ No'}}")
    
    print(f"\\nğŸ† RANKING DE MODELOS:")
    print("-"*50)
    
    # Ordenar modelos por F1-Score
    model_ranking = sorted(
        evaluation_results.items(),
        key=lambda x: x[1]['avg_metrics']['avg_f1'],
        reverse=True
    )
    
    for i, (model_name, results) in enumerate(model_ranking, 1):
        metrics = results['avg_metrics']
        print(f"{{i}}. {{model_name}}")
        print(f"   F1: {{metrics['avg_f1']:.4f}} | MAP: {{metrics['avg_map']:.4f}} | MRR: {{metrics['avg_mrr']:.4f}}")
    
    print(f"\\nğŸ“ ARCHIVOS GENERADOS:")
    print("-"*30)
    print(f"ğŸ“„ {{output_file}} (resultados completos)")
    print(f"ğŸ“Š {{csv_file}} (resumen CSV)")
    
    print(f"\\nğŸ’¾ Para descargar:")
    print("1. Haz clic en la carpeta ğŸ“ en el panel izquierdo")
    print("2. Busca los archivos generados")
    print("3. Haz clic derecho â†’ Download")
    
    print(f"\\nğŸ‰ Â¡EVALUACIÃ“N COMPLETADA EXITOSAMENTE!")
    print("âœ… Importa estos archivos en tu aplicaciÃ³n Streamlit local")
    
except Exception as e:
    print(f"\\nâŒ Error durante la evaluaciÃ³n: {{e}}")
    import traceback
    traceback.print_exc()
    print("\\nğŸ’¡ Revisa los errores y vuelve a ejecutar")

print("\\n" + "="*60)
print("ğŸ¯ PROCESO FINALIZADO")
print("="*60)
'''

    return code


def show_cached_results():
    """Muestra los resultados cacheados si existen."""
    cached_keys = [k for k in st.session_state.keys() if 'cumulative_results' in k]
    
    if not cached_keys:
        st.info("â„¹ï¸ No hay resultados de evaluaciÃ³n. Ejecuta una evaluaciÃ³n para ver los resultados.")
        return
    
    # Seleccionar quÃ© resultados mostrar
    if len(cached_keys) > 1:
        st.subheader("ğŸ“‹ Resultados Disponibles")
        selected_key = st.selectbox(
            "Selecciona resultados:",
            cached_keys,
            format_func=lambda x: f"EvaluaciÃ³n {x.split('_')[-1]} ({st.session_state[x]['params']['embedding_model_name']})"
        )
    else:
        selected_key = cached_keys[0]
    
    if selected_key not in st.session_state:
        return
    
    cached_results = st.session_state[selected_key]
    results = cached_results['results']
    evaluate_all_models = cached_results['evaluate_all_models']
    params = cached_results['params']
    
    st.markdown("---")
    
    # Mostrar resultados
    if evaluate_all_models:
        st.subheader("ğŸ“ˆ ComparaciÃ³n Multi-Modelo")
        display_models_comparison(results, params['use_llm_reranker'])
    else:
        st.subheader(f"ğŸ“Š Resultados para {params['embedding_model_name']}")
        model_name = list(results.keys())[0]
        display_cumulative_metrics(results[model_name], model_name, params['use_llm_reranker'])
    
    # SecciÃ³n de descarga
    st.markdown("---")
    display_download_section(cached_results)


def show_memory_stats():
    """Muestra estadÃ­sticas detalladas de memoria."""
    current_memory = get_memory_usage()
    
    st.subheader("ğŸ“Š EstadÃ­sticas de Memoria")
    st.metric("Memoria Actual", f"{current_memory:.1f} MB")
    
    # Contar elementos en cachÃ©
    cache_count = len([k for k in st.session_state.keys() if 'cumulative_results' in k])
    st.metric("Resultados en CachÃ©", cache_count)
    
    if cache_count > 0:
        st.info("ğŸ’¡ Usa 'Limpiar CachÃ©' para liberar memoria")


if __name__ == "__main__":
    show_cumulative_metrics_page()