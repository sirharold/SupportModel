#!/usr/bin/env python3
"""
Script para probar que la lectura de configuraciÃ³n funciona correctamente
con el archivo de configuraciÃ³n actualizado.
"""

import json
import os

def test_config_reading(config_path):
    """
    Prueba la lectura de configuraciÃ³n simulando el comportamiento del Colab
    """
    print(f"ğŸ” TESTING CONFIG READING: {os.path.basename(config_path)}")
    print("=" * 60)
    
    try:
        # Cargar configuraciÃ³n
        with open(config_path, 'r', encoding='utf-8') as f:
            config_data = json.load(f)
        
        print(f"âœ… Config loaded successfully")
        
        # Simular lectura como en el cÃ³digo actualizado
        params = config_data.get('params', {})
        
        # Buscar top_k primero en el nivel raÃ­z, luego en params
        top_k = config_data.get('top_k', params.get('top_k', 10))
        
        # Buscar generate_rag_metrics en nivel raÃ­z, luego en params  
        generate_rag = config_data.get('generate_rag_metrics', params.get('generate_rag_metrics', True))
        
        # Buscar questions_data primero, luego questions (compatibilidad)
        questions_data = config_data.get('questions_data', config_data.get('questions', []))
        
        # Otros parÃ¡metros importantes
        selected_models = config_data.get('selected_models', [])
        reranking_method = config_data.get('reranking_method', 'standard')
        use_llm_reranker = config_data.get('use_llm_reranker', False)
        
        print(f"\nğŸ“Š PARÃMETROS LEÃDOS:")
        print(f"â”œâ”€â”€ top_k: {top_k}")
        print(f"â”œâ”€â”€ generate_rag_metrics: {generate_rag}")
        print(f"â”œâ”€â”€ num_questions: {len(questions_data)}")
        print(f"â”œâ”€â”€ selected_models: {selected_models}")
        print(f"â”œâ”€â”€ reranking_method: {reranking_method}")
        print(f"â”œâ”€â”€ use_llm_reranker: {use_llm_reranker}")
        
        # Verificar que top_k es el esperado
        expected_top_k = 50
        if top_k == expected_top_k:
            print(f"\nâœ… TOP-K CORRECTO: {top_k} (esperado: {expected_top_k})")
        else:
            print(f"\nâŒ TOP-K INCORRECTO: {top_k} (esperado: {expected_top_k})")
        
        # Verificar que tenemos preguntas
        if len(questions_data) > 0:
            print(f"âœ… PREGUNTAS ENCONTRADAS: {len(questions_data)}")
            
            # Mostrar muestra de primera pregunta
            first_question = questions_data[0]
            print(f"\nğŸ“ MUESTRA DE PRIMERA PREGUNTA:")
            print(f"â”œâ”€â”€ Keys: {list(first_question.keys())}")
            question_text = first_question.get('question', first_question.get('question_content', 'N/A'))
            print(f"â”œâ”€â”€ Texto: {question_text[:100]}...")
            
            # Verificar ground truth links
            validated_links = first_question.get('validated_links', [])
            if validated_links:
                print(f"â”œâ”€â”€ Ground truth links: {len(validated_links)}")
                print(f"â””â”€â”€ Primer link: {validated_links[0]}")
            else:
                print(f"â””â”€â”€ âš ï¸  No se encontraron validated_links")
        else:
            print(f"âŒ NO SE ENCONTRARON PREGUNTAS")
        
        print(f"\nğŸ¯ SIMULACIÃ“N DE EVALUACIÃ“N:")
        print(f"Se evaluarÃ­an {len(selected_models)} modelos")
        print(f"Con {len(questions_data)} preguntas")
        print(f"Recuperando top-{top_k} documentos por pregunta")
        if use_llm_reranker:
            print(f"Con reranking usando mÃ©todo: {reranking_method}")
        
        return True
        
    except Exception as e:
        print(f"âŒ Error: {e}")
        return False

if __name__ == "__main__":
    config_path = "/Users/haroldgomez/Downloads/evaluation_config_1753771775.json"
    
    if os.path.exists(config_path):
        success = test_config_reading(config_path)
        if success:
            print(f"\nâœ… Test completado exitosamente")
            print(f"ğŸ’¡ El cÃ³digo actualizado deberÃ­a leer top_k=50 correctamente")
        else:
            print(f"\nâŒ Test fallÃ³")
    else:
        print(f"âŒ Archivo de configuraciÃ³n no encontrado: {config_path}")