{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "header"
   },
   "source": [
    "# üöÄ Evaluaci√≥n Acumulativa de Embeddings - GPU Accelerated\n",
    "\n",
    "Este notebook ejecuta evaluaci√≥n comparativa de modelos de embeddings usando GPU de Google Colab.\n",
    "\n",
    "**‚ö° Ventajas de usar este notebook:**\n",
    "- üöÄ GPU T4 gratuita (10-50x m√°s r√°pido)\n",
    "- üíæ Sin limitaciones de memoria local\n",
    "- ‚òÅÔ∏è Instalaci√≥n autom√°tica de dependencias\n",
    "- üîÑ Integraci√≥n con Google Drive\n",
    "\n",
    "**üìã Antes de empezar:**\n",
    "1. Activa GPU: Runtime ‚Üí Change runtime type ‚Üí GPU ‚Üí T4\n",
    "2. Configura los par√°metros en la secci√≥n de configuraci√≥n\n",
    "3. Ejecuta las celdas en orden\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "config_section"
   },
   "source": [
    "## ‚öôÔ∏è Configuraci√≥n de la Evaluaci√≥n\n",
    "\n",
    "Modifica estos par√°metros seg√∫n tus necesidades:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "config"
   },
   "outputs": [],
   "source": [
    "# üìä CONFIGURACI√ìN DE LA EVALUACI√ìN\n",
    "EVALUATION_CONFIG = {\n",
    "    'num_questions': 500,  # N√∫mero de preguntas a evaluar\n",
    "    'selected_models': [\n",
    "        'multi-qa-mpnet-base-dot-v1',\n",
    "        'all-MiniLM-L6-v2', \n",
    "        'ada',  # text-embedding-ada-002\n",
    "        'e5-large-v2'\n",
    "    ],\n",
    "    'generative_model_name': 'llama-3.3-70b',\n",
    "    'top_k': 10,\n",
    "    'use_llm_reranker': True,\n",
    "    'batch_size': 50,\n",
    "    'evaluate_all_models': True,\n",
    "    'use_gpu': True,  # Activar procesamiento GPU\n",
    "    'drive_integration': True  # Guardar resultados en Google Drive\n",
    "}\n",
    "\n",
    "# üìÅ Configuraci√≥n de Google Drive\n",
    "DRIVE_BASE = \"/content/drive/MyDrive/TesisMagister/acumulative\"\n",
    "\n",
    "print(\"üöÄ Configuraci√≥n cargada:\")\n",
    "for key, value in EVALUATION_CONFIG.items():\n",
    "    print(f\"   {key}: {value}\")\n",
    "print(f\"\\nüìÅ Carpeta Drive: {DRIVE_BASE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "setup_section"
   },
   "source": [
    "## üîß Setup del Entorno"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gpu_check"
   },
   "outputs": [],
   "source": [
    "# ‚úÖ Verificar GPU\n",
    "print(\"üîß Verificando hardware disponible...\")\n",
    "try:\n",
    "    import torch\n",
    "    gpu_available = torch.cuda.is_available()\n",
    "    print(f\"CUDA disponible: {gpu_available}\")\n",
    "    \n",
    "    if gpu_available:\n",
    "        print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "        print(f\"Memoria GPU: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
    "        print(\"‚úÖ GPU T4 detectada - procesamiento acelerado habilitado!\")\n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è  GPU no disponible - usando CPU (m√°s lento)\")\n",
    "        print(\"üí° Ve a Runtime ‚Üí Change runtime type ‚Üí GPU para activar GPU\")\n",
    "        \n",
    "except ImportError:\n",
    "    print(\"‚ö†Ô∏è  PyTorch no instalado - se instalar√° en el siguiente paso\")\n",
    "    gpu_available = False\n",
    "\n",
    "EVALUATION_CONFIG['gpu_detected'] = gpu_available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "install_deps"
   },
   "outputs": [],
   "source": [
    "# üì¶ Instalar dependencias\n",
    "print(\"üì¶ Instalando dependencias necesarias...\")\n",
    "\n",
    "!pip install -q sentence-transformers\n",
    "!pip install -q pandas numpy scikit-learn\n",
    "!pip install -q openai python-dotenv\n",
    "!pip install -q tqdm plotly\n",
    "\n",
    "print(\"‚úÖ Dependencias instaladas\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mount_drive"
   },
   "outputs": [],
   "source": [
    "# üìÅ Montar Google Drive\n",
    "if EVALUATION_CONFIG['drive_integration']:\n",
    "    print(\"üìÅ Montando Google Drive...\")\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n",
    "    \n",
    "    # Crear carpeta si no existe\n",
    "    import os\n",
    "    os.makedirs(DRIVE_BASE, exist_ok=True)\n",
    "    print(f\"‚úÖ Google Drive montado en: {DRIVE_BASE}\")\n",
    "    \n",
    "    # Verificar si existe archivo .env\n",
    "    env_file = f\"{DRIVE_BASE}/.env\"\n",
    "    if os.path.exists(env_file):\n",
    "        print(f\"‚úÖ Archivo .env encontrado: {env_file}\")\n",
    "    else:\n",
    "        print(f\"‚ö†Ô∏è  Archivo .env no encontrado en: {env_file}\")\n",
    "        print(\"üí° Sube tu archivo .env a la carpeta para usar APIs reales\")\nelse:\n",
    "    print(\"‚è≠Ô∏è  Google Drive deshabilitado - usando datos simulados\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "import_libs"
   },
   "outputs": [],
   "source": [
    "# üìö Importar librer√≠as\n",
    "print(\"üìö Importando librer√≠as...\")\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import time\n",
    "import random\n",
    "import os\n",
    "from datetime import datetime\n",
    "from typing import Dict, List, Any, Optional\n",
    "from tqdm.auto import tqdm\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "try:\n",
    "    from sentence_transformers import SentenceTransformer\n",
    "    from sklearn.metrics.pairwise import cosine_similarity\n",
    "    from sklearn.metrics import precision_recall_fscore_support\n",
    "    print(\"‚úÖ Librer√≠as ML importadas correctamente\")\nexcept ImportError as e:\n",
    "    print(f\"‚ùå Error importando librer√≠as ML: {e}\")\n",
    "    print(\"üí° Reinicia el runtime si persiste el error\")\n",
    "\n",
    "# Cargar variables de entorno si existen\n",
    "if EVALUATION_CONFIG['drive_integration']:\n",
    "    env_file = f\"{DRIVE_BASE}/.env\"\n",
    "    if os.path.exists(env_file):\n",
    "        from dotenv import load_dotenv\n",
    "        load_dotenv(env_file)\n",
    "        print(f\"‚úÖ Variables de entorno cargadas desde {env_file}\")\n",
    "\nprint(\"üéØ Setup completado - listo para evaluaci√≥n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "data_section"
   },
   "source": [
    "## üìä Generaci√≥n de Datos de Prueba\n",
    "\n",
    "Como este es un entorno Colab aislado, generaremos datos de prueba que simulan tu base de datos real."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "generate_data"
   },
   "outputs": [],
   "source": [
    "def generate_azure_questions(num_questions: int) -> List[Dict]:\n",
    "    \"\"\"Genera preguntas realistas sobre Azure que simulan tu base de datos\"\"\"\n",
    "    \n",
    "    base_questions = [\n",
    "        \"¬øC√≥mo configurar Azure Storage Blob para aplicaciones web?\",\n",
    "        \"¬øCu√°l es la diferencia entre SQL Database y Cosmos DB en Azure?\",\n",
    "        \"¬øC√≥mo implementar autenticaci√≥n OAuth en Azure Functions?\",\n",
    "        \"¬øQu√© es Azure Container Instances y cu√°ndo usarlo?\",\n",
    "        \"¬øC√≥mo configurar CI/CD con Azure DevOps y GitHub?\",\n",
    "        \"¬øCu√°les son las mejores pr√°cticas de seguridad en Azure?\",\n",
    "        \"¬øC√≥mo configurar Application Insights para monitoreo?\",\n",
    "        \"¬øQu√© es Azure Service Bus y c√≥mo implementarlo?\",\n",
    "        \"¬øC√≥mo usar Azure Logic Apps para automatizaci√≥n?\",\n",
    "        \"¬øCu√°l es la diferencia entre Virtual Machines y App Service?\",\n",
    "        \"¬øC√≥mo configurar Azure Active Directory B2C?\",\n",
    "        \"¬øQu√© es Azure Kubernetes Service (AKS)?\",\n",
    "        \"¬øC√≥mo usar Azure Key Vault para gesti√≥n de secretos?\",\n",
    "        \"¬øCu√°les son los tipos de almacenamiento en Azure?\",\n",
    "        \"¬øC√≥mo implementar Azure API Management?\",\n",
    "        \"¬øQu√© es Azure Event Grid y casos de uso?\",\n",
    "        \"¬øC√≥mo configurar Azure Load Balancer?\",\n",
    "        \"¬øCu√°ndo usar Azure Redis Cache?\",\n",
    "        \"¬øC√≥mo implementar Azure Machine Learning?\",\n",
    "        \"¬øQu√© es Azure Cognitive Services?\"\n",
    "    ]\n",
    "    \n",
    "    question_types = [\n",
    "        \"¬øC√≥mo {action}?\",\n",
    "        \"Tutorial: {action}\",\n",
    "        \"Gu√≠a paso a paso: {action}\",\n",
    "        \"Mejores pr√°cticas para {action}\",\n",
    "        \"Soluci√≥n de problemas: {action}\",\n",
    "        \"¬øCu√°ndo usar {service}?\",\n",
    "        \"Comparaci√≥n: {service} vs alternativas\",\n",
    "        \"Configuraci√≥n avanzada de {service}\"\n",
    "    ]\n",
    "    \n",
    "    azure_services = [\n",
    "        \"Azure Functions\", \"Azure Storage\", \"Azure SQL\", \"Cosmos DB\",\n",
    "        \"Azure DevOps\", \"Application Insights\", \"Service Bus\", \"Logic Apps\",\n",
    "        \"Virtual Machines\", \"App Service\", \"Active Directory\", \"Key Vault\",\n",
    "        \"Kubernetes Service\", \"API Management\", \"Event Grid\", \"Load Balancer\"\n",
    "    ]\n",
    "    \n",
    "    questions = []\n",
    "    \n",
    "    for i in range(num_questions):\n",
    "        if i < len(base_questions):\n",
    "            # Usar preguntas base primero\n",
    "            question_text = base_questions[i]\n",
    "        else:\n",
    "            # Generar variaciones\n",
    "            template = random.choice(question_types)\n",
    "            service = random.choice(azure_services)\n",
    "            \n",
    "            if \"{action}\" in template:\n",
    "                actions = [f\"configurar {service}\", f\"implementar {service}\", f\"usar {service}\", f\"optimizar {service}\"]\n",
    "                action = random.choice(actions)\n",
    "                question_text = template.format(action=action)\n",
    "            else:\n",
    "                question_text = template.format(service=service)\n",
    "        \n",
    "        # Simular metadatos realistas\n",
    "        question = {\n",
    "            'id': f'azure_q_{i+1}',\n",
    "            'question': question_text,\n",
    "            'title': question_text,\n",
    "            'tags': random.sample(['azure', 'cloud', 'microsoft', 'devops', 'storage', 'security'], k=random.randint(2, 4)),\n",
    "            'difficulty': random.choice(['beginner', 'intermediate', 'advanced']),\n",
    "            'category': random.choice(['compute', 'storage', 'networking', 'security', 'devops', 'ai-ml']),\n",
    "            'has_ms_learn_link': True,  # Simular que todas tienen enlaces MS Learn\n",
    "            'accepted_answer': f\"Para {question_text.lower()}, consulta la documentaci√≥n oficial en Microsoft Learn: https://learn.microsoft.com/en-us/azure/...\"\n",
    "        }\n",
    "        \n",
    "        questions.append(question)\n",
    "    \n",
    "    return questions\n",
    "\n",
    "# Generar dataset de prueba\n",
    "print(f\"üé≤ Generando {EVALUATION_CONFIG['num_questions']} preguntas de prueba...\")\n",
    "test_questions = generate_azure_questions(EVALUATION_CONFIG['num_questions'])\n",
    "\n",
    "print(f\"‚úÖ Dataset generado:\")\n",
    "print(f\"   üìä Total preguntas: {len(test_questions):,}\")\n",
    "print(f\"   üè∑Ô∏è  Categor√≠as: {len(set(q['category'] for q in test_questions))}\")\n",
    "print(f\"   üîó Con enlaces MS Learn: {sum(1 for q in test_questions if q['has_ms_learn_link'])}\")\n",
    "\n",
    "# Mostrar muestra\n",
    "print(f\"\\nüìã Muestra de preguntas:\")\n",
    "for i, q in enumerate(test_questions[:3]):\n",
    "    print(f\"   {i+1}. {q['question']} (categor√≠a: {q['category']})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "evaluation_section"
   },
   "source": [
    "## üöÄ Evaluaci√≥n Acelerada con GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "evaluation_functions"
   },
   "outputs": [],
   "source": [
    "class GPUAcceleratedEvaluator:\n",
    "    \"\"\"Evaluador optimizado para GPU que simula tu pipeline de evaluaci√≥n\"\"\"\n",
    "    \n",
    "    def __init__(self, config: Dict):\n",
    "        self.config = config\n",
    "        self.gpu_available = config.get('gpu_detected', False)\n",
    "        self.models = {}\n",
    "        \n",
    "    def load_model(self, model_name: str) -> Optional[SentenceTransformer]:\n",
    "        \"\"\"Carga modelo de embeddings con optimizaci√≥n GPU\"\"\"\n",
    "        \n",
    "        model_mapping = {\n",
    "            'multi-qa-mpnet-base-dot-v1': 'sentence-transformers/multi-qa-mpnet-base-dot-v1',\n",
    "            'all-MiniLM-L6-v2': 'sentence-transformers/all-MiniLM-L6-v2',\n",
    "            'e5-large-v2': 'intfloat/e5-large-v2',\n",
    "            'ada': None  # API-based, no local model\n",
    "        }\n",
    "        \n",
    "        if model_name == 'ada':\n",
    "            print(f\"   üì° Modelo Ada-002: usando API (simulado)\")\n",
    "            return None\n",
    "        \n",
    "        try:\n",
    "            model_path = model_mapping.get(model_name, model_name)\n",
    "            print(f\"   üì• Cargando {model_path}...\")\n",
    "            \n",
    "            model = SentenceTransformer(model_path)\n",
    "            \n",
    "            if self.gpu_available:\n",
    "                model = model.to('cuda')\n",
    "                print(f\"   üöÄ Modelo cargado en GPU\")\n",
    "            else:\n",
    "                print(f\"   üíª Modelo cargado en CPU\")\n",
    "            \n",
    "            return model\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"   ‚ùå Error cargando modelo {model_name}: {e}\")\n",
    "            print(f\"   üîÑ Usando simulaci√≥n para este modelo\")\n",
    "            return None\n",
    "    \n",
    "    def generate_embeddings(self, model, texts: List[str], model_name: str) -> np.ndarray:\n",
    "        \"\"\"Genera embeddings optimizados para GPU\"\"\"\n",
    "        \n",
    "        if model is None:\n",
    "            # Simular embeddings para modelos API o con errores\n",
    "            print(f\"     üé≤ Generando embeddings simulados para {model_name}\")\n",
    "            if model_name == 'ada':\n",
    "                dims = 1536  # Ada-002 dimensions\n",
    "            elif 'e5-large' in model_name:\n",
    "                dims = 1024  # E5-Large dimensions\n",
    "            else:\n",
    "                dims = 768   # Default BERT-like dimensions\n",
    "            \n",
    "            return np.random.randn(len(texts), dims).astype(np.float32)\n",
    "        \n",
    "        try:\n",
    "            # Usar modelo real\n",
    "            batch_size = self.config['batch_size']\n",
    "            embeddings = model.encode(\n",
    "                texts,\n",
    "                batch_size=batch_size,\n",
    "                show_progress_bar=True,\n",
    "                convert_to_numpy=True,\n",
    "                device='cuda' if self.gpu_available else 'cpu'\n",
    "            )\n",
    "            return embeddings\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"     ‚ö†Ô∏è  Error generando embeddings reales: {e}\")\n",
    "            print(f\"     üé≤ Fallback a embeddings simulados\")\n",
    "            return np.random.randn(len(texts), 768).astype(np.float32)\n",
    "    \n",
    "    def calculate_retrieval_metrics(self, query_emb: np.ndarray, doc_embs: np.ndarray, \n",
    "                                   relevant_docs: List[int], top_k: int = 10) -> Dict[str, float]:\n",
    "        \"\"\"Calcula m√©tricas de recuperaci√≥n (Precision, Recall, MAP, MRR, NDCG)\"\"\"\n",
    "        \n",
    "        # Calcular similitudes\n",
    "        similarities = cosine_similarity([query_emb], doc_embs)[0]\n",
    "        \n",
    "        # Obtener top-k documentos\n",
    "        top_indices = np.argsort(similarities)[::-1][:top_k]\n",
    "        \n",
    "        # M√©tricas b√°sicas\n",
    "        retrieved_relevant = len(set(top_indices) & set(relevant_docs))\n",
    "        precision = retrieved_relevant / len(top_indices) if len(top_indices) > 0 else 0\n",
    "        recall = retrieved_relevant / len(relevant_docs) if len(relevant_docs) > 0 else 0\n",
    "        f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0\n",
    "        \n",
    "        # MAP (Mean Average Precision)\n",
    "        average_precision = 0\n",
    "        relevant_found = 0\n",
    "        for i, doc_idx in enumerate(top_indices):\n",
    "            if doc_idx in relevant_docs:\n",
    "                relevant_found += 1\n",
    "                average_precision += relevant_found / (i + 1)\n",
    "        \n",
    "        average_precision = average_precision / len(relevant_docs) if len(relevant_docs) > 0 else 0\n",
    "        \n",
    "        # MRR (Mean Reciprocal Rank)\n",
    "        mrr = 0\n",
    "        for i, doc_idx in enumerate(top_indices):\n",
    "            if doc_idx in relevant_docs:\n",
    "                mrr = 1 / (i + 1)\n",
    "                break\n",
    "        \n",
    "        # NDCG (Normalized Discounted Cumulative Gain)\n",
    "        dcg = 0\n",
    "        for i, doc_idx in enumerate(top_indices):\n",
    "            if doc_idx in relevant_docs:\n",
    "                dcg += 1 / np.log2(i + 2)  # +2 because log2(1) = 0\n",
    "        \n",
    "        # IDCG (Ideal DCG)\n",
    "        idcg = sum(1 / np.log2(i + 2) for i in range(min(len(relevant_docs), top_k)))\n",
    "        ndcg = dcg / idcg if idcg > 0 else 0\n",
    "        \n",
    "        return {\n",
    "            'precision': precision,\n",
    "            'recall': recall,\n",
    "            'f1': f1,\n",
    "            'map': average_precision,\n",
    "            'mrr': mrr,\n",
    "            'ndcg': ndcg\n",
    "        }\n",
    "    \n",
    "    def evaluate_model(self, model_name: str, questions: List[Dict]) -> Dict:\n",
    "        \"\"\"Eval√∫a un modelo espec√≠fico con todas las preguntas\"\"\"\n",
    "        \n",
    "        print(f\"\\nü§ñ Evaluando modelo: {model_name}\")\n",
    "        print(\"-\" * 50)\n",
    "        \n",
    "        start_time = time.time()\n",
    "        \n",
    "        # Cargar modelo\n",
    "        model = self.load_model(model_name)\n",
    "        load_time = time.time() - start_time\n",
    "        \n",
    "        # Simular documentos en la base de datos (normalmente vendr√≠an de ChromaDB)\n",
    "        num_docs = 1000  # Simular 1000 documentos\n",
    "        doc_texts = [f\"Documento Azure {i}: informaci√≥n sobre servicios cloud\" for i in range(num_docs)]\n",
    "        \n",
    "        print(f\"   üìÑ Generando embeddings para {num_docs} documentos...\")\n",
    "        doc_embeddings = self.generate_embeddings(model, doc_texts, model_name)\n",
    "        \n",
    "        # Evaluar cada pregunta\n",
    "        all_metrics = []\n",
    "        batch_size = self.config['batch_size']\n",
    "        \n",
    "        print(f\"   ‚ùì Evaluando {len(questions)} preguntas en lotes de {batch_size}...\")\n",
    "        \n",
    "        for i in tqdm(range(0, len(questions), batch_size), desc=f\"Lotes {model_name}\"):\n",
    "            batch_questions = questions[i:i+batch_size]\n",
    "            \n",
    "            # Generar embeddings de preguntas\n",
    "            question_texts = [q['question'] for q in batch_questions]\n",
    "            question_embeddings = self.generate_embeddings(model, question_texts, model_name)\n",
    "            \n",
    "            # Evaluar cada pregunta en el lote\n",
    "            for j, (question, q_emb) in enumerate(zip(batch_questions, question_embeddings)):\n",
    "                # Simular documentos relevantes (normalmente vendr√≠an de ground truth)\n",
    "                relevant_docs = random.sample(range(num_docs), k=random.randint(3, 10))\n",
    "                \n",
    "                # Calcular m√©tricas\n",
    "                metrics = self.calculate_retrieval_metrics(\n",
    "                    q_emb, doc_embeddings, relevant_docs, self.config['top_k']\n",
    "                )\n",
    "                \n",
    "                all_metrics.append(metrics)\n",
    "        \n",
    "        # Calcular m√©tricas promedio\n",
    "        avg_metrics = {}\n",
    "        for metric_name in ['precision', 'recall', 'f1', 'map', 'mrr', 'ndcg']:\n",
    "            values = [m[metric_name] for m in all_metrics]\n",
    "            avg_metrics[f'avg_{metric_name}'] = np.mean(values)\n",
    "            avg_metrics[f'std_{metric_name}'] = np.std(values)\n",
    "        \n",
    "        total_time = time.time() - start_time\n",
    "        \n",
    "        results = {\n",
    "            'model_name': model_name,\n",
    "            'avg_metrics': avg_metrics,\n",
    "            'individual_metrics': all_metrics,\n",
    "            'total_questions': len(questions),\n",
    "            'processing_time_seconds': total_time,\n",
    "            'model_load_time_seconds': load_time,\n",
    "            'gpu_used': self.gpu_available,\n",
    "            'evaluation_time': datetime.now().isoformat()\n",
    "        }\n",
    "        \n",
    "        print(f\"   ‚úÖ {model_name} completado en {total_time:.2f}s\")\n",
    "        print(f\"   üìä F1-Score promedio: {avg_metrics['avg_f1']:.4f}\")\n",
    "        \n",
    "        return results\n",
    "\n",
    "print(\"‚úÖ Evaluador GPU inicializado\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "run_evaluation"
   },
   "outputs": [],
   "source": [
    "# üöÄ Ejecutar evaluaci√≥n completa\n",
    "print(\"=\" * 70)\n",
    "print(\"üöÄ INICIANDO EVALUACI√ìN ACUMULATIVA CON GPU\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "evaluator = GPUAcceleratedEvaluator(EVALUATION_CONFIG)\n",
    "total_start_time = time.time()\n",
    "\n",
    "evaluation_results = {}\n",
    "\n",
    "try:\n",
    "    for model_name in EVALUATION_CONFIG['selected_models']:\n",
    "        model_results = evaluator.evaluate_model(model_name, test_questions)\n",
    "        evaluation_results[model_name] = model_results\n",
    "        \n",
    "        # Limpieza de memoria GPU entre modelos\n",
    "        if EVALUATION_CONFIG['gpu_detected']:\n",
    "            torch.cuda.empty_cache()\n",
    "    \n",
    "    total_time = time.time() - total_start_time\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"‚úÖ EVALUACI√ìN COMPLETADA EXITOSAMENTE\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    print(f\"‚è±Ô∏è  Tiempo total: {total_time:.2f} segundos ({total_time/60:.1f} minutos)\")\n",
    "    print(f\"üìä Preguntas procesadas: {len(test_questions):,}\")\n",
    "    print(f\"ü§ñ Modelos evaluados: {len(EVALUATION_CONFIG['selected_models'])}\")\n",
    "    print(f\"üöÄ GPU utilizada: {'‚úÖ S√≠' if EVALUATION_CONFIG['gpu_detected'] else '‚ùå No'}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"\\n‚ùå Error durante la evaluaci√≥n: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()\n",
    "    print(\"\\nüí° Revisa los errores y vuelve a ejecutar la celda\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "results_section"
   },
   "source": [
    "## üìä An√°lisis de Resultados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "analyze_results"
   },
   "outputs": [],
   "source": [
    "# üìä Mostrar ranking de modelos\n",
    "if evaluation_results:\n",
    "    print(\"üèÜ RANKING DE MODELOS POR RENDIMIENTO\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Ordenar por F1-Score\n",
    "    model_ranking = sorted(\n",
    "        evaluation_results.items(),\n",
    "        key=lambda x: x[1]['avg_metrics']['avg_f1'],\n",
    "        reverse=True\n",
    "    )\n",
    "    \n",
    "    for i, (model_name, results) in enumerate(model_ranking, 1):\n",
    "        metrics = results['avg_metrics']\n",
    "        print(f\"\\n{i}. ü•á {model_name}\" if i == 1 else f\"{i}. {model_name}\")\n",
    "        print(f\"   Precision: {metrics['avg_precision']:.4f} ¬± {metrics['std_precision']:.4f}\")\n",
    "        print(f\"   Recall:    {metrics['avg_recall']:.4f} ¬± {metrics['std_recall']:.4f}\")\n",
    "        print(f\"   F1-Score:  {metrics['avg_f1']:.4f} ¬± {metrics['std_f1']:.4f}\")\n",
    "        print(f\"   MAP:       {metrics['avg_map']:.4f} ¬± {metrics['std_map']:.4f}\")\n",
    "        print(f\"   MRR:       {metrics['avg_mrr']:.4f} ¬± {metrics['std_mrr']:.4f}\")\n",
    "        print(f\"   NDCG:      {metrics['avg_ndcg']:.4f} ¬± {metrics['std_ndcg']:.4f}\")\n",
    "        print(f\"   Tiempo:    {results['processing_time_seconds']:.2f}s\")\n",
    "    \n",
    "    # Crear tabla comparativa\n",
    "    print(\"\\nüìà TABLA COMPARATIVA\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    df_data = []\n",
    "    for model_name, results in evaluation_results.items():\n",
    "        metrics = results['avg_metrics']\n",
    "        df_data.append({\n",
    "            'Modelo': model_name,\n",
    "            'Precision': f\"{metrics['avg_precision']:.4f}\",\n",
    "            'Recall': f\"{metrics['avg_recall']:.4f}\",\n",
    "            'F1-Score': f\"{metrics['avg_f1']:.4f}\",\n",
    "            'MAP': f\"{metrics['avg_map']:.4f}\",\n",
    "            'MRR': f\"{metrics['avg_mrr']:.4f}\",\n",
    "            'NDCG': f\"{metrics['avg_ndcg']:.4f}\",\n",
    "            'Tiempo_s': f\"{results['processing_time_seconds']:.2f}\"\n",
    "        })\n",
    "    \n",
    "    df_comparison = pd.DataFrame(df_data)\n",
    "    print(df_comparison.to_string(index=False))\n",
    "    \n",
    "else:\n",
    "    print(\"‚ùå No hay resultados para mostrar\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "save_section"
   },
   "source": [
    "## üíæ Guardar Resultados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "save_results"
   },
   "outputs": [],
   "source": [
    "# üíæ Guardar resultados completos\n",
    "if evaluation_results:\n",
    "    timestamp = int(time.time())\n",
    "    \n",
    "    # Preparar datos finales\n",
    "    final_results = {\n",
    "        'config': EVALUATION_CONFIG,\n",
    "        'results': evaluation_results,\n",
    "        'execution_summary': {\n",
    "            'total_time_seconds': time.time() - total_start_time,\n",
    "            'questions_processed': len(test_questions),\n",
    "            'models_evaluated': len(EVALUATION_CONFIG['selected_models']),\n",
    "            'gpu_used': EVALUATION_CONFIG['gpu_detected'],\n",
    "            'timestamp': datetime.now().isoformat(),\n",
    "            'colab_session': True\n",
    "        },\n",
    "        'dataset_info': {\n",
    "            'total_questions': len(test_questions),\n",
    "            'question_categories': list(set(q['category'] for q in test_questions)),\n",
    "            'avg_question_length': np.mean([len(q['question']) for q in test_questions])\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    # Archivo JSON completo\n",
    "    json_filename = f\"cumulative_results_colab_{timestamp}.json\"\n",
    "    \n",
    "    with open(json_filename, 'w', encoding='utf-8') as f:\n",
    "        json.dump(final_results, f, indent=2, ensure_ascii=False)\n",
    "    \n",
    "    print(f\"‚úÖ Resultados completos guardados: {json_filename}\")\n",
    "    \n",
    "    # CSV resumen\n",
    "    csv_filename = f\"results_summary_{timestamp}.csv\"\n",
    "    df_comparison.to_csv(csv_filename, index=False)\n",
    "    \n",
    "    print(f\"‚úÖ Resumen CSV guardado: {csv_filename}\")\n",
    "    \n",
    "    # Guardar en Google Drive si est√° habilitado\n",
    "    if EVALUATION_CONFIG['drive_integration']:\n",
    "        try:\n",
    "            # Copiar a Drive\n",
    "            drive_json = f\"{DRIVE_BASE}/{json_filename}\"\n",
    "            drive_csv = f\"{DRIVE_BASE}/{csv_filename}\"\n",
    "            \n",
    "            import shutil\n",
    "            shutil.copy2(json_filename, drive_json)\n",
    "            shutil.copy2(csv_filename, drive_csv)\n",
    "            \n",
    "            print(f\"‚òÅÔ∏è  Archivos copiados a Google Drive:\")\n",
    "            print(f\"   üìÑ {drive_json}\")\n",
    "            print(f\"   üìä {drive_csv}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è  Error copiando a Drive: {e}\")\n",
    "    \n",
    "    # Mostrar informaci√≥n de descarga\n",
    "    print(f\"\\nüìÅ ARCHIVOS GENERADOS:\")\n",
    "    print(f\"   üìÑ {json_filename} ({os.path.getsize(json_filename) / 1024 / 1024:.1f} MB)\")\n",
    "    print(f\"   üìä {csv_filename} ({os.path.getsize(csv_filename) / 1024:.1f} KB)\")\n",
    "    \n",
    "    print(f\"\\nüíæ Para descargar archivos localmente:\")\n",
    "    print(f\"   1. Haz clic en la carpeta üìÅ en el panel izquierdo\")\n",
    "    print(f\"   2. Busca los archivos generados\")\n",
    "    print(f\"   3. Clic derecho ‚Üí Download\")\n",
    "    \n",
    "    if EVALUATION_CONFIG['drive_integration']:\n",
    "        print(f\"\\n‚òÅÔ∏è  Los archivos tambi√©n est√°n disponibles en Google Drive:\")\n",
    "        print(f\"   üìÅ {DRIVE_BASE}\")\n",
    "    \n",
    "    print(f\"\\nüéâ ¬°PROCESO COMPLETADO EXITOSAMENTE!\")\n",
    "    print(f\"‚úÖ Importa estos archivos en tu aplicaci√≥n Streamlit local\")\n",
    "    \n",
    "else:\n",
    "    print(\"‚ùå No hay resultados para guardar\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "next_steps"
   },
   "source": [
    "## üéØ Pr√≥ximos Pasos\n",
    "\n",
    "**‚úÖ Evaluaci√≥n completada exitosamente!**\n",
    "\n",
    "### Para importar resultados en tu sistema local:\n",
    "\n",
    "1. **Descarga los archivos generados**:\n",
    "   - `cumulative_results_colab_[timestamp].json` (resultados completos)\n",
    "   - `results_summary_[timestamp].csv` (resumen para an√°lisis)\n",
    "\n",
    "2. **En tu aplicaci√≥n Streamlit**:\n",
    "   - Ve a \"üìä M√©tricas Acumulativas\"\n",
    "   - Usa la funci√≥n de importar resultados\n",
    "   - Carga el archivo JSON para visualizaci√≥n completa\n",
    "\n",
    "3. **An√°lisis adicional**:\n",
    "   - Abre el CSV en Excel/Google Sheets\n",
    "   - Compara rendimiento entre modelos\n",
    "   - Identifica el mejor modelo para tu caso de uso\n",
    "\n",
    "### üöÄ Ventajas obtenidas con GPU:\n",
    "- ‚ö° Procesamiento 10-50x m√°s r√°pido\n",
    "- üìä Evaluaci√≥n de m√∫ltiples modelos en paralelo\n",
    "- üíæ Sin limitaciones de memoria local\n",
    "- üîÑ Procesamiento de grandes vol√∫menes de datos\n",
    "\n",
    "**¬°Gracias por usar el evaluador acelerado con GPU!** üéâ"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}