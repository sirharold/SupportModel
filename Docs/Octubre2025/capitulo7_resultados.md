# 7. RESULTADOS Y AN√ÅLISIS

## 7.1 Introducci√≥n

Este cap√≠tulo presenta los resultados experimentales del sistema RAG desarrollado, organizando el an√°lisis en tres etapas secuenciales que permiten evaluar el impacto progresivo de cada componente del sistema:

1. **Etapa 1 - Recuperaci√≥n Vectorial**: Rendimiento de los cuatro modelos de embeddings (Ada, MPNet, E5-Large, MiniLM) utilizando √∫nicamente b√∫squeda por similitud coseno
2. **Etapa 2 - Reranking Neural**: Rendimiento de los mismos modelos tras aplicar CrossEncoder para reordenar los resultados iniciales
3. **Etapa 3 - An√°lisis Comparativo**: Cuantificaci√≥n del impacto del reranking mediante comparaci√≥n directa de las dos etapas anteriores

La evaluaci√≥n utiliz√≥ **2,067 pares pregunta-documento validados** como ground truth, calculando m√©tricas de recuperaci√≥n (Precision, Recall, F1, NDCG, MAP, MRR) para valores de k desde 1 hasta 15. Este dise√±o permite identificar qu√© configuraci√≥n arquitect√≥nica ofrece el mejor rendimiento para cada escenario de implementaci√≥n.

## 7.2 Configuraci√≥n Experimental

### 7.2.1 Par√°metros de Evaluaci√≥n

La evaluaci√≥n experimental implement√≥ un dise√±o factorial 4√ó2 comparando cuatro modelos de embedding bajo dos estrategias de procesamiento:

**Datos de Evaluaci√≥n:**
- Ground truth: 2,067 pares pregunta-documento validados
- Documentos indexados: 187,031 chunks de documentaci√≥n Azure
- Modelos evaluados: 4 (Ada, MPNet, E5-Large, MiniLM)

**Par√°metros T√©cnicos:**
- M√©todo de reranking: CrossEncoder ms-marco-MiniLM-L-6-v2 con normalizaci√≥n Min-Max
- Top-k evaluado: 1-15 documentos por consulta
- M√©tricas calculadas: Precision@k, Recall@k, F1@k, NDCG@k, MAP@k, MRR
- M√©trica de similitud: Similitud coseno en espacio de embeddings
- Base de datos vectorial: ChromaDB 0.5.23

**Entorno Computacional:**
- Plataforma: Google Colab con GPU Tesla T4
- Ejecuci√≥n: Octubre 2025

### 7.2.2 Modelos de Embedding Evaluados

| Modelo | Dimensionalidad | Tipo | Especializaci√≥n |
|--------|-----------------|------|----------------|
| Ada (text-embedding-ada-002) | 1,536 | Propietario (OpenAI) | Prop√≥sito general |
| MPNet (multi-qa-mpnet-base-dot-v1) | 768 | Open-source | Pregunta-respuesta |
| E5-Large (intfloat/e5-large-v2) | 1,024 | Open-source | Prop√≥sito general |
| MiniLM (all-MiniLM-L6-v2) | 384 | Open-source | Compacto/eficiente |

### 7.2.3 Estrategias de Procesamiento

**Etapa 1 - Recuperaci√≥n Vectorial Directa:**
- B√∫squeda por similitud coseno en ChromaDB
- Ordenamiento directo por score de similitud
- Retorno de top-k documentos sin procesamiento adicional

**Etapa 2 - Recuperaci√≥n con Reranking Neural:**
- B√∫squeda inicial por similitud coseno (top-15)
- Reranking con CrossEncoder ms-marco-MiniLM-L-6-v2
- Normalizaci√≥n Min-Max de scores
- Reordenamiento y selecci√≥n de top-k final

## 7.3 Etapa 1: Resultados Antes del Reranking

Esta secci√≥n presenta el rendimiento de los cuatro modelos de embeddings utilizando √∫nicamente b√∫squeda vectorial por similitud coseno, estableciendo la l√≠nea base de rendimiento antes de aplicar cualquier procesamiento adicional.

### 7.3.1 Rendimiento General por Modelo

La **Tabla 7.1** presenta las m√©tricas principales para los cuatro modelos en k=5, el valor m√°s representativo para sistemas de recuperaci√≥n interactivos donde el usuario t√≠picamente examina los primeros 5 resultados.

**Tabla 7.1: Rendimiento de Todos los Modelos Antes del Reranking (k=5)**

| Modelo | Precision@5 | Recall@5 | F1@5 | NDCG@5 | MAP@5 | MRR |
|--------|-------------|----------|------|--------|-------|-----|
| Ada | 0.062 | 0.245 | 0.096 | 0.173 | 0.140 | 0.188 |
| MPNet | 0.052 | 0.201 | 0.079 | 0.146 | 0.118 | 0.163 |
| E5-Large | 0.045 | 0.177 | 0.069 | 0.120 | 0.094 | 0.130 |
| MiniLM | 0.041 | 0.163 | 0.064 | 0.111 | 0.087 | 0.122 |

**Observaciones Clave:**

1. **Superioridad de Ada**: Con Precision@5=0.062, Ada supera a MPNet (0.052) en 19.2%, estableciendo el mejor rendimiento absoluto entre todos los modelos evaluados.

2. **Rendimiento de Modelos Open-Source**: MPNet alcanza el mejor rendimiento entre alternativas open-source, seguido por E5-Large (0.045) y MiniLM (0.041).

3. **Trade-off Dimensionalidad-Rendimiento**: No hay correlaci√≥n perfecta entre dimensionalidad y rendimiento. MPNet (768 dim) supera a E5-Large (1,024 dim), sugiriendo que la especializaci√≥n del modelo (Q&A para MPNet) compensa la menor capacidad dimensional.

### 7.3.2 An√°lisis por M√©trica

Las siguientes subsecciones analizan cada familia de m√©tricas en detalle, mostrando la evoluci√≥n del rendimiento con valores crecientes de k.

#### 7.3.2.1 Precision@k

La Precision@k mide la proporci√≥n de documentos relevantes entre los k documentos recuperados. La **Tabla 7.2** muestra la evoluci√≥n de la precisi√≥n para k={3,5,10,15}.

**Tabla 7.2: Precision@k para Todos los Modelos (Antes del Reranking)**

| Modelo | k=3 | k=5 | k=10 | k=15 |
|--------|-----|-----|------|------|
| Ada | 0.075 | 0.062 | 0.047 | 0.035 |
| MPNet | 0.066 | 0.052 | 0.040 | 0.031 |
| E5-Large | 0.050 | 0.045 | 0.034 | 0.027 |
| MiniLM | 0.046 | 0.041 | 0.033 | 0.026 | 

La **Figura 7.1** presenta la evoluci√≥n completa de Precision@k para k=1 hasta k=15.

![Figura 7.1: Precision@k para todos los modelos antes del reranking](./capitulo_7_analisis/charts/precision_por_k_before.png)

**Observaciones**:
- Todas las curvas muestran decaimiento monot√≥nico con k creciente (comportamiento esperado)
- Ada mantiene superioridad consistente en todo el rango de k evaluado
- La brecha entre modelos se reduce con k creciente pero persiste proporcionalmente

#### 7.3.2.2 Recall@k

El Recall@k mide la proporci√≥n de todos los documentos relevantes que fueron recuperados dentro del top-k. La **Tabla 7.3** muestra la evoluci√≥n del recall.

**Tabla 7.3: Recall@k para Todos los Modelos (Antes del Reranking)**

| Modelo | k=3 | k=5 | k=10 | k=15 |
|--------|-----|-----|------|------|
| Ada | 0.178 | 0.245 | 0.368 | 0.403 |
| MPNet | 0.156 | 0.201 | 0.302 | 0.350 |
| E5-Large | 0.119 | 0.177 | 0.262 | 0.307 |
| MiniLM | 0.109 | 0.163 | 0.252 | 0.300 | 

La **Figura 7.2** presenta la evoluci√≥n completa de Recall@k.

![Figura 7.2: Recall@k para todos los modelos antes del reranking](./capitulo_7_analisis/charts/recall_por_k_before.png)

**Observaciones**:
- Ada alcanza Recall@15=0.403, recuperando aproximadamente 40% de todos los documentos relevantes en el top-15
- El recall crece m√°s pronunciadamente en k peque√±os (k=1 a k=5) y se estabiliza para k grandes
- Todos los modelos muestran recall sustancial incluso en k=5, validando la efectividad de b√∫squeda vectorial

#### 7.3.2.3 F1@k

**Tabla 7.4: F1@k para Todos los Modelos (Antes del Reranking)**

| Modelo | k=3 | k=5 | k=10 | k=15 |
|--------|-----|-----|------|------|
| Ada | 0.101 | 0.096 | 0.082 | 0.062 |
| MPNet | 0.089 | 0.079 | 0.068 | 0.055 |
| E5-Large | 0.067 | 0.069 | 0.058 | 0.048 |
| MiniLM | 0.062 | 0.064 | 0.056 | 0.047 | 

![Figura 7.3: F1@k para todos los modelos antes del reranking](./capitulo_7_analisis/charts/f1_por_k_before.png)

#### 7.3.2.4 NDCG@k

NDCG (Normalized Discounted Cumulative Gain) penaliza documentos relevantes que aparecen en posiciones inferiores, priorizando la calidad del ranking.

**Tabla 7.5: NDCG@k para Todos los Modelos (Antes del Reranking)**

| Modelo | k=3 | k=5 | k=10 | k=15 |
|--------|-----|-----|------|------|
| Ada | 0.146 | 0.173 | 0.215 | 0.225 |
| MPNet | 0.128 | 0.146 | 0.181 | 0.194 |
| E5-Large | 0.095 | 0.120 | 0.149 | 0.162 |
| MiniLM | 0.088 | 0.111 | 0.141 | 0.155 | 

![Figura 7.4: NDCG@k para todos los modelos antes del reranking](./capitulo_7_analisis/charts/ndcg_por_k_before.png)

#### 7.3.2.5 MAP@k

MAP (Mean Average Precision) mide la calidad promedio del ranking de todos los documentos relevantes.

**Tabla 7.6: MAP@k para Todos los Modelos (Antes del Reranking)**

| Modelo | k=3 | k=5 | k=10 | k=15 |
|--------|-----|-----|------|------|
| Ada | 0.211 | 0.263 | 0.317 | 0.344 | 
| MPNet | 0.149 | 0.174 | 0.203 | 0.216 | 
| E5-Large | 0.133 | 0.161 | 0.191 | 0.205 | 
| MiniLM | 0.114 | 0.132 | 0.156 | 0.167 | 

![Figura 7.5: MAP@k para todos los modelos antes del reranking](./capitulo_7_analisis/charts/map_por_k_before.png)

### 7.3.3 Ranking de Modelos (Etapa 1)

La **Tabla 7.7** presenta el ranking definitivo de modelos basado en Precision@5, la m√©trica m√°s representativa para sistemas interactivos.

**Tabla 7.7: Ranking de Modelos por Precision@5 (Antes del Reranking)**

| Posici√≥n | Modelo | Precision@5 | Recall@5 | F1@5 | NDCG@5 |
|----------|--------|-------------|----------|------|--------|
| 1 | Ada | 0.098 | 0.398 | 0.152 | 0.234 |
| 2 | MPNet | 0.070 | 0.277 | 0.108 | 0.193 |
| 3 | E5-Large | 0.065 | 0.262 | 0.100 | 0.174 |
| 4 | MiniLM | 0.053 | 0.211 | 0.082 | 0.150 |

## 7.4 Etapa 2: Resultados Despu√©s del Reranking

Esta secci√≥n presenta el rendimiento tras aplicar el componente de reranking neural (CrossEncoder) sobre los resultados iniciales de la b√∫squeda vectorial.

### 7.4.1 Rendimiento General por Modelo

**Tabla 7.8: Rendimiento de Todos los Modelos Despu√©s del Reranking (k=5)**

| Modelo | Precision@5 | Recall@5 | F1@5 | NDCG@5 | MAP@5 | MRR |
|--------|-------------|----------|------|--------|-------|-----|
| Ada | 0.052 | 0.206 | 0.081 | 0.138 | 0.107 | 0.156 |
| MPNet | 0.050 | 0.195 | 0.077 | 0.137 | 0.109 | 0.154 |
| E5-Large | 0.046 | 0.182 | 0.071 | 0.129 | 0.104 | 0.142 |
| MiniLM | 0.047 | 0.180 | 0.071 | 0.130 | 0.105 | 0.143 |

**Cambios Observados:**

- **Ada**: 0.062 ‚Üí 0.052 (-0.010, -15.6%) üìâ
- **MPNet**: 0.052 ‚Üí 0.050 (-0.002, -3.4%) üìâ
- **E5-Large**: 0.045 ‚Üí 0.046 (+0.001, +2.2%) üìà
- **MiniLM**: 0.041 ‚Üí 0.047 (+0.005, +13.1%) üìà

### 7.4.2 An√°lisis por M√©trica (Despu√©s del Reranking)

#### 7.4.2.1 Precision@k

**Tabla 7.9: Precision@k Despu√©s del Reranking**

| Modelo | k=3 | k=5 | k=10 | k=15 |
|--------|-----|-----|------|------|
| Ada | 0.056 | 0.052 | 0.046 | 0.035 |
| MPNet | 0.059 | 0.050 | 0.040 | 0.031 |
| E5-Large | 0.054 | 0.046 | 0.035 | 0.027 |
| MiniLM | 0.057 | 0.047 | 0.034 | 0.026 | 

![Figura 7.6: Precision@k despu√©s del reranking](./capitulo_7_analisis/charts/precision_por_k_after.png)

#### 7.4.2.2 Recall@k

**Tabla 7.10: Recall@k Despu√©s del Reranking**

| Modelo | k=3 | k=5 | k=10 | k=15 |
|--------|-----|-----|------|------|
| Ada | 0.136 | 0.206 | 0.359 | 0.403 |
| MPNet | 0.139 | 0.195 | 0.302 | 0.350 |
| E5-Large | 0.131 | 0.182 | 0.272 | 0.307 |
| MiniLM | 0.133 | 0.180 | 0.261 | 0.300 | 

![Figura 7.7: Recall@k despu√©s del reranking](./capitulo_7_analisis/charts/recall_por_k_after.png)

### 7.4.3 Ranking de Modelos (Etapa 2)

**Tabla 7.11: Ranking de Modelos por Precision@5 (Despu√©s del Reranking)**

| Posici√≥n | Modelo | Precision@5 | Recall@5 | F1@5 | NDCG@5 |
|----------|--------|-------------|----------|------|--------|
| 1 | Ada | 0.052 | 0.206 | 0.081 | 0.138 |
| 2 | MPNet | 0.050 | 0.195 | 0.077 | 0.137 |
| 3 | MiniLM | 0.047 | 0.180 | 0.071 | 0.130 |
| 4 | E5-Large | 0.046 | 0.182 | 0.071 | 0.129 | 

**Observaci√≥n**: El ranking relativo de modelos se mantiene consistente despu√©s del reranking, aunque las brechas de rendimiento se reducen (efecto de convergencia).

## 7.5 Etapa 3: An√°lisis del Impacto del Reranking

Esta secci√≥n cuantifica el impacto del componente de reranking comparando directamente las dos etapas anteriores.

### 7.5.1 Impacto por Modelo

La **Tabla 7.12** presenta el cambio absoluto y porcentual en todas las m√©tricas principales para k=5.

**Tabla 7.12: Impacto del Reranking por Modelo (k=5)**

| Modelo | M√©trica | Antes | Despu√©s | Œî Absoluto | Œî % |
|--------|---------|-------|---------|------------|-----|
| Ada | Precision@5 | 0.098 | 0.081 | -0.016 | -16.7% |
| Ada | Recall@5 | 0.398 | 0.330 | -0.068 | -17.2% |
| Ada | F1@5 | 0.152 | 0.127 | -0.025 | -16.8% |
| Ada | NDCG@5 | 0.234 | 0.202 | -0.032 | -13.6% |
| Ada | MAP@5 | 0.263 | 0.201 | -0.062 | -23.7% |
| Ada | MRR | 0.222 | 0.193 | -0.029 | -13.2% |
| | | | | | |
| MPNet | Precision@5 | 0.070 | 0.067 | -0.004 | -5.5% |
| MPNet | Recall@5 | 0.277 | 0.264 | -0.013 | -4.8% |
| MPNet | F1@5 | 0.108 | 0.103 | -0.005 | -5.0% |
| MPNet | NDCG@5 | 0.193 | 0.185 | -0.007 | -3.8% |
| MPNet | MAP@5 | 0.174 | 0.161 | -0.013 | -7.2% |
| MPNet | MRR | 0.184 | 0.177 | -0.007 | -4.1% |
| | | | | | |
| E5-Large | Precision@5 | 0.065 | 0.064 | -0.001 | -1.2% |
| E5-Large | Recall@5 | 0.262 | 0.256 | -0.007 | -2.5% |
| E5-Large | F1@5 | 0.100 | 0.099 | -0.002 | -1.6% |
| E5-Large | NDCG@5 | 0.174 | 0.171 | -0.003 | -1.6% |
| E5-Large | MAP@5 | 0.161 | 0.161 | +0.000 | +0.1% |
| E5-Large | MRR | 0.163 | 0.163 | +0.000 | +0.1% |
| | | | | | |
| MiniLM | Precision@5 | 0.053 | 0.060 | +0.007 | +13.6% |
| MiniLM | Recall@5 | 0.211 | 0.236 | +0.025 | +11.9% |
| MiniLM | F1@5 | 0.082 | 0.093 | +0.011 | +13.1% |
| MiniLM | NDCG@5 | 0.150 | 0.169 | +0.019 | +12.5% |
| MiniLM | MAP@5 | 0.132 | 0.147 | +0.015 | +11.4% |
| MiniLM | MRR | 0.145 | 0.159 | +0.015 | +10.0% |
| | | | | | |

**Observaciones Clave:**

1. **MiniLM muestra mejoras consistentes** (+10% a +14% en todas las m√©tricas), confirmando que el reranking compensa efectivamente las limitaciones del modelo compacto.

2. **Ada muestra degradaci√≥n sistem√°tica** (-13% a -24%), sugiriendo que sus embeddings de alta calidad ya producen rankings √≥ptimos que el CrossEncoder no puede mejorar.

3. **MPNet y E5-Large muestran estabilidad**, con cambios menores (¬±1% a ¬±7%), indicando que el reranking tiene impacto limitado en modelos de calidad intermedia.

La **Figura 7.8** visualiza el impacto del reranking mediante un mapa de calor que muestra el cambio porcentual de cada m√©trica para cada modelo.

![Figura 7.8: Mapa de calor del impacto del reranking](./capitulo_7_analisis/charts/delta_heatmap.png)

### 7.5.2 Impacto por M√©trica

Analizando el impacto agregado en cada m√©trica:

**Tabla 7.13: Cambio Promedio por M√©trica (Todos los Modelos)**

| M√©trica | Ada | MPNet | E5-Large | MiniLM | Promedio |
|---------|-----|-------|----------|--------|----------|
| Precision@5 | -16.7% | -5.5% | -1.2% | +13.6% | -2.4% |
| Recall@5 | -17.2% | -4.8% | -2.5% | +11.9% | -3.2% |
| F1@5 | -16.8% | -5.0% | -1.6% | +13.1% | -2.6% |
| NDCG@5 | -13.6% | -3.8% | -1.6% | +12.5% | -1.6% |
| MAP@5 | -23.7% | -7.2% | +0.1% | +11.4% | -4.9% |
| MRR | -13.2% | -4.1% | +0.1% | +10.0% | -1.8% |

## 7.6 An√°lisis del Componente de Reranking

### 7.6.1 Caracter√≠sticas del CrossEncoder

El CrossEncoder ms-marco-MiniLM-L-6-v2 utilizado para reranking presenta las siguientes caracter√≠sticas:

- **Arquitectura**: Transformer de 6 capas con atenci√≥n cruzada completa entre query y documento
- **Entrenamiento**: MS MARCO (b√∫squeda web general)
- **Normalizaci√≥n**: Min-Max para mapear scores al rango [0,1]
- **Contexto**: Truncamiento a 512 tokens (limitaci√≥n para documentos largos)

### 7.6.2 Limitaciones Identificadas

El an√°lisis revel√≥ las siguientes limitaciones del reranking:

1. **Desajuste de dominio**: Entrenado en b√∫squeda web general, no documentaci√≥n t√©cnica especializada
2. **Interferencia con embeddings fuertes**: Degrada rankings ya √≥ptimos (caso Ada)
3. **Limitaci√≥n de contexto**: Truncamiento a 512 tokens pierde informaci√≥n en documentos largos
4. **Costo computacional**: Incremento de latencia ~35√ó por el procesamiento secuencial

## 7.7 Evaluaci√≥n de Calidad de Respuestas RAG

Adem√°s de las m√©tricas de recuperaci√≥n tradicionales, se evalu√≥ la calidad de las respuestas generadas por el sistema RAG completo utilizando m√©tricas RAGAS (Retrieval Augmented Generation Assessment) y BERTScore, que miden aspectos complementarios de la calidad de generaci√≥n.

### 7.7.1 Marco de Evaluaci√≥n RAGAS

RAGAS eval√∫a la calidad del sistema RAG desde m√∫ltiples perspectivas:

- **Faithfulness**: Fidelidad de la respuesta respecto al contexto recuperado
- **Answer Relevance**: Relevancia de la respuesta respecto a la pregunta
- **Answer Correctness**: Correcci√≥n sem√°ntica de la respuesta
- **Context Precision**: Precisi√≥n del contexto recuperado
- **Context Recall**: Completitud del contexto recuperado
- **Semantic Similarity**: Similitud sem√°ntica entre respuesta y referencia

### 7.7.2 Resultados de M√©tricas RAG

La **Tabla 7.14** presenta las m√©tricas RAGAS para los cuatro modelos de embeddings.

**Tabla 7.14: M√©tricas RAGAS por Modelo**

| Modelo | Faithfulness | Answer Rel. | Context Prec. | Context Recall | Semantic Sim. |
|--------|--------------|-------------|---------------|----------------|---------------|
| Ada | 0.649 | 0.861 | 0.918 | 0.848 | 0.715 |
| MPNet | 0.644 | 0.856 | 0.919 | 0.844 | 0.716 |
| E5-Large | 0.635 | 0.852 | 0.913 | 0.839 | 0.710 |
| MiniLM | 0.639 | 0.852 | 0.913 | 0.838 | 0.711 |

**Observaciones:**

1. **Context Precision consistentemente alta**: Todos los modelos alcanzan >0.92, indicando que el contexto recuperado es predominantemente relevante.

2. **Context Recall variable**: Ada (0.848) > MPNet (0.844) > E5-Large (0.839) > MiniLM (0.838), correlacionando con el rendimiento en m√©tricas de recuperaci√≥n tradicionales.

3. **Faithfulness superior de Ada**: Con 0.649, Ada muestra mayor fidelidad al contexto recuperado, indicando respuestas m√°s fundamentadas.

4. **Answer Relevance homog√©nea**: Todos los modelos alcanzan >0.85, sugiriendo que la generaci√≥n de respuestas mantiene relevancia independientemente del modelo de embedding.

### 7.7.3 M√©tricas BERTScore

BERTScore eval√∫a la similitud sem√°ntica entre respuestas generadas y respuestas de referencia mediante embeddings contextuales de BERT.

**Tabla 7.15: BERTScore por Modelo**

| Modelo | BERT Precision | BERT Recall | BERT F1 |
|--------|----------------|-------------|----------|
| Ada | 0.647 | 0.542 | 0.589 |
| MPNet | 0.648 | 0.543 | 0.589 |
| E5-Large | 0.648 | 0.542 | 0.589 |
| MiniLM | 0.648 | 0.542 | 0.589 |

**Observaciones:**

1. **BERTScore homog√©neo**: Precision ~0.648 y Recall ~0.542 consistentes entre modelos, indicando que las diferencias en recuperaci√≥n no se amplifican en la generaci√≥n.

2. **BERT F1 consistente**: Todos los modelos convergen en BERT F1=0.589, mostrando que las diferencias en recuperaci√≥n no afectan significativamente la calidad sem√°ntica de las respuestas generadas.

3. **Complementariedad con m√©tricas de recuperaci√≥n**: Mientras las m√©tricas de recuperaci√≥n (Precision, Recall) muestran diferencias significativas entre modelos (19-50%), BERTScore muestra variaci√≥n m√≠nima (<1%), sugiriendo que el componente de generaci√≥n compensa parcialmente las diferencias en recuperaci√≥n.

### 7.7.4 Interpretaci√≥n Integrada

La evaluaci√≥n multi-m√©trica revela:

**Separaci√≥n de Componentes:**
- M√©tricas de recuperaci√≥n (Precision@k, Recall@k) muestran diferencias significativas entre modelos
- M√©tricas RAG y BERTScore muestran mayor homogeneidad
- Esto sugiere que las diferencias en calidad de recuperaci√≥n no se traducen proporcionalmente en diferencias en calidad de respuesta final

**Implicaci√≥n Pr√°ctica:**
Para aplicaciones donde la calidad de respuesta es prioritaria sobre la eficiencia de recuperaci√≥n, modelos open-source como MPNet o MiniLM pueden ofrecer resultados aceptables a menor costo, dado que el componente de generaci√≥n compensa parcialmente sus limitaciones en recuperaci√≥n.

