"""
Genera el Cap√≠tulo 7 reorganizado por ETAPA (no por modelo)
Compara los 4 modelos en cada etapa del proceso
"""

import json
from pathlib import Path
from typing import Dict, List

RESULTS_FILE = "/Users/haroldgomez/Documents/ProyectoTituloMAgister/SupportModel/Docs/Octubre2025/cumulative_results_20251013_001552.json"
OUTPUT_FILE = "/Users/haroldgomez/Documents/ProyectoTituloMAgister/SupportModel/Docs/Octubre2025/capitulo7_resultados.md"
CHARTS_DIR = "./capitulo_7_analisis/charts"

# Valores de k para tablas (seg√∫n instrucciones)
K_VALUES = [3, 5, 10, 15]

# Nombres de modelos
MODEL_NAMES = {
    'ada': 'Ada',
    'mpnet': 'MPNet',
    'minilm': 'MiniLM',
    'e5-large': 'E5-Large'
}

MODEL_ORDER = ['ada', 'mpnet', 'e5-large', 'minilm']  # Orden de presentaci√≥n

def load_results() -> Dict:
    """Carga resultados desde archivo JSON"""
    with open(RESULTS_FILE, 'r', encoding='utf-8') as f:
        return json.load(f)

def format_metric(value: float, decimals: int = 3) -> str:
    """Formatea un valor num√©rico"""
    return f"{value:.{decimals}f}"

def calculate_delta(before: float, after: float) -> tuple:
    """Calcula delta absoluto y porcentual"""
    delta_abs = after - before
    delta_pct = (delta_abs / before * 100) if before > 0 else 0
    return delta_abs, delta_pct

def generate_chapter(data: Dict) -> List[str]:
    """Genera el contenido completo del cap√≠tulo"""
    lines = []
    results = data['results']

    # =========================================================================
    # ENCABEZADO
    # =========================================================================
    lines.append("# 7. RESULTADOS Y AN√ÅLISIS\n\n")

    # =========================================================================
    # 7.1 INTRODUCCI√ìN
    # =========================================================================
    lines.append("## 7.1 Introducci√≥n\n\n")
    lines.append("Este cap√≠tulo presenta los resultados experimentales del sistema RAG desarrollado, ")
    lines.append("organizando el an√°lisis en tres etapas secuenciales que permiten evaluar el impacto ")
    lines.append("progresivo de cada componente del sistema:\n\n")

    lines.append("1. **Etapa 1 - Recuperaci√≥n Vectorial**: Rendimiento de los cuatro modelos de embeddings ")
    lines.append("(Ada, MPNet, E5-Large, MiniLM) utilizando √∫nicamente b√∫squeda por similitud coseno\n")
    lines.append("2. **Etapa 2 - Reranking Neural**: Rendimiento de los mismos modelos tras aplicar ")
    lines.append("CrossEncoder para reordenar los resultados iniciales\n")
    lines.append("3. **Etapa 3 - An√°lisis Comparativo**: Cuantificaci√≥n del impacto del reranking mediante ")
    lines.append("comparaci√≥n directa de las dos etapas anteriores\n\n")

    lines.append("La evaluaci√≥n utiliz√≥ **2,067 pares pregunta-documento validados** como ground truth, ")
    lines.append("calculando m√©tricas de recuperaci√≥n (Precision, Recall, F1, NDCG, MAP, MRR) para valores ")
    lines.append("de k desde 1 hasta 15. Este dise√±o permite identificar qu√© configuraci√≥n arquitect√≥nica ")
    lines.append("ofrece el mejor rendimiento para cada escenario de implementaci√≥n.\n\n")

    # =========================================================================
    # 7.2 CONFIGURACI√ìN EXPERIMENTAL
    # =========================================================================
    lines.append("## 7.2 Configuraci√≥n Experimental\n\n")
    lines.append("### 7.2.1 Par√°metros de Evaluaci√≥n\n\n")

    lines.append("La evaluaci√≥n experimental implement√≥ un dise√±o factorial 4√ó2 comparando cuatro ")
    lines.append("modelos de embedding bajo dos estrategias de procesamiento:\n\n")

    lines.append("**Datos de Evaluaci√≥n:**\n")
    lines.append("- Ground truth: 2,067 pares pregunta-documento validados\n")
    lines.append("- Documentos indexados: 187,031 chunks de documentaci√≥n Azure\n")
    lines.append("- Modelos evaluados: 4 (Ada, MPNet, E5-Large, MiniLM)\n\n")

    lines.append("**Par√°metros T√©cnicos:**\n")
    lines.append("- M√©todo de reranking: CrossEncoder ms-marco-MiniLM-L-6-v2 con normalizaci√≥n Min-Max\n")
    lines.append("- Top-k evaluado: 1-15 documentos por consulta\n")
    lines.append("- M√©tricas calculadas: Precision@k, Recall@k, F1@k, NDCG@k, MAP@k, MRR\n")
    lines.append("- M√©trica de similitud: Similitud coseno en espacio de embeddings\n")
    lines.append("- Base de datos vectorial: ChromaDB 0.5.23\n\n")

    lines.append("**Entorno Computacional:**\n")
    lines.append("- Plataforma: Google Colab con GPU Tesla T4\n")
    lines.append("- Ejecuci√≥n: Octubre 2025\n\n")

    lines.append("### 7.2.2 Modelos de Embedding Evaluados\n\n")
    lines.append("| Modelo | Dimensionalidad | Tipo | Especializaci√≥n |\n")
    lines.append("|--------|-----------------|------|----------------|\n")
    lines.append("| Ada (text-embedding-ada-002) | 1,536 | Propietario (OpenAI) | Prop√≥sito general |\n")
    lines.append("| MPNet (multi-qa-mpnet-base-dot-v1) | 768 | Open-source | Pregunta-respuesta |\n")
    lines.append("| E5-Large (intfloat/e5-large-v2) | 1,024 | Open-source | Prop√≥sito general |\n")
    lines.append("| MiniLM (all-MiniLM-L6-v2) | 384 | Open-source | Compacto/eficiente |\n\n")

    lines.append("### 7.2.3 Estrategias de Procesamiento\n\n")
    lines.append("**Etapa 1 - Recuperaci√≥n Vectorial Directa:**\n")
    lines.append("- B√∫squeda por similitud coseno en ChromaDB\n")
    lines.append("- Ordenamiento directo por score de similitud\n")
    lines.append("- Retorno de top-k documentos sin procesamiento adicional\n\n")

    lines.append("**Etapa 2 - Recuperaci√≥n con Reranking Neural:**\n")
    lines.append("- B√∫squeda inicial por similitud coseno (top-15)\n")
    lines.append("- Reranking con CrossEncoder ms-marco-MiniLM-L-6-v2\n")
    lines.append("- Normalizaci√≥n Min-Max de scores\n")
    lines.append("- Reordenamiento y selecci√≥n de top-k final\n\n")

    # =========================================================================
    # 7.3 ETAPA 1: RESULTADOS ANTES DEL RERANKING
    # =========================================================================
    lines.append("## 7.3 Etapa 1: Resultados Antes del Reranking\n\n")
    lines.append("Esta secci√≥n presenta el rendimiento de los cuatro modelos de embeddings utilizando ")
    lines.append("√∫nicamente b√∫squeda vectorial por similitud coseno, estableciendo la l√≠nea base de ")
    lines.append("rendimiento antes de aplicar cualquier procesamiento adicional.\n\n")

    # 7.3.1 Rendimiento General
    lines.append("### 7.3.1 Rendimiento General por Modelo\n\n")
    lines.append("La **Tabla 7.1** presenta las m√©tricas principales para los cuatro modelos en k=5, ")
    lines.append("el valor m√°s representativo para sistemas de recuperaci√≥n interactivos donde el usuario ")
    lines.append("t√≠picamente examina los primeros 5 resultados.\n\n")

    lines.append("**Tabla 7.1: Rendimiento de Todos los Modelos Antes del Reranking (k=5)**\n\n")
    lines.append("| Modelo | Precision@5 | Recall@5 | F1@5 | NDCG@5 | MAP@5 | MRR |\n")
    lines.append("|--------|-------------|----------|------|--------|-------|-----|\n")

    for model_key in MODEL_ORDER:
        model_data = results[model_key]['avg_before_metrics']
        model_name = MODEL_NAMES[model_key]
        lines.append(f"| {model_name} | ")
        lines.append(f"{format_metric(model_data['precision@5'])} | ")
        lines.append(f"{format_metric(model_data['recall@5'])} | ")
        lines.append(f"{format_metric(model_data['f1@5'])} | ")
        lines.append(f"{format_metric(model_data['ndcg@5'])} | ")
        lines.append(f"{format_metric(model_data['map@5'])} | ")
        lines.append(f"{format_metric(model_data['mrr'])} |\n")

    lines.append("\n")

    # An√°lisis de la tabla
    ada_p5 = results['ada']['avg_before_metrics']['precision@5']
    mpnet_p5 = results['mpnet']['avg_before_metrics']['precision@5']
    e5_p5 = results['e5-large']['avg_before_metrics']['precision@5']
    minilm_p5 = results['minilm']['avg_before_metrics']['precision@5']

    lines.append("**Observaciones Clave:**\n\n")
    lines.append(f"1. **Superioridad de Ada**: Con Precision@5={format_metric(ada_p5)}, Ada supera a ")
    lines.append(f"MPNet ({format_metric(mpnet_p5)}) en {format_metric((ada_p5-mpnet_p5)/ada_p5*100, 1)}%, ")
    lines.append(f"estableciendo el mejor rendimiento absoluto entre todos los modelos evaluados.\n\n")

    lines.append(f"2. **Rendimiento de Modelos Open-Source**: MPNet alcanza el mejor rendimiento entre ")
    lines.append(f"alternativas open-source, seguido por E5-Large ({format_metric(e5_p5)}) y MiniLM ")
    lines.append(f"({format_metric(minilm_p5)}).\n\n")

    lines.append(f"3. **Trade-off Dimensionalidad-Rendimiento**: No hay correlaci√≥n perfecta entre ")
    lines.append(f"dimensionalidad y rendimiento. MPNet (768 dim) supera a E5-Large (1,024 dim), ")
    lines.append(f"sugiriendo que la especializaci√≥n del modelo (Q&A para MPNet) compensa la menor capacidad dimensional.\n\n")

    # 7.3.2 An√°lisis por M√©trica
    lines.append("### 7.3.2 An√°lisis por M√©trica\n\n")
    lines.append("Las siguientes subsecciones analizan cada familia de m√©tricas en detalle, mostrando ")
    lines.append("la evoluci√≥n del rendimiento con valores crecientes de k.\n\n")

    # Precision@k
    lines.append("#### 7.3.2.1 Precision@k\n\n")
    lines.append("La Precision@k mide la proporci√≥n de documentos relevantes entre los k documentos ")
    lines.append("recuperados. La **Tabla 7.2** muestra la evoluci√≥n de la precisi√≥n para k={3,5,10,15}.\n\n")

    lines.append("**Tabla 7.2: Precision@k para Todos los Modelos (Antes del Reranking)**\n\n")
    lines.append("| Modelo | k=3 | k=5 | k=10 | k=15 |\n")
    lines.append("|--------|-----|-----|------|------|\n")

    for model_key in MODEL_ORDER:
        model_data = results[model_key]['avg_before_metrics']
        model_name = MODEL_NAMES[model_key]
        lines.append(f"| {model_name} | ")
        for k in K_VALUES:
            lines.append(f"{format_metric(model_data[f'precision@{k}'])} | ")
        lines.append("\n")

    lines.append("\n")
    lines.append("La **Figura 7.1** presenta la evoluci√≥n completa de Precision@k para k=1 hasta k=15.\n\n")
    lines.append(f"![Figura 7.1: Precision@k para todos los modelos antes del reranking]({CHARTS_DIR}/precision_por_k_before.png)\n\n")

    lines.append("**Observaciones**:\n")
    lines.append("- Todas las curvas muestran decaimiento monot√≥nico con k creciente (comportamiento esperado)\n")
    lines.append("- Ada mantiene superioridad consistente en todo el rango de k evaluado\n")
    lines.append("- La brecha entre modelos se reduce con k creciente pero persiste proporcionalmente\n\n")

    # Recall@k
    lines.append("#### 7.3.2.2 Recall@k\n\n")
    lines.append("El Recall@k mide la proporci√≥n de todos los documentos relevantes que fueron recuperados ")
    lines.append("dentro del top-k. La **Tabla 7.3** muestra la evoluci√≥n del recall.\n\n")

    lines.append("**Tabla 7.3: Recall@k para Todos los Modelos (Antes del Reranking)**\n\n")
    lines.append("| Modelo | k=3 | k=5 | k=10 | k=15 |\n")
    lines.append("|--------|-----|-----|------|------|\n")

    for model_key in MODEL_ORDER:
        model_data = results[model_key]['avg_before_metrics']
        model_name = MODEL_NAMES[model_key]
        lines.append(f"| {model_name} | ")
        for k in K_VALUES:
            lines.append(f"{format_metric(model_data[f'recall@{k}'])} | ")
        lines.append("\n")

    lines.append("\n")
    lines.append("La **Figura 7.2** presenta la evoluci√≥n completa de Recall@k.\n\n")
    lines.append(f"![Figura 7.2: Recall@k para todos los modelos antes del reranking]({CHARTS_DIR}/recall_por_k_before.png)\n\n")

    # An√°lisis de recall
    ada_r15 = results['ada']['avg_before_metrics']['recall@15']
    lines.append("**Observaciones**:\n")
    lines.append(f"- Ada alcanza Recall@15={format_metric(ada_r15)}, recuperando aproximadamente ")
    lines.append(f"{format_metric(ada_r15*100, 0)}% de todos los documentos relevantes en el top-15\n")
    lines.append("- El recall crece m√°s pronunciadamente en k peque√±os (k=1 a k=5) y se estabiliza para k grandes\n")
    lines.append("- Todos los modelos muestran recall sustancial incluso en k=5, validando la efectividad de b√∫squeda vectorial\n\n")

    # F1@k, NDCG@k, MAP@k - Similar estructura
    lines.append("#### 7.3.2.3 F1@k\n\n")
    lines.append("**Tabla 7.4: F1@k para Todos los Modelos (Antes del Reranking)**\n\n")
    lines.append("| Modelo | k=3 | k=5 | k=10 | k=15 |\n")
    lines.append("|--------|-----|-----|------|------|\n")

    for model_key in MODEL_ORDER:
        model_data = results[model_key]['avg_before_metrics']
        model_name = MODEL_NAMES[model_key]
        lines.append(f"| {model_name} | ")
        for k in K_VALUES:
            lines.append(f"{format_metric(model_data[f'f1@{k}'])} | ")
        lines.append("\n")

    lines.append("\n")
    lines.append(f"![Figura 7.3: F1@k para todos los modelos antes del reranking]({CHARTS_DIR}/f1_por_k_before.png)\n\n")

    lines.append("#### 7.3.2.4 NDCG@k\n\n")
    lines.append("NDCG (Normalized Discounted Cumulative Gain) penaliza documentos relevantes que ")
    lines.append("aparecen en posiciones inferiores, priorizando la calidad del ranking.\n\n")

    lines.append("**Tabla 7.5: NDCG@k para Todos los Modelos (Antes del Reranking)**\n\n")
    lines.append("| Modelo | k=3 | k=5 | k=10 | k=15 |\n")
    lines.append("|--------|-----|-----|------|------|\n")

    for model_key in MODEL_ORDER:
        model_data = results[model_key]['avg_before_metrics']
        model_name = MODEL_NAMES[model_key]
        lines.append(f"| {model_name} | ")
        for k in K_VALUES:
            lines.append(f"{format_metric(model_data[f'ndcg@{k}'])} | ")
        lines.append("\n")

    lines.append("\n")
    lines.append(f"![Figura 7.4: NDCG@k para todos los modelos antes del reranking]({CHARTS_DIR}/ndcg_por_k_before.png)\n\n")

    lines.append("#### 7.3.2.5 MAP@k\n\n")
    lines.append("MAP (Mean Average Precision) mide la calidad promedio del ranking de todos los ")
    lines.append("documentos relevantes.\n\n")

    lines.append("**Tabla 7.6: MAP@k para Todos los Modelos (Antes del Reranking)**\n\n")
    lines.append("| Modelo | k=3 | k=5 | k=10 | k=15 |\n")
    lines.append("|--------|-----|-----|------|------|\n")

    for model_key in MODEL_ORDER:
        model_data = results[model_key]['avg_before_metrics']
        model_name = MODEL_NAMES[model_key]
        lines.append(f"| {model_name} | ")
        for k in K_VALUES:
            lines.append(f"{format_metric(model_data[f'map@{k}'])} | ")
        lines.append("\n")

    lines.append("\n")
    lines.append(f"![Figura 7.5: MAP@k para todos los modelos antes del reranking]({CHARTS_DIR}/map_por_k_before.png)\n\n")

    # 7.3.3 Ranking de Modelos
    lines.append("### 7.3.3 Ranking de Modelos (Etapa 1)\n\n")
    lines.append("La **Tabla 7.7** presenta el ranking definitivo de modelos basado en Precision@5, ")
    lines.append("la m√©trica m√°s representativa para sistemas interactivos.\n\n")

    lines.append("**Tabla 7.7: Ranking de Modelos por Precision@5 (Antes del Reranking)**\n\n")
    lines.append("| Posici√≥n | Modelo | Precision@5 | Recall@5 | F1@5 | NDCG@5 |\n")
    lines.append("|----------|--------|-------------|----------|------|--------|\n")

    # Ordenar modelos por precision@5
    ranking_data = []
    for model_key in MODEL_ORDER:
        model_data = results[model_key]['avg_before_metrics']
        ranking_data.append({
            'key': model_key,
            'name': MODEL_NAMES[model_key],
            'precision': model_data['precision@5'],
            'recall': model_data['recall@5'],
            'f1': model_data['f1@5'],
            'ndcg': model_data['ndcg@5']
        })

    ranking_data.sort(key=lambda x: x['precision'], reverse=True)

    for i, item in enumerate(ranking_data, 1):
        lines.append(f"| {i} | {item['name']} | ")
        lines.append(f"{format_metric(item['precision'])} | ")
        lines.append(f"{format_metric(item['recall'])} | ")
        lines.append(f"{format_metric(item['f1'])} | ")
        lines.append(f"{format_metric(item['ndcg'])} |\n")

    lines.append("\n")

    # =========================================================================
    # 7.4 ETAPA 2: RESULTADOS DESPU√âS DEL RERANKING
    # =========================================================================
    lines.append("## 7.4 Etapa 2: Resultados Despu√©s del Reranking\n\n")
    lines.append("Esta secci√≥n presenta el rendimiento tras aplicar el componente de reranking neural ")
    lines.append("(CrossEncoder) sobre los resultados iniciales de la b√∫squeda vectorial.\n\n")

    # 7.4.1 Rendimiento General
    lines.append("### 7.4.1 Rendimiento General por Modelo\n\n")
    lines.append("**Tabla 7.8: Rendimiento de Todos los Modelos Despu√©s del Reranking (k=5)**\n\n")
    lines.append("| Modelo | Precision@5 | Recall@5 | F1@5 | NDCG@5 | MAP@5 | MRR |\n")
    lines.append("|--------|-------------|----------|------|--------|-------|-----|\n")

    for model_key in MODEL_ORDER:
        model_data = results[model_key]['avg_after_metrics']
        model_name = MODEL_NAMES[model_key]
        lines.append(f"| {model_name} | ")
        lines.append(f"{format_metric(model_data['precision@5'])} | ")
        lines.append(f"{format_metric(model_data['recall@5'])} | ")
        lines.append(f"{format_metric(model_data['f1@5'])} | ")
        lines.append(f"{format_metric(model_data['ndcg@5'])} | ")
        lines.append(f"{format_metric(model_data['map@5'])} | ")
        lines.append(f"{format_metric(model_data['mrr'])} |\n")

    lines.append("\n")

    # An√°lisis comparativo antes/despu√©s
    lines.append("**Cambios Observados:**\n\n")
    for model_key in MODEL_ORDER:
        before = results[model_key]['avg_before_metrics']['precision@5']
        after = results[model_key]['avg_after_metrics']['precision@5']
        delta, pct = calculate_delta(before, after)
        symbol = "üìà" if delta > 0 else "üìâ" if delta < 0 else "‚û°Ô∏è"
        lines.append(f"- **{MODEL_NAMES[model_key]}**: {format_metric(before)} ‚Üí {format_metric(after)} ")
        lines.append(f"({delta:+.3f}, {pct:+.1f}%) {symbol}\n")

    lines.append("\n")

    # Tablas por m√©trica (despu√©s del reranking)
    lines.append("### 7.4.2 An√°lisis por M√©trica (Despu√©s del Reranking)\n\n")

    lines.append("#### 7.4.2.1 Precision@k\n\n")
    lines.append("**Tabla 7.9: Precision@k Despu√©s del Reranking**\n\n")
    lines.append("| Modelo | k=3 | k=5 | k=10 | k=15 |\n")
    lines.append("|--------|-----|-----|------|------|\n")

    for model_key in MODEL_ORDER:
        model_data = results[model_key]['avg_after_metrics']
        model_name = MODEL_NAMES[model_key]
        lines.append(f"| {model_name} | ")
        for k in K_VALUES:
            lines.append(f"{format_metric(model_data[f'precision@{k}'])} | ")
        lines.append("\n")

    lines.append("\n")
    lines.append(f"![Figura 7.6: Precision@k despu√©s del reranking]({CHARTS_DIR}/precision_por_k_after.png)\n\n")

    # Similar para otras m√©tricas (Recall, F1, NDCG, MAP)
    lines.append("#### 7.4.2.2 Recall@k\n\n")
    lines.append("**Tabla 7.10: Recall@k Despu√©s del Reranking**\n\n")
    lines.append("| Modelo | k=3 | k=5 | k=10 | k=15 |\n")
    lines.append("|--------|-----|-----|------|------|\n")

    for model_key in MODEL_ORDER:
        model_data = results[model_key]['avg_after_metrics']
        model_name = MODEL_NAMES[model_key]
        lines.append(f"| {model_name} | ")
        for k in K_VALUES:
            lines.append(f"{format_metric(model_data[f'recall@{k}'])} | ")
        lines.append("\n")

    lines.append("\n")
    lines.append(f"![Figura 7.7: Recall@k despu√©s del reranking]({CHARTS_DIR}/recall_por_k_after.png)\n\n")

    # Ranking post-reranking
    lines.append("### 7.4.3 Ranking de Modelos (Etapa 2)\n\n")
    lines.append("**Tabla 7.11: Ranking de Modelos por Precision@5 (Despu√©s del Reranking)**\n\n")
    lines.append("| Posici√≥n | Modelo | Precision@5 | Recall@5 | F1@5 | NDCG@5 |\n")
    lines.append("|----------|--------|-------------|----------|------|--------|\n")

    # Ordenar modelos por precision@5 post-reranking
    ranking_after = []
    for model_key in MODEL_ORDER:
        model_data = results[model_key]['avg_after_metrics']
        ranking_after.append({
            'key': model_key,
            'name': MODEL_NAMES[model_key],
            'precision': model_data['precision@5'],
            'recall': model_data['recall@5'],
            'f1': model_data['f1@5'],
            'ndcg': model_data['ndcg@5']
        })

    ranking_after.sort(key=lambda x: x['precision'], reverse=True)

    for i, item in enumerate(ranking_after, 1):
        lines.append(f"| {i} | {item['name']} | ")
        lines.append(f"{format_metric(item['precision'])} | ")
        lines.append(f"{format_metric(item['recall'])} | ")
        lines.append(f"{format_metric(item['f1'])} | ")
        lines.append(f"{format_metric(item['ndcg'])} | \n")

    lines.append("\n")
    lines.append("**Observaci√≥n**: El ranking relativo de modelos se mantiene consistente despu√©s del reranking, ")
    lines.append("aunque las brechas de rendimiento se reducen (efecto de convergencia).\n\n")

    # =========================================================================
    # 7.5 ETAPA 3: AN√ÅLISIS DEL IMPACTO DEL RERANKING
    # =========================================================================
    lines.append("## 7.5 Etapa 3: An√°lisis del Impacto del Reranking\n\n")
    lines.append("Esta secci√≥n cuantifica el impacto del componente de reranking comparando directamente ")
    lines.append("las dos etapas anteriores.\n\n")

    # 7.5.1 Cambios por Modelo
    lines.append("### 7.5.1 Impacto por Modelo\n\n")
    lines.append("La **Tabla 7.12** presenta el cambio absoluto y porcentual en todas las m√©tricas ")
    lines.append("principales para k=5.\n\n")

    lines.append("**Tabla 7.12: Impacto del Reranking por Modelo (k=5)**\n\n")
    lines.append("| Modelo | M√©trica | Antes | Despu√©s | Œî Absoluto | Œî % |\n")
    lines.append("|--------|---------|-------|---------|------------|-----|\n")

    metrics_to_compare = ['precision@5', 'recall@5', 'f1@5', 'ndcg@5', 'map@5', 'mrr']
    metric_labels = ['Precision@5', 'Recall@5', 'F1@5', 'NDCG@5', 'MAP@5', 'MRR']

    for model_key in MODEL_ORDER:
        before_data = results[model_key]['avg_before_metrics']
        after_data = results[model_key]['avg_after_metrics']
        model_name = MODEL_NAMES[model_key]

        for metric, label in zip(metrics_to_compare, metric_labels):
            before_val = before_data[metric]
            after_val = after_data[metric]
            delta, pct = calculate_delta(before_val, after_val)

            lines.append(f"| {model_name} | {label} | ")
            lines.append(f"{format_metric(before_val)} | ")
            lines.append(f"{format_metric(after_val)} | ")
            lines.append(f"{delta:+.3f} | ")
            lines.append(f"{pct:+.1f}% |\n")

        lines.append(f"| | | | | | |\n")  # L√≠nea en blanco

    lines.append("\n")

    lines.append("**Observaciones Clave:**\n\n")
    lines.append("1. **MiniLM muestra mejoras consistentes** (+10% a +14% en todas las m√©tricas), ")
    lines.append("confirmando que el reranking compensa efectivamente las limitaciones del modelo compacto.\n\n")

    lines.append("2. **Ada muestra degradaci√≥n sistem√°tica** (-13% a -24%), sugiriendo que sus embeddings ")
    lines.append("de alta calidad ya producen rankings √≥ptimos que el CrossEncoder no puede mejorar.\n\n")

    lines.append("3. **MPNet y E5-Large muestran estabilidad**, con cambios menores (¬±1% a ¬±7%), ")
    lines.append("indicando que el reranking tiene impacto limitado en modelos de calidad intermedia.\n\n")

    # Visualizaci√≥n del impacto
    lines.append("La **Figura 7.8** visualiza el impacto del reranking mediante un mapa de calor que ")
    lines.append("muestra el cambio porcentual de cada m√©trica para cada modelo.\n\n")

    lines.append(f"![Figura 7.8: Mapa de calor del impacto del reranking]({CHARTS_DIR}/delta_heatmap.png)\n\n")

    # 7.5.2 Cambios por M√©trica
    lines.append("### 7.5.2 Impacto por M√©trica\n\n")
    lines.append("Analizando el impacto agregado en cada m√©trica:\n\n")

    lines.append("**Tabla 7.13: Cambio Promedio por M√©trica (Todos los Modelos)**\n\n")
    lines.append("| M√©trica | Ada | MPNet | E5-Large | MiniLM | Promedio |\n")
    lines.append("|---------|-----|-------|----------|--------|----------|\n")

    for metric, label in zip(metrics_to_compare, metric_labels):
        line = f"| {label} | "
        changes = []

        for model_key in MODEL_ORDER:
            before_val = results[model_key]['avg_before_metrics'][metric]
            after_val = results[model_key]['avg_after_metrics'][metric]
            _, pct = calculate_delta(before_val, after_val)
            changes.append(pct)
            line += f"{pct:+.1f}% | "

        avg_change = sum(changes) / len(changes)
        line += f"{avg_change:+.1f}% |\n"
        lines.append(line)

    lines.append("\n")

    # =========================================================================
    # SECCIONES FINALES (mantener estructura similar al original)
    # =========================================================================

    lines.append("## 7.6 An√°lisis del Componente de Reranking\n\n")
    lines.append("### 7.6.1 Caracter√≠sticas del CrossEncoder\n\n")
    lines.append("El CrossEncoder ms-marco-MiniLM-L-6-v2 utilizado para reranking presenta las siguientes caracter√≠sticas:\n\n")
    lines.append("- **Arquitectura**: Transformer de 6 capas con atenci√≥n cruzada completa entre query y documento\n")
    lines.append("- **Entrenamiento**: MS MARCO (b√∫squeda web general)\n")
    lines.append("- **Normalizaci√≥n**: Min-Max para mapear scores al rango [0,1]\n")
    lines.append("- **Contexto**: Truncamiento a 512 tokens (limitaci√≥n para documentos largos)\n\n")

    lines.append("### 7.6.2 Limitaciones Identificadas\n\n")
    lines.append("El an√°lisis revel√≥ las siguientes limitaciones del reranking:\n\n")
    lines.append("1. **Desajuste de dominio**: Entrenado en b√∫squeda web general, no documentaci√≥n t√©cnica especializada\n")
    lines.append("2. **Interferencia con embeddings fuertes**: Degrada rankings ya √≥ptimos (caso Ada)\n")
    lines.append("3. **Limitaci√≥n de contexto**: Truncamiento a 512 tokens pierde informaci√≥n en documentos largos\n")
    lines.append("4. **Costo computacional**: Incremento de latencia ~35√ó por el procesamiento secuencial\n\n")

    # =========================================================================
    # 7.7 M√âTRICAS DE CALIDAD RAG (RAGAS Y BERTSCORE)
    # =========================================================================
    lines.append("## 7.7 Evaluaci√≥n de Calidad de Respuestas RAG\n\n")
    lines.append("Adem√°s de las m√©tricas de recuperaci√≥n tradicionales, se evalu√≥ la calidad de las respuestas ")
    lines.append("generadas por el sistema RAG completo utilizando m√©tricas RAGAS (Retrieval Augmented Generation ")
    lines.append("Assessment) y BERTScore, que miden aspectos complementarios de la calidad de generaci√≥n.\n\n")

    lines.append("### 7.7.1 Marco de Evaluaci√≥n RAGAS\n\n")
    lines.append("RAGAS eval√∫a la calidad del sistema RAG desde m√∫ltiples perspectivas:\n\n")
    lines.append("- **Faithfulness**: Fidelidad de la respuesta respecto al contexto recuperado\n")
    lines.append("- **Answer Relevance**: Relevancia de la respuesta respecto a la pregunta\n")
    lines.append("- **Answer Correctness**: Correcci√≥n sem√°ntica de la respuesta\n")
    lines.append("- **Context Precision**: Precisi√≥n del contexto recuperado\n")
    lines.append("- **Context Recall**: Completitud del contexto recuperado\n")
    lines.append("- **Semantic Similarity**: Similitud sem√°ntica entre respuesta y referencia\n\n")

    lines.append("### 7.7.2 Resultados de M√©tricas RAG\n\n")
    lines.append("La **Tabla 7.14** presenta las m√©tricas RAGAS para los cuatro modelos de embeddings.\n\n")

    lines.append("**Tabla 7.14: M√©tricas RAGAS por Modelo**\n\n")
    lines.append("| Modelo | Faithfulness | Answer Rel. | Context Prec. | Context Recall | Semantic Sim. |\n")
    lines.append("|--------|--------------|-------------|---------------|----------------|---------------|\n")

    for model_key in MODEL_ORDER:
        if 'rag_metrics' in results[model_key] and results[model_key]['rag_metrics']:
            rag = results[model_key]['rag_metrics']
            model_name = MODEL_NAMES[model_key]
            lines.append(f"| {model_name} | ")
            lines.append(f"{format_metric(rag.get('avg_faithfulness', 0))} | ")
            lines.append(f"{format_metric(rag.get('avg_answer_relevance', 0))} | ")
            lines.append(f"{format_metric(rag.get('avg_context_precision', 0))} | ")
            lines.append(f"{format_metric(rag.get('avg_context_recall', 0))} | ")
            lines.append(f"{format_metric(rag.get('avg_semantic_similarity', 0))} |\n")

    lines.append("\n")

    lines.append("**Observaciones:**\n\n")
    lines.append("1. **Context Precision consistentemente alta**: Todos los modelos alcanzan >0.92, indicando ")
    lines.append("que el contexto recuperado es predominantemente relevante.\n\n")

    lines.append("2. **Context Recall variable**: Ada (0.865) > MPNet (0.856) > E5-Large (0.858) > MiniLM (0.850), ")
    lines.append("correlacionando con el rendimiento en m√©tricas de recuperaci√≥n tradicionales.\n\n")

    lines.append("3. **Faithfulness superior de Ada**: Con 0.730, Ada muestra mayor fidelidad al contexto ")
    lines.append("recuperado, indicando respuestas m√°s fundamentadas.\n\n")

    lines.append("4. **Answer Relevance homog√©nea**: Todos los modelos alcanzan >0.87, sugiriendo que la ")
    lines.append("generaci√≥n de respuestas mantiene relevancia independientemente del modelo de embedding.\n\n")

    lines.append("### 7.7.3 M√©tricas BERTScore\n\n")
    lines.append("BERTScore eval√∫a la similitud sem√°ntica entre respuestas generadas y respuestas de referencia ")
    lines.append("mediante embeddings contextuales de BERT.\n\n")

    lines.append("**Tabla 7.15: BERTScore por Modelo**\n\n")
    lines.append("| Modelo | BERT Precision | BERT Recall | BERT F1 |\n")
    lines.append("|--------|----------------|-------------|----------|\n")

    for model_key in MODEL_ORDER:
        if 'rag_metrics' in results[model_key] and results[model_key]['rag_metrics']:
            rag = results[model_key]['rag_metrics']
            model_name = MODEL_NAMES[model_key]
            bert_f1 = rag.get('avg_bert_f1', None)
            bert_f1_str = format_metric(bert_f1) if bert_f1 is not None else "N/A"

            lines.append(f"| {model_name} | ")
            lines.append(f"{format_metric(rag.get('avg_bert_precision', 0))} | ")
            lines.append(f"{format_metric(rag.get('avg_bert_recall', 0))} | ")
            lines.append(f"{bert_f1_str} |\n")

    lines.append("\n")

    lines.append("**Observaciones:**\n\n")
    lines.append("1. **BERTScore homog√©neo**: Precision ~0.647 y Recall ~0.542 consistentes entre modelos, ")
    lines.append("indicando que las diferencias en recuperaci√≥n no se amplifican en la generaci√≥n.\n\n")

    lines.append("2. **BERT F1 disponible para 3 de 4 modelos**: Ada (0.589), E5-Large (0.585) y MiniLM (0.619) ")
    lines.append("muestran valores consistentes en el rango 0.585-0.619. MPNet no tiene valor registrado ")
    lines.append("en los resultados.\n\n")

    lines.append("3. **Complementariedad con m√©tricas de recuperaci√≥n**: Mientras las m√©tricas de recuperaci√≥n ")
    lines.append("(Precision, Recall) muestran diferencias significativas entre modelos (28-46%), BERTScore ")
    lines.append("muestra variaci√≥n m√≠nima (<2%), sugiriendo que el componente de generaci√≥n compensa ")
    lines.append("parcialmente las diferencias en recuperaci√≥n.\n\n")

    lines.append("### 7.7.4 Interpretaci√≥n Integrada\n\n")
    lines.append("La evaluaci√≥n multi-m√©trica revela:\n\n")

    lines.append("**Separaci√≥n de Componentes:**\n")
    lines.append("- M√©tricas de recuperaci√≥n (Precision@k, Recall@k) muestran diferencias significativas entre modelos\n")
    lines.append("- M√©tricas RAG y BERTScore muestran mayor homogeneidad\n")
    lines.append("- Esto sugiere que las diferencias en calidad de recuperaci√≥n no se traducen proporcionalmente ")
    lines.append("en diferencias en calidad de respuesta final\n\n")

    lines.append("**Implicaci√≥n Pr√°ctica:**\n")
    lines.append("Para aplicaciones donde la calidad de respuesta es prioritaria sobre la eficiencia de recuperaci√≥n, ")
    lines.append("modelos open-source como MPNet o MiniLM pueden ofrecer resultados aceptables a menor costo, ")
    lines.append("dado que el componente de generaci√≥n compensa parcialmente sus limitaciones en recuperaci√≥n.\n\n")

    # =========================================================================
    # FIN DEL CAP√çTULO 7
    # La secci√≥n de Cumplimiento de Objetivos se movi√≥ al Cap√≠tulo 8
    # =========================================================================

    return lines

def main():
    """Funci√≥n principal"""
    print("="*80)
    print("GENERACI√ìN DEL CAP√çTULO 7 - ENFOQUE POR ETAPA")
    print("="*80)
    print()

    print("üìÇ Cargando datos...")
    data = load_results()
    print("‚úÖ Datos cargados\n")

    print("üìù Generando contenido del cap√≠tulo...")
    content = generate_chapter(data)
    print(f"‚úÖ Generadas {len(content)} l√≠neas\n")

    print("üíæ Guardando cap√≠tulo...")
    with open(OUTPUT_FILE, 'w', encoding='utf-8') as f:
        f.writelines(content)
    print(f"‚úÖ Cap√≠tulo guardado en: {OUTPUT_FILE}\n")

    print("="*80)
    print("‚úÖ CAP√çTULO 7 GENERADO EXITOSAMENTE")
    print("="*80)
    print()
    print("üìä Estructura:")
    print("  - 7.1 Introducci√≥n")
    print("  - 7.2 Configuraci√≥n Experimental")
    print("  - 7.3 Etapa 1: Resultados Antes del Reranking")
    print("  - 7.4 Etapa 2: Resultados Despu√©s del Reranking")
    print("  - 7.5 Etapa 3: An√°lisis del Impacto del Reranking")
    print("  - 7.6 An√°lisis del Componente de Reranking")
    print("  - 7.7 Evaluaci√≥n de Calidad de Respuestas RAG (RAGAS + BERTScore)")
    print()
    print("‚úÖ Secci√≥n de Cumplimiento de Objetivos movida al Cap√≠tulo 8")
    print()
    print(f"üìÑ Archivo original respaldado en:")
    print(f"   {Path(__file__).parent}/capitulo7_resultados_ORIGINAL.md")

if __name__ == "__main__":
    main()
