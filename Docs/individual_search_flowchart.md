# ğŸ” Diagrama de Flujo: BÃºsqueda Individual Azure Q&A System

## Flujo Completo del Sistema de BÃºsqueda Individual

```mermaid
flowchart TD
    A[ğŸš€ Inicio: Usuario en BÃºsqueda Individual] --> B[âš™ï¸ ConfiguraciÃ³n de ParÃ¡metros]
    
    B --> B1{ğŸ“ TÃ­tulo ingresado?}
    B1 -->|SÃ­| B2[ğŸ’¾ Guardar tÃ­tulo en session_state]
    B1 -->|No| B3[ğŸ“‹ TÃ­tulo vacÃ­o]
    B2 --> B4[â“ Ãrea de pregunta]
    B3 --> B4
    
    B4 --> B5{â“ Pregunta ingresada?}
    B5 -->|No| B6[âš ï¸ Mostrar advertencia]
    B5 -->|SÃ­| C[ğŸ” Click Buscar Documentacion]
    B6 --> B4
    
    C --> D[â±ï¸ Iniciar timer de respuesta]
    D --> E[ğŸ”— Combinar tÃ­tulo + pregunta = full_query]
    E --> F[ğŸ”§ initialize_clients]
    
    F --> F1[ğŸŒ Conectar a Weaviate]
    F1 --> F2[ğŸ¤– Inicializar embedding_client local]
    F2 --> F3[ğŸ”‘ Inicializar OpenAI client]
    F3 --> F4[ğŸ’ Inicializar Gemini client]
    F4 --> F5[ğŸ¦™ Inicializar Local Llama client]
    F5 --> F6[ğŸ¤– Inicializar Local Mistral client]
    F6 --> G{ğŸ¤– RAG habilitado?}
    
    %% Flujo RAG Completo
    G -->|SÃ­| H[ğŸ“ Llamar answer_question_with_rag]
    G -->|No| I[ğŸ“ Llamar answer_question_documents_only]
    
    %% Pipeline RAG Completo
    H --> H1[ğŸ” refine_and_prepare_query]
    H1 --> H1a{ğŸ¤– Local refinement disponible?}
    H1a -->|SÃ­| H1b[ğŸ¦™ Usar Mistral local para refinar query]
    H1a -->|No| H1c{ğŸ’ Gemini disponible?}
    H1c -->|SÃ­| H1d[ğŸ’ Usar Gemini para refinar query]
    H1c -->|No| H1e[ğŸ“ Usar query original]
    H1b --> H2[ğŸ”§ Aplicar prefijo condicional]
    H1d --> H2
    H1e --> H2
    
    H2 --> H3[ğŸ§® Generar embedding de query]
    H3 --> H4{ğŸ“š use_questions_collection?}
    H4 -->|SÃ­| H5[ğŸŒ BÃºsqueda en QuestionsMiniLM]
    H4 -->|No| H6[ğŸ“„ Saltar bÃºsqueda de preguntas]
    
    H5 --> H5a[ğŸ” search_questions_by_vector]
    H5a --> H5b[ğŸŒ WEAVIATE: Buscar preguntas similares]
    H5b --> H5c[ğŸ“Š Obtener top 15 preguntas]
    H5c --> H5d[ğŸ”— Extraer enlaces Ãºnicos]
    H5d --> H5e[ğŸŒ WEAVIATE: Buscar docs por enlaces]
    H5e --> H6
    
    H6 --> H7[ğŸŒ WEAVIATE: BÃºsqueda vectorial en DocumentsMpnet]
    H7 --> H7a[ğŸ” search_docs_by_vector]
    H7a --> H7b[ğŸ“Š Obtener documentos candidatos]
    H7b --> H8[ğŸ”„ Aplicar filtro de diversidad]
    H8 --> H9{ğŸ§  use_llm_reranker?}
    
    H9 -->|SÃ­| H10[ğŸ”§ rerank_with_llm]
    H9 -->|No| H11[ğŸ“Š rerank_documents estÃ¡ndar]
    
    H10 --> H10a[ğŸ¤– Usar CrossEncoder local ms-marco-MiniLM]
    H10a --> H10b[ğŸ“ˆ Calcular scores normalizados]
    H10b --> H12[ğŸ“‹ Documentos reordenados]
    
    H11 --> H11a[ğŸ§® Generar embeddings de documentos]
    H11a --> H11b[ğŸ“Š Calcular similitud coseno]
    H11b --> H12
    
    H12 --> H13{ğŸ¤– Generar respuesta?}
    H13 -->|SÃ­| H14{ğŸ¦™ Modelo local seleccionado?}
    H13 -->|No| H20[ğŸ“‹ Solo devolver documentos]
    
    H14 -->|llama-3.1-8b| H15[ğŸ¦™ generate_final_answer_local]
    H14 -->|mistral-7b| H16[ğŸ¤– generate_final_answer_local]
    H14 -->|gemini-pro| H17[ğŸ’ generate_final_answer_gemini]
    H14 -->|gpt-4| H18[ğŸ”‘ generate_final_answer OpenAI]
    H14 -->|Ninguno| H19[âŒ Error: No hay modelo]
    
    H15 --> H15a[ğŸ¦™ Cargar modelo Llama local]
    H15a --> H15b[ğŸ¤– Generar respuesta con contexto]
    H15b --> H21[ğŸ“ Respuesta generada]
    
    H16 --> H16a[ğŸ¤– Cargar modelo Mistral local]
    H16a --> H16b[ğŸ¤– Generar respuesta con contexto]
    H16b --> H21
    
    H17 --> H17a[ğŸ’ API GEMINI: Generar respuesta]
    H17a --> H21
    
    H18 --> H18a[ğŸ”‘ API OPENAI: Generar respuesta]
    H18a --> H21
    
    H19 --> H21
    H20 --> H25
    H21 --> H22{ğŸ“Š evaluate_quality?}
    
    H22 -->|SÃ­| H23[ğŸ”‘ API OPENAI: Evaluar calidad]
    H22 -->|No| H24[ğŸ“Š MÃ©tricas bÃ¡sicas]
    H23 --> H25[ğŸ“‹ Compilar resultados RAG]
    H24 --> H25
    
    %% Pipeline Solo Documentos
    I --> I1[ğŸ” Ejecutar pasos H1-H12]
    I1 --> I2[ğŸ“‹ Devolver solo documentos]
    I2 --> J[ğŸ“Š Calcular mÃ©tricas de sesiÃ³n]
    
    H25 --> J
    
    J --> J1[â±ï¸ Actualizar response_time]
    J1 --> J2[ğŸ“ˆ Actualizar queries_made++]
    J2 --> J3[ğŸ“š Actualizar total_docs_retrieved]
    J3 --> J4[âš¡ Actualizar avg_response_time]
    J4 --> K{ğŸ“‹ Resultados encontrados?}
    
    K -->|No| L[âŒ Mostrar mensaje: No hay resultados]
    K -->|SÃ­| M[âœ… Mostrar mensaje de Ã©xito]
    
    M --> N{ğŸ¤– RAG habilitado Y respuesta generada?}
    N -->|SÃ­| O[ğŸ¤– Mostrar Respuesta RAG]
    N -->|No| P[ğŸ“š Mostrar solo documentos]
    
    O --> O1[ğŸ¯ Mostrar mÃ©tricas RAG]
    O1 --> O1a[ğŸ“Š Confianza]
    O1a --> O1b[ğŸ“‹ Completitud]
    O1b --> O1c[ğŸ“š Docs Usados]
    O1c --> O1d{ğŸ“Š evaluate_rag_quality?}
    O1d -->|SÃ­| O1e[ğŸ” Mostrar Fidelidad]
    O1d -->|No| O1f[âš¡ Mostrar Estado]
    O1e --> O2[ğŸ’¬ Mostrar respuesta generada]
    O1f --> O2
    O2 --> P
    
    P --> P1[ğŸ“„ Iterar documentos encontrados]
    P1 --> P2[ğŸ“Š Calcular nivel de confianza por score]
    P2 --> P3[ğŸ¨ Aplicar CSS styling segÃºn confianza]
    P3 --> P4[ğŸ“‹ Mostrar tÃ­tulo, score, enlace]
    P4 --> P5{ğŸ”— MÃ¡s documentos?}
    P5 -->|SÃ­| P1
    P5 -->|No| Q[ğŸ“Š Mostrar informaciÃ³n de debug]
    
    Q --> Q1{ğŸ› show_debug_info?}
    Q1 -->|SÃ­| Q2[ğŸ“‹ Mostrar logs de refinamiento]
    Q1 -->|No| Q3[ğŸ“ˆ Mostrar solo mÃ©tricas finales]
    Q2 --> Q3
    Q3 --> R[ğŸ¯ Actualizar mÃ©tricas de sesiÃ³n en UI]
    R --> S[âœ… Fin del proceso]
    
    L --> S
    
    %% Servicios Externos destacados
    H5b:::external
    H5e:::external  
    H7:::external
    H17a:::external
    H18a:::external
    H23:::external
    
    %% Decisiones destacadas
    G:::decision
    H4:::decision
    H9:::decision
    H13:::decision
    H14:::decision
    H22:::decision
    B1:::decision
    B5:::decision
    K:::decision
    N:::decision
    O1d:::decision
    Q1:::decision
    P5:::decision
    H1a:::decision
    H1c:::decision
    
    %% Procesos locales destacados
    H10a:::local
    H15a:::local
    H16a:::local
    F2:::local
    H1b:::local
    H11a:::local
    
    classDef external fill:#ffcccc,stroke:#ff6666,stroke-width:3px,color:#000
    classDef decision fill:#fff2cc,stroke:#d6b656,stroke-width:2px,color:#000
    classDef local fill:#d4edda,stroke:#28a745,stroke-width:2px,color:#000
```

## ğŸ”§ Leyenda de Componentes

### ğŸŒ Servicios Externos (Rojo)
- **Weaviate Cloud**: Base de datos vectorial externa
  - `QuestionsMiniLM`: BÃºsqueda de preguntas similares
  - `DocumentsMpnet`: BÃºsqueda de documentos
- **OpenAI API**: GeneraciÃ³n y evaluaciÃ³n
- **Gemini API**: GeneraciÃ³n de respuestas (si habilitado)

### ğŸ’š Procesos Locales (Verde)
- **Embedding local**: sentence-transformers
- **CrossEncoder local**: ms-marco-MiniLM-L-6-v2
- **Llama 3.1 8B**: Modelo local de generaciÃ³n
- **Mistral 7B**: Modelo local de refinamiento
- **CÃ¡lculos de similitud**: numpy/sklearn local

### ğŸŸ¡ Puntos de DecisiÃ³n (Amarillo)
- **RAG habilitado**: Determina pipeline completo vs solo documentos
- **use_questions_collection**: Habilita bÃºsqueda en preguntas
- **use_llm_reranker**: Usa CrossEncoder vs similitud estÃ¡ndar
- **SelecciÃ³n de modelo**: Determina API externa vs local
- **evaluate_quality**: Activa evaluaciÃ³n adicional con APIs

## ğŸ“Š MÃ©tricas y Tiempos

### âš¡ Tiempos Estimados por Componente
- **BÃºsqueda Weaviate**: 0.1-0.5s
- **Embedding local**: 0.05-0.2s  
- **CrossEncoder reranking**: 0.3-1.0s
- **GeneraciÃ³n local (Llama)**: 2-10s
- **GeneraciÃ³n API (GPT-4)**: 1-5s
- **EvaluaciÃ³n OpenAI**: 1-3s

### ğŸ“ˆ Optimizaciones de Costo
- **Modelos locales por defecto**: Llama + Mistral
- **Reranking local**: CrossEncoder instead of GPT-4
- **Refinamiento local**: Mistral instead of Gemini
- **Embeddings locales**: sentence-transformers

## ğŸ”„ Flujos Alternativos

### ğŸš€ Modo RÃ¡pido (Sin RAG)
```
Usuario â†’ ConfiguraciÃ³n â†’ BÃºsqueda â†’ Weaviate â†’ Reranking â†’ Documentos â†’ UI
```

### ğŸ§  Modo Completo (Con RAG)
```
Usuario â†’ ConfiguraciÃ³n â†’ Refinamiento â†’ BÃºsqueda â†’ Reranking â†’ GeneraciÃ³n â†’ EvaluaciÃ³n â†’ UI
```

### ğŸ’° Modo Costo Cero
```
Todo local: Mistral refinement + Local embeddings + CrossEncoder + Llama generation
```