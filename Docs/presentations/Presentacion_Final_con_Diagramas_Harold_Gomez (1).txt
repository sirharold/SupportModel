Integración de NLP en la
Gestión de Tickets de
Soporte Técnico
Harold Gómez V.

Contexto y Diagnóstico

• Alta carga operativa en resolución de tickets
• Documentación técnica infrautilizada
• Necesidad de automatización basada en
comprensión semántica

Objetivo General y Específicos

• Desarrollar un sistema de recuperación
semántica para soporte técnico
• Recolectar y vectorizar documentación
pública de Azure
• Evaluar diferentes modelos de embedding
• Integrar y consultar en una base de datos
vectorial

Flujo General del Sistema
Propuesto

Flujo general

Extracción de Documentación y Q&A

• Microsoft Q&A: +30.000 preguntas con
respuestas aceptadas
• Microsoft Learn: +50000 artículos técnicos
(Sólo Azure)
• Scraping con Selenium y BeautifulSoup

Modelos de Embedding Evaluados

• S-BERT (all-MiniLM-L6-v2)
• E5 (base y large)
• OpenAI (text-embedding-3-large)

Integración con Base de Datos Vectorial
• Weaviate: consultas híbridas (vector + filtros)
• Clases separadas: Documentación, QnA,
Embeddings
Class: Documentation
Campos:
• title (string): Título del artículo
técnico.
• summary (string): Resumen del
contenido.
• content (text): Cuerpo completo del
documento.
• url (string): Enlace a la fuente
(Microsoft Learn).
• Vectorización: text2vec-openai

Class: QnA
Campos:
• title (string): Título de la
pregunta.
• question_content (text):
Detalle de la consulta.
• accepted_answer (text):
Respuesta validada por la
comunidad.
• tags (string[]): Etiquetas
técnicas (ej: "Azure", "NSG").

Class: EmbeddingModelX
Campos:
• input_type (string): (title,
content, title+content)
• OfDocument (UUID):
Reference to the
Documentation .
• Vector: Manually created
with the respective model

Evaluación del Sistema de Recuperación
Recall@k (Sensibilidad)

¿Cuántos documentos relevantes se recuperan en los top k resultados?
Ejemplo: Recall@5 = 0.10 → 10% de los documentos relevantes están en el top 5.

Precision@k (Exactitud)

¿Qué proporción de los top k resultados son relevantes?
Ejemplo: Precision@5 = 0.03 → 3% de los 5 resultados son correctos.

MRR (Mean Reciprocal Rank)

Posición promedio del primer resultado relevante (prioriza rankings altos).
Ejemplo: MRR = 0.05 → El primer acierto suele estar en la posición ~20.

nDCG (Normalized Discounted Cumulative Gain)

Calidad del ranking considerando relevancia y posición.
Ejemplo: nDCG = 0.06 → Los documentos más útiles aparecen lejos del top.

Desempeño sentence-transformers/all-MiniLM-L6v2

input_type

F1

Tiempo
total (s)

Tiempo
prom.
(s)

nDCG

Recall

Precision

MRR

title

0.0502

0.0893

0.0272

0.0447 0.0393

334.04

0.1156

title+summary

0.0483

0.0897

0.0278

0.0441 0.0399

319.52

0.1116

content

0.0578

0.0685

0.0216

0.0512 0.0308

318.44

0.1080

title+content

0.0630

0.0779

0.0241

0.0550 0.0347

321.87

0.1087

title+summary+c
0.0649
ontent

0.0833

0.0256

0.0573 0.0370

322.12

0.1084

Cambios al enfoque debido a los resultados

Se tienen 3089 preguntas con links a evaluar
Se dividirá en dos sets: 2000 training
Se creará una capa de preentrenamiento y
utilizando el resultado, se volverá a evaluar

Futuras Líneas de estudio
1. Segmentación Semántica por Dominio
Problema: Los embeddings actuales no distinguen entre áreas técnicas
(ej: Azure Compute vs. Networking).
Solución: Clasificar preguntas/documentos en subdominios (clustering no
supervisado).
Entrenar embeddings especializados por dominio (ej: fine-tuning de E5 en
datos de Azure Storage).
Impacto esperado: ↑Recall@k al reducir ruido semántico.

Futuras Líneas de estudio
2. Capa de Re-Ranking con Modelos Cruzados
Problema: Baja Precision@k (muchos resultados irrelevantes en el top 5).
Solución:
Añadir un modelo de re-ranking (ej: Cohere Rerank o BGE-Reranker) sobre
los top-50 resultados.
Entrenar un clasificador binario (relevante/no relevante) con feedback de
agentes humanos.
Impacto esperado: ↑Precision@k sin sacrificar Recall.

Futuras Líneas de estudio
3. Aumento de Datos con Sintéticos
Problema: Falta de ejemplos etiquetados para dominios específicos.
Solución:
Generar preguntas-respuestas sintéticas con LLMs (ej: GPT-4) basadas en
la documentación.
Usar data augmentation para cubrir casos raros (ej: errores de
configuración en Azure Kubernetes).
Impacto esperado: ↑nDCG al mejorar la cobertura de escenarios.

Futuras Líneas de estudio
4. Filtros Híbridos (Vector + Reglas)
Problema: MRR bajo indica que los primeros resultados no son relevantes.
Solución:
Combinar búsqueda semántica con reglas basadas en metadatos (ej:
priorizar documentos actualizados en los últimos 6 meses).
Usar weighted hybrid search en Weaviate (ej: 70% vector + 30% BM25).
Impacto esperado: ↑MRR al mejorar el ranking inicial.

Agradecimientos y Preguntas

• Agradecimiento a familia, docentes y
compañeros
• Espacio para preguntas del jurado

