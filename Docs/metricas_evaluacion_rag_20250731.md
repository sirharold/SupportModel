# M√©tricas de Evaluaci√≥n para Sistemas de Recuperaci√≥n de Informaci√≥n Aumentada por Generaci√≥n (RAG)

**Fecha**: 31 de Julio, 2025  
**Versi√≥n**: 1.0  
**Autor**: Proyecto de Investigaci√≥n - Sistema de Soporte T√©cnico  

## Resumen Ejecutivo

Este documento presenta una revisi√≥n exhaustiva de las m√©tricas de evaluaci√≥n utilizadas en sistemas de Recuperaci√≥n de Informaci√≥n Aumentada por Generaci√≥n (Retrieval-Augmented Generation, RAG). Se analizan tres categor√≠as principales de m√©tricas: m√©tricas de recuperaci√≥n de informaci√≥n tradicionales, m√©tricas espec√≠ficas del framework RAGAS, y m√©tricas de evaluaci√≥n sem√°ntica BERTScore. El an√°lisis incluye fundamentos te√≥ricos, formulaciones matem√°ticas, interpretaci√≥n pr√°ctica y aplicabilidad en dominios t√©cnicos especializados.

## 1. Introducci√≥n

Los sistemas de Recuperaci√≥n de Informaci√≥n Aumentada por Generaci√≥n (RAG) combinan t√©cnicas de recuperaci√≥n de informaci√≥n con modelos de generaci√≥n de lenguaje natural para producir respuestas contextualmente relevantes y factualmente precisas (Lewis et al., 2020). La evaluaci√≥n de estos sistemas requiere un conjunto comprehensivo de m√©tricas que capture tanto la efectividad de la recuperaci√≥n como la calidad de la generaci√≥n.

### 1.1 Desaf√≠os en la Evaluaci√≥n de Sistemas RAG

La evaluaci√≥n de sistemas RAG presenta desaf√≠os √∫nicos que difieren de los sistemas tradicionales de recuperaci√≥n de informaci√≥n o generaci√≥n de texto de manera aislada:

1. **Evaluaci√≥n Multi-dimensional**: Necesidad de evaluar tanto la recuperaci√≥n de documentos relevantes como la calidad de las respuestas generadas
2. **Dependencia Contextual**: La calidad de la generaci√≥n depende cr√≠ticamente de la relevancia y completitud de los documentos recuperados
3. **Evaluaci√≥n de Fidelidad**: Verificaci√≥n de que las respuestas generadas se mantengan fieles al contenido recuperado
4. **Escalabilidad de Evaluaci√≥n**: Necesidad de m√©tricas automatizadas que permitan evaluaci√≥n a gran escala

## 2. M√©tricas de Recuperaci√≥n de Informaci√≥n Tradicionales

Las m√©tricas de recuperaci√≥n de informaci√≥n tradicionales eval√∫an la efectividad del componente de recuperaci√≥n del sistema RAG, determinando qu√© tan bien el sistema identifica y ordena documentos relevantes para una consulta dada.

### 2.1 Precision@k

La Precision@k mide la proporci√≥n de documentos relevantes entre los primeros k documentos recuperados.

**Formulaci√≥n Matem√°tica:**
```
Precision@k = |{documentos relevantes} ‚à© {top-k documentos recuperados}| / k
```

**Interpretaci√≥n:**
- Rango: [0, 1]
- Valor 1: Todos los documentos en top-k son relevantes
- Valor 0: Ning√∫n documento en top-k es relevante

**Aplicaci√≥n en Contexto RAG:**
La Precision@k es particularmente relevante en sistemas RAG porque determina la calidad del contexto proporcionado al modelo generativo. Manning et al. (2008) se√±alan que la precisi√≥n en posiciones tempranas del ranking es cr√≠tica para aplicaciones donde el n√∫mero de documentos utilizados como contexto es limitado.

**Ejemplo Pr√°ctico:**
Para una consulta sobre "configuraci√≥n de Azure Storage encryption" con k=5:
- Documentos recuperados: [Doc1: Azure Storage, Doc2: Azure Encryption, Doc3: AWS Storage, Doc4: Azure Keys, Doc5: Google Cloud]
- Documentos relevantes: Doc1, Doc2, Doc4
- Precision@5 = 3/5 = 0.60

### 2.2 Recall@k

El Recall@k mide la proporci√≥n de documentos relevantes que son recuperados entre los primeros k documentos.

**Formulaci√≥n Matem√°tica:**
```
Recall@k = |{documentos relevantes} ‚à© {top-k documentos recuperados}| / |{total documentos relevantes}|
```

**Interpretaci√≥n:**
- Rango: [0, 1]
- Valor 1: Todos los documentos relevantes est√°n en top-k
- Valor 0: Ning√∫n documento relevante est√° en top-k

**Limitaciones en Contextos Pr√°cticos:**
El c√°lculo de Recall@k requiere conocimiento completo del conjunto de documentos relevantes, lo cual es raramente disponible en aplicaciones del mundo real. Buckley & Voorhees (2004) argumentan que esta limitaci√≥n hace que el recall sea m√°s √∫til como m√©trica de investigaci√≥n que como m√©trica operacional.

### 2.3 F1-Score@k

El F1-Score@k combina Precision@k y Recall@k en una m√©trica √∫nica que representa la media arm√≥nica de ambas.

**Formulaci√≥n Matem√°tica:**
```
F1@k = 2 √ó (Precision@k √ó Recall@k) / (Precision@k + Recall@k)
```

**Ventajas:**
- Proporciona balance entre precision y recall
- Penaliza desbalances extremos entre precision y recall
- Permite comparaci√≥n hol√≠stica entre sistemas

**Aplicaci√≥n en Sistemas RAG:**
El F1-Score@k es especialmente valioso en sistemas RAG donde tanto la cobertura de informaci√≥n relevante como la precisi√≥n del contexto son cr√≠ticas para la calidad de la respuesta generada.

### 2.4 Normalized Discounted Cumulative Gain (NDCG@k)

NDCG@k eval√∫a la calidad del ranking considerando tanto la relevancia como la posici√≥n de los documentos recuperados.

**Formulaci√≥n Matem√°tica:**
```
DCG@k = rel‚ÇÅ + Œ£·µ¢‚Çå‚ÇÇ·µè (rel·µ¢ / log‚ÇÇ(i+1))

IDCG@k = DCG ideal ordenando documentos por relevancia descendente

NDCG@k = DCG@k / IDCG@k
```

**Caracter√≠sticas:**
- Considera relevancia gradual (no binaria)
- Penaliza documentos relevantes en posiciones bajas
- Normalizado para permitir comparaciones entre consultas

**Relevancia en RAG:**
J√§rvelin & Kek√§l√§inen (2002) demuestran que NDCG es particularmente efectivo para evaluar sistemas donde el orden de presentaci√≥n afecta la utilidad, como en sistemas RAG donde los primeros documentos tienen mayor influencia en la generaci√≥n.

### 2.5 Mean Average Precision (MAP@k)

MAP@k calcula la media de la precisi√≥n promedio para cada consulta, considerando todas las posiciones donde aparecen documentos relevantes.

**Formulaci√≥n Matem√°tica:**
```
AP@k = (1/|rel|) √ó Œ£·µ¢‚Çå‚ÇÅ·µè (Precision@i √ó rel(i))

MAP@k = (1/|Q|) √ó Œ£·µ©‚Çå‚ÇÅ|Q| AP@k(q)
```

donde rel(i) = 1 si el documento en posici√≥n i es relevante, 0 en caso contrario.

**Interpretaci√≥n:**
MAP@k proporciona una medida comprehensiva de la calidad del ranking que considera tanto el n√∫mero como las posiciones de los documentos relevantes recuperados.

### 2.6 Mean Reciprocal Rank (MRR)

MRR eval√∫a la efectividad del sistema considerando la posici√≥n del primer documento relevante recuperado.

**Formulaci√≥n Matem√°tica:**
```
RR = 1/rank_primer_relevante

MRR = (1/|Q|) √ó Œ£·µ¢‚Çå‚ÇÅ|Q| RR·µ¢
```

**Aplicaci√≥n Espec√≠fica:**
MRR es particularmente relevante en sistemas RAG donde la respuesta puede ser efectivamente generada bas√°ndose en un √∫nico documento altamente relevante, como en sistemas de preguntas y respuestas t√©cnicas.

## 3. M√©tricas del Framework RAGAS

RAGAS (Retrieval Augmented Generation Assessment) es un framework especializado para la evaluaci√≥n de sistemas RAG que introduce m√©tricas espec√≠ficamente dise√±adas para evaluar aspectos √∫nicos de estos sistemas (Es et al., 2023).

### 3.1 Faithfulness (Fidelidad)

Faithfulness eval√∫a el grado en que la respuesta generada se mantiene fiel al contexto proporcionado, evitando alucinaciones o contradicciones.

**Definici√≥n Formal:**
```
Faithfulness = |{claims verificables en contexto}| / |{total claims en respuesta}|
```

**Metodolog√≠a de Evaluaci√≥n:**
1. Extracci√≥n de afirmaciones (claims) de la respuesta generada
2. Verificaci√≥n de cada afirmaci√≥n contra el contexto proporcionado
3. C√°lculo de la proporci√≥n de afirmaciones verificables

**Importancia en Sistemas T√©cnicos:**
En dominios t√©cnicos como documentaci√≥n de Azure, la fidelidad es cr√≠tica porque informaci√≥n incorrecta puede llevar a configuraciones err√≥neas o fallos de seguridad.

### 3.2 Answer Relevancy (Relevancia de Respuesta)

Answer Relevancy mide qu√© tan bien la respuesta generada aborda la pregunta planteada, independientemente de su fidelidad al contexto.

**Formulaci√≥n Conceptual:**
```
Answer Relevancy = similitud_sem√°ntica(pregunta_original, pregunta_reconstruida_desde_respuesta)
```

**Proceso de Evaluaci√≥n:**
1. Generaci√≥n de preguntas que podr√≠an ser respondidas por la respuesta generada
2. C√°lculo de similitud sem√°ntica entre pregunta original y preguntas generadas
3. Promedio de similitudes como medida de relevancia

**Consideraciones T√©cnicas:**
La m√©trica utiliza modelos de lenguaje para generar preguntas contrafactuales, lo que introduce dependencia en la calidad del modelo evaluador.

### 3.3 Answer Correctness (Correcci√≥n de Respuesta)

Answer Correctness eval√∫a la precisi√≥n factual de la respuesta generada compar√°ndola con una respuesta de referencia (ground truth).

**Componentes:**
```
Answer Correctness = w‚ÇÅ √ó Exactitud_Factual + w‚ÇÇ √ó Completitud_Sem√°ntica
```

donde w‚ÇÅ + w‚ÇÇ = 1 son pesos que balancean exactitud y completitud.

**Desaf√≠os de Implementaci√≥n:**
- Requiere respuestas de referencia de alta calidad
- Evaluaci√≥n subjetiva de completitud sem√°ntica
- Sensibilidad a variaciones ling√º√≠sticas en respuestas correctas

### 3.4 Context Precision (Precisi√≥n de Contexto)

Context Precision eval√∫a qu√© tan relevantes son los documentos recuperados para responder la pregunta espec√≠fica.

**Formulaci√≥n:**
```
Context Precision@k = Œ£·µ¢‚Çå‚ÇÅ·µè (relevancia(doc·µ¢) √ó Œ†‚±º‚Çå‚ÇÅ‚Å±‚Åª¬π irrelevancia(doc‚±º)) / Œ£·µ¢‚Çå‚ÇÅ·µè Œ†‚±º‚Çå‚ÇÅ‚Å±‚Åª¬π irrelevancia(doc‚±º)
```

**Interpretaci√≥n:**
La m√©trica penaliza documentos irrelevantes en posiciones tempranas del ranking, reflejando la importancia del orden en sistemas RAG.

### 3.5 Context Recall (Recall de Contexto)

Context Recall mide qu√© tan completo es el contexto recuperado en relaci√≥n a la informaci√≥n necesaria para generar la respuesta ground truth.

**Definici√≥n:**
```
Context Recall = |{oraciones en ground truth atribuibles a contexto}| / |{total oraciones en ground truth}|
```

**Proceso de Evaluaci√≥n:**
1. Segmentaci√≥n de respuesta ground truth en oraciones
2. Verificaci√≥n de atribuibilidad de cada oraci√≥n al contexto recuperado
3. C√°lculo de proporci√≥n de oraciones atribuibles

### 3.6 Semantic Similarity (Similitud Sem√°ntica)

Semantic Similarity cuantifica la similitud sem√°ntica entre la respuesta generada y la respuesta ground truth utilizando embeddings de texto.

**Formulaci√≥n:**
```
Semantic Similarity = cosine_similarity(embedding(respuesta_generada), embedding(ground_truth))
```

**Ventajas:**
- Captura similitudes sem√°nticas m√°s all√° de coincidencias l√©xicas
- Robusta a variaciones sint√°cticas
- Escalable computacionalmente

## 4. M√©tricas BERTScore

BERTScore utiliza representaciones contextuales pre-entrenadas para evaluar la calidad del texto generado mediante similitud sem√°ntica a nivel de token (Zhang et al., 2019).

### 4.1 Fundamentaci√≥n Te√≥rica

BERTScore supera limitaciones de m√©tricas tradicionales como BLEU al:
- Utilizar representaciones contextuales en lugar de n-gramas
- Capturar similitudes sem√°nticas profundas
- Ser robusta a variaciones superficiales

### 4.2 BERT Precision

**Formulaci√≥n:**
```
BERT Precision = (1/|x|) √ó Œ£‚Çì·µ¢‚àà‚Çì max_{≈∑‚±º‚àà≈∑} cos(ùê±·µ¢, ùê≤ÃÇ‚±º)
```

donde x es la respuesta generada, ≈∑ es la referencia, y ùê±·µ¢, ùê≤ÃÇ‚±º son embeddings BERT.

**Interpretaci√≥n:**
Mide qu√© proporci√≥n de tokens en la respuesta generada tienen correspondencias sem√°nticamente similares en la referencia.

### 4.3 BERT Recall

**Formulaci√≥n:**
```
BERT Recall = (1/|≈∑|) √ó Œ£_{≈∑‚±º‚àà≈∑} max_{x·µ¢‚ààx} cos(ùê±·µ¢, ùê≤ÃÇ‚±º)
```

**Interpretaci√≥n:**
Mide qu√© proporci√≥n de tokens en la referencia tienen correspondencias sem√°nticamente similares en la respuesta generada.

### 4.4 BERT F1

**Formulaci√≥n:**
```
BERT F1 = 2 √ó (BERT Precision √ó BERT Recall) / (BERT Precision + BERT Recall)
```

**Ventajas en Evaluaci√≥n RAG:**
- Eval√∫a tanto completitud como precisi√≥n sem√°ntica
- Menos sensible a variaciones estil√≠sticas
- Correlaciona mejor con juicio humano que m√©tricas l√©xicas

## 5. Consideraciones para Dominios T√©cnicos Especializados

### 5.1 Desaf√≠os Espec√≠ficos

La evaluaci√≥n de sistemas RAG en dominios t√©cnicos presenta desaf√≠os √∫nicos:

1. **Terminolog√≠a Especializada**: Vocabulario t√©cnico requiere embeddings domain-espec√≠ficos
2. **Precisi√≥n Cr√≠tica**: Errores pueden tener consecuencias operacionales severas
3. **Contexto Extenso**: Documentaci√≥n t√©cnica requiere procesamiento de contextos largos
4. **Evaluaci√≥n Multi-modal**: Inclusi√≥n de diagramas, c√≥digo, y configuraciones

### 5.2 Adaptaciones Metodol√≥gicas

**Ground Truth Escaso:**
En dominios especializados, el ground truth t√≠picamente contiene un √∫nico documento relevante por consulta, limitando la precision m√°xima te√≥rica:

```
Precision@k_max = 1/k
```

**M√©tricas Adaptadas:**
- Success@k: Presencia del documento correcto en top-k
- Reciprocal Rank: Inverso de la posici√≥n del documento correcto
- Coverage: Proporci√≥n de consultas con al menos un documento relevante en top-k

### 5.3 Validaci√≥n Estad√≠stica

La evaluaci√≥n en dominios especializados requiere validaci√≥n estad√≠stica rigurosa debido a:
- Tama√±os de muestra limitados
- Variabilidad en complejidad de consultas
- Sesgos de evaluaci√≥n humana

**Pruebas Recomendadas:**
- Test de Wilcoxon para comparaci√≥n de distribuciones no param√©tricas
- Bootstrap para estimaci√≥n de intervalos de confianza
- An√°lisis de potencia estad√≠stica para determinar tama√±os de muestra

## 6. Integraci√≥n de M√©tricas en Pipeline de Evaluaci√≥n

### 6.1 Arquitectura de Evaluaci√≥n Multi-dimensional

Un sistema de evaluaci√≥n robusto para RAG debe integrar las tres categor√≠as de m√©tricas:

1. **Fase de Recuperaci√≥n**: M√©tricas IR tradicionales eval√∫an calidad del ranking
2. **Fase de Generaci√≥n**: M√©tricas RAGAS eval√∫an calidad de respuesta contextual
3. **Fase de Validaci√≥n**: BERTScore proporciona evaluaci√≥n sem√°ntica automatizada

### 6.2 Ponderaci√≥n de M√©tricas

La combinaci√≥n de m√∫ltiples m√©tricas requiere esquemas de ponderaci√≥n apropiados:

```
Score_Compuesto = Œ±‚ÇÅ √ó NDCG@k + Œ±‚ÇÇ √ó Faithfulness + Œ±‚ÇÉ √ó BERT_F1
```

donde Œ±‚ÇÅ + Œ±‚ÇÇ + Œ±‚ÇÉ = 1 y los pesos reflejan la importancia relativa de cada aspecto.

### 6.3 Interpretaci√≥n Hol√≠stica

La interpretaci√≥n de resultados debe considerar:
- Correlaciones entre m√©tricas de diferentes categor√≠as
- Trade-offs entre precision y recall en recuperaci√≥n
- Balance entre fidelidad y completitud en generaci√≥n

## 7. Limitaciones y Direcciones Futuras

### 7.1 Limitaciones Actuales

1. **Dependencia en Ground Truth**: Muchas m√©tricas requieren datos de referencia de alta calidad
2. **Evaluaci√≥n Subjetiva**: Aspectos como relevancia y calidad requieren juicio humano
3. **Escalabilidad**: Evaluaci√≥n manual no escala a sistemas de producci√≥n
4. **Sesgo de Modelo**: M√©tricas basadas en LLM heredan sesgos del modelo evaluador

### 7.2 Direcciones de Investigaci√≥n

1. **M√©tricas sin Referencia**: Desarrollo de m√©tricas que no requieran ground truth
2. **Evaluaci√≥n Multimodal**: Extensi√≥n a contenido visual y multimedia
3. **M√©tricas Espec√≠ficas de Dominio**: Desarrollo de m√©tricas especializadas por dominio
4. **Evaluaci√≥n Continua**: Sistemas de evaluaci√≥n en tiempo real para entornos de producci√≥n

## 8. Conclusiones

La evaluaci√≥n de sistemas RAG requiere un enfoque multi-dimensional que capture tanto la efectividad de la recuperaci√≥n como la calidad de la generaci√≥n. Las m√©tricas tradicionales de recuperaci√≥n de informaci√≥n proporcionan fundamentos s√≥lidos para evaluar el componente de recuperaci√≥n, mientras que frameworks especializados como RAGAS y m√©tricas sem√°nticas como BERTScore abordan aspectos √∫nicos de la generaci√≥n aumentada.

En dominios t√©cnicos especializados, las limitaciones de ground truth escaso requieren adaptaciones metodol√≥gicas y m√©tricas alternativas. La validaci√≥n estad√≠stica rigurosa es esencial para conclusiones confiables, y la integraci√≥n de m√∫ltiples m√©tricas proporciona una evaluaci√≥n m√°s comprehensiva que m√©tricas individuales.

El desarrollo continuo de m√©tricas de evaluaci√≥n para sistemas RAG permanece como un √°rea activa de investigaci√≥n, con necesidades particulares en dominios especializados donde la precisi√≥n y fidelidad son cr√≠ticas para aplicaciones operacionales.

## Referencias

Buckley, C., & Voorhees, E. M. (2004). Retrieval evaluation with incomplete information. *Proceedings of the 27th annual international ACM SIGIR conference on Research and development in information retrieval*, 25-32.

Es, S., James, J., Espinosa-Anke, L., & Schockaert, S. (2023). RAGAS: Automated evaluation of retrieval augmented generation. *arXiv preprint arXiv:2309.15217*.

J√§rvelin, K., & Kek√§l√§inen, J. (2002). Cumulated gain-based evaluation of IR techniques. *ACM Transactions on Information Systems*, 20(4), 422-446.

Lewis, P., Perez, E., Piktus, A., Petroni, F., Karpukhin, V., Goyal, N., ... & Kiela, D. (2020). Retrieval-augmented generation for knowledge-intensive nlp tasks. *Advances in Neural Information Processing Systems*, 33, 9459-9474.

Manning, C. D., Raghavan, P., & Sch√ºtze, H. (2008). *Introduction to information retrieval*. Cambridge University Press.

Zhang, T., Kishore, V., Wu, F., Weinberger, K. Q., & Artzi, Y. (2019). BERTScore: Evaluating text generation with BERT. *arXiv preprint arXiv:1904.09675*.

---

**Nota**: Este documento forma parte de la metodolog√≠a de evaluaci√≥n desarrollada para el proyecto de investigaci√≥n sobre sistemas de soporte t√©cnico automatizado. Las m√©tricas y metodolog√≠as descritas han sido implementadas y validadas emp√≠ricamente en el contexto de documentaci√≥n t√©cnica de Microsoft Azure.