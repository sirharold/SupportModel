# Configuraci√≥n de Modelos Locales para Optimizaci√≥n de Costos

Este documento explica c√≥mo configurar y usar modelos locales para eliminar completamente los costos de APIs externas.

## üéØ Beneficios

- **Costo Cero**: Sin gastos de APIs de OpenAI, Gemini, etc.
- **Privacidad**: Todos los datos permanecen en tu sistema
- **Control Total**: Sin dependencias de servicios externos
- **Rendimiento**: Latencia constante sin l√≠mites de rate

## üìã Requisitos del Sistema

### Hardware M√≠nimo
- **RAM**: 8GB+ (16GB recomendado)
- **GPU**: NVIDIA con 6GB+ VRAM (opcional pero recomendado)
- **Almacenamiento**: 20GB libres para modelos

### Hardware Recomendado
- **RAM**: 32GB+
- **GPU**: NVIDIA RTX 3080/4080 o superior
- **CPU**: Intel i7/AMD Ryzen 7 o superior

## üöÄ Instalaci√≥n

### 1. Instalar Dependencias

```bash
pip install -r requirements.txt
```

### 2. Instalar PyTorch con CUDA (si tienes GPU NVIDIA)

```bash
# Para CUDA 11.8
pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118

# Para CUDA 12.1
pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121
```

### 3. Configurar Hugging Face Token (opcional)

```bash
# Para acceso a modelos de Meta (Llama)
huggingface-cli login
```

## üîß Configuraci√≥n

### 1. Modelos Disponibles

La aplicaci√≥n ahora incluye estos modelos locales:

- **Llama 3.1 8B**: Generaci√≥n de respuestas de alta calidad
- **Mistral 7B**: Refinamiento de queries y generaci√≥n r√°pida
- **SentenceTransformers**: Embeddings locales (ya configurados)

### 2. Configuraci√≥n Autom√°tica

Los modelos se descargan autom√°ticamente en el primer uso:

```python
# En config.py - ya configurado
DEFAULT_GENERATIVE_MODEL = "llama-3.1-8b"  # Ahora usa modelo local por defecto
```

### 3. Variables de Entorno

Aseg√∫rate de que estas variables est√©n configuradas en tu `.env`:

```env
# Opcional: Solo para modelos HuggingFace privados
HUGGINGFACE_API_KEY=tu_token_aqui

# Las siguientes son ahora OPCIONALES
# OPENAI_API_KEY=  # Solo si quieres usar GPT-4 como fallback
# GEMINI_API_KEY=  # Solo si quieres usar Gemini como fallback
```

## üñ•Ô∏è Uso

### 1. Selecci√≥n de Modelos

En la interfaz de Streamlit:

1. Ve a **Configuraci√≥n Avanzada**
2. Selecciona **Modelo Generativo**: `llama-3.1-8b` o `mistral-7b`
3. Mant√©n **Modelos de Embedding** en opciones locales (HuggingFace)

### 2. Primera Ejecuci√≥n

El primer uso descargar√° los modelos (puede tomar 10-30 minutos):

```
[INFO] Loading model: llama-3.1-8b from meta-llama/Llama-3.1-8B-Instruct
[INFO] Model downloaded successfully
```

### 3. Monitoreo de Recursos

Para monitorear uso de GPU:

```bash
# En terminal separada
nvidia-smi -l 1
```

## ‚ö° Optimizaciones

### 1. Cuantizaci√≥n Autom√°tica

Los modelos usan cuantizaci√≥n 4-bit autom√°ticamente para optimizar memoria:

```python
# Configuraci√≥n autom√°tica en local_models.py
BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_quant_type="nf4",
    bnb_4bit_compute_dtype=torch.float16
)
```

### 2. Gesti√≥n de Memoria

Para liberar memoria manualmente:

```python
from utils.local_models import cleanup_models
cleanup_models()
```

### 3. Modelos M√°s Peque√±os

Si tienes recursos limitados, puedes modificar `local_models.py`:

```python
# Cambiar a modelos m√°s peque√±os
"mistral-7b": "microsoft/DialoGPT-medium"  # Modelo m√°s peque√±o
"llama-3.1-8b": "microsoft/DialoGPT-large"  # Alternativa m√°s ligera
```

## üîç Troubleshooting

### Error: "CUDA out of memory"

```python
# Reducir batch size en local_models.py
max_new_tokens=256  # Reducir de 512
```

### Error: "Model not found"

```bash
# Limpiar cach√© de HuggingFace
rm -rf ~/.cache/huggingface/
```

### Rendimiento Lento

1. **Verificar GPU**: Aseg√∫rate de que PyTorch detecte tu GPU
2. **Aumentar RAM**: Cerrar aplicaciones innecesarias
3. **Usar modelos m√°s peque√±os**: Cambiar a Mistral 7B

### Error de Permisos con Llama

```bash
# Obtener acceso a modelos de Meta
huggingface-cli login
# Ir a https://huggingface.co/meta-llama/Llama-3.1-8B-Instruct
# Solicitar acceso si es necesario
```

## üìä Comparaci√≥n de Rendimiento

| M√©trica | APIs Externas | Modelos Locales |
|---------|---------------|-----------------|
| **Costo por consulta** | $0.10-0.20 | $0.00 |
| **Latencia inicial** | 2-5s | 10-30s (primera carga) |
| **Latencia normal** | 2-5s | 1-3s |
| **Privacidad** | Datos enviados | 100% local |
| **Disponibilidad** | Depende de API | 24/7 |

## ‚úÖ Verificaci√≥n de Instalaci√≥n

Ejecuta este script para verificar que todo funciona:

```python
from utils.local_models import get_llama_client, get_mistral_client

# Probar Llama
llama = get_llama_client()
response = llama.generate_answer("¬øQu√© es Python?", "Python es un lenguaje de programaci√≥n")
print("Llama respuesta:", response)

# Probar Mistral  
mistral = get_mistral_client()
refined = mistral.refine_query("Hola, ¬øpodr√≠as ayudarme con Python por favor?")
print("Mistral refinado:", refined)
```

## üéâ ¬°Listo!

Una vez configurado, tu aplicaci√≥n funcionar√° completamente sin costos de API. Los modelos locales ofrecen calidad comparable a GPT-4 y Gemini Pro para la mayor√≠a de casos de uso.

Para soporte adicional, revisa los logs en la consola o abre un issue en el repositorio.