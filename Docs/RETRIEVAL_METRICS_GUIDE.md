# üìä Gu√≠a de M√©tricas de Recuperaci√≥n para Sistema RAG

## üìã Descripci√≥n General

Este sistema incluye m√©tricas especializadas para evaluar la calidad de recuperaci√≥n de documentos antes y despu√©s del reranking. Las m√©tricas implementadas son est√°ndar en sistemas de recuperaci√≥n de informaci√≥n y permiten una evaluaci√≥n objetiva del rendimiento del sistema RAG.

## üéØ M√©tricas Implementadas

### 1. **MRR (Mean Reciprocal Rank)**
- **Definici√≥n**: Posici√≥n promedio del primer documento relevante
- **F√≥rmula**: `MRR = 1 / rank_of_first_relevant_document`
- **Rango**: [0, 1] donde 1 es perfecto (primer documento es relevante)
- **Interpretaci√≥n**: Mayor es mejor

### 2. **Recall@k**
- **Definici√≥n**: Fracci√≥n de documentos relevantes recuperados en los top k
- **F√≥rmula**: `Recall@k = |documentos_relevantes_en_top_k| / |total_documentos_relevantes|`
- **Rango**: [0, 1] donde 1 es perfecto (todos los relevantes recuperados)
- **Interpretaci√≥n**: Mayor es mejor

### 3. **Precision@k**
- **Definici√≥n**: Fracci√≥n de documentos recuperados que son relevantes en los top k
- **F√≥rmula**: `Precision@k = |documentos_relevantes_en_top_k| / k`
- **Rango**: [0, 1] donde 1 es perfecto (todos los recuperados son relevantes)
- **Interpretaci√≥n**: Mayor es mejor

### 4. **F1@k**
- **Definici√≥n**: Media arm√≥nica de Precision@k y Recall@k
- **F√≥rmula**: `F1@k = 2 √ó (Precision@k √ó Recall@k) / (Precision@k + Recall@k)`
- **Rango**: [0, 1] donde 1 es perfecto
- **Interpretaci√≥n**: Mayor es mejor

### 5. **Accuracy@k**
- **Definici√≥n**: Proporci√≥n de documentos correctamente clasificados (relevantes/no relevantes) en los top k
- **F√≥rmula**: `Accuracy@k = (TP + TN) / (TP + TN + FP + FN)`
- **Rango**: [0, 1] donde 1 es perfecto
- **Interpretaci√≥n**: Mayor es mejor

### 6. **BinaryAccuracy@k**
- **Definici√≥n**: Proporci√≥n de predicciones correctas en los top k (equivalente a Precision@k)
- **F√≥rmula**: `BinaryAccuracy@k = documentos_relevantes_en_top_k / k`
- **Rango**: [0, 1] donde 1 es perfecto
- **Interpretaci√≥n**: Mayor es mejor

### 7. **RankingAccuracy@k**
- **Definici√≥n**: Qu√© tan bien el sistema rankea documentos relevantes vs no relevantes
- **F√≥rmula**: Proporci√≥n de pares (relevante, no_relevante) donde relevante aparece antes
- **Rango**: [0, 1] donde 1 es perfecto
- **Interpretaci√≥n**: Mayor es mejor

## üìà Valores de k Evaluados

El sistema eval√∫a autom√°ticamente las m√©tricas para:
- **k=1**: Solo el primer documento (m√°s estricto)
- **k=3**: Top 3 documentos (uso t√≠pico)
- **k=5**: Top 5 documentos (balanceado)
- **k=10**: Top 10 documentos (m√°s permisivo)

## üîß Uso B√°sico

### 1. M√©tricas para Una Pregunta Individual

```python
from utils.qa_pipeline_with_metrics import answer_question_with_retrieval_metrics
from utils.clients import initialize_clients

# Inicializar clientes
weaviate_wrapper, embedding_client, openai_client, gemini_client, local_llama_client, local_mistral_client, _ = initialize_clients("multi-qa-mpnet-base-dot-v1")

# Ejecutar pipeline con m√©tricas
result = answer_question_with_retrieval_metrics(
    question="¬øC√≥mo configurar Azure Blob Storage?",
    weaviate_wrapper=weaviate_wrapper,
    embedding_client=embedding_client,
    openai_client=openai_client,
    gemini_client=gemini_client,
    local_llama_client=local_llama_client,
    local_mistral_client=local_mistral_client,
    top_k=10,
    use_llm_reranker=True,
    generate_answer=False,  # Solo documentos para m√©tricas
    calculate_metrics=True,
    ground_truth_answer=ground_truth_answer,
    ms_links=ms_links
)

# Extraer m√©tricas
docs, debug_info, retrieval_metrics = result
```

### 2. Mostrar M√©tricas Formateadas

```python
from utils.retrieval_metrics import format_metrics_for_display

# Formatear para display
formatted_output = format_metrics_for_display(retrieval_metrics)
print(formatted_output)
```

### 3. Evaluaci√≥n en Lotes

```python
from utils.qa_pipeline_with_metrics import batch_calculate_retrieval_metrics

# Preparar datos
questions_and_answers = [
    {
        'question': "¬øC√≥mo crear una VM en Azure?",
        'accepted_answer': "Para crear una VM...",
        'ms_links': ["https://learn.microsoft.com/azure/virtual-machines/..."]
    },
    # ... m√°s preguntas
]

# Calcular m√©tricas para todas las preguntas
all_metrics = batch_calculate_retrieval_metrics(
    questions_and_answers=questions_and_answers,
    weaviate_wrapper=weaviate_wrapper,
    embedding_client=embedding_client,
    openai_client=openai_client,
    top_k=10,
    use_llm_reranker=True
)

# Mostrar resumen
from utils.qa_pipeline_with_metrics import print_batch_metrics_summary
print_batch_metrics_summary(all_metrics)
```

## üìä Interpretaci√≥n de Resultados

### Ejemplo de Salida

```
üìä M√âTRICAS DE RECUPERACI√ìN - COMPARACI√ìN BEFORE/AFTER RERANKING
================================================================================
Ground Truth Links: 3
Docs Before: 10, Docs After: 10
--------------------------------------------------------------------------------
M√©trica         Before     After      Mejora     % Mejora  
--------------------------------------------------------------------------------
MRR             0.3333     1.0000     0.6667     200.00    %
Recall@1        0.0000     0.3333     0.3333     0.00      %
Precision@1     0.0000     1.0000     1.0000     0.00      %
F1@1            0.0000     0.5000     0.5000     0.00      %
--------------------------------------------------
Recall@5        0.6667     1.0000     0.3333     50.00     %
Precision@5     0.4000     0.6000     0.2000     50.00     %
F1@5            0.5000     0.7500     0.2500     50.00     %
```

### An√°lisis de Ejemplo

- **MRR mejor√≥ 200%**: El reranking movi√≥ el primer documento relevante de la posici√≥n 3 a la posici√≥n 1
- **Recall@5 mejor√≥ 50%**: Se recuperaron m√°s documentos relevantes en el top 5
- **Precision@5 mejor√≥ 50%**: Mayor proporci√≥n de documentos relevantes en el top 5
- **F1@5 balance√≥ ambos**: Mejora combinada en precisi√≥n y recall

## üéØ Casos de Uso en el Proyecto

### 1. **Evaluaci√≥n de Modelos de Embedding**
```python
# Comparar diferentes modelos
for model_key in ["multi-qa-mpnet-base-dot-v1", "all-MiniLM-L6-v2", "ada"]:
    # Calcular m√©tricas para cada modelo
    # Comparar resultados
```

### 2. **Impacto del Reranking**
```python
# Evaluar con y sin reranking
metrics_with_reranking = calculate_metrics(use_llm_reranker=True)
metrics_without_reranking = calculate_metrics(use_llm_reranker=False)
```

### 3. **Optimizaci√≥n de Hiperpar√°metros**
```python
# Probar diferentes valores de top_k
for top_k in [5, 10, 15, 20]:
    # Calcular m√©tricas para cada top_k
    # Encontrar valor √≥ptimo
```

## üîç Validaci√≥n de Ground Truth

### Extracci√≥n Autom√°tica de Enlaces

El sistema extrae autom√°ticamente enlaces de Microsoft Learn de las respuestas aceptadas:

```python
from utils.retrieval_metrics import extract_ground_truth_links

# Extraer ground truth de respuesta
ground_truth_links = extract_ground_truth_links(
    ground_truth_answer="Para configurar Azure Storage... https://learn.microsoft.com/azure/storage/...",
    ms_links=None  # Se extraen autom√°ticamente
)
```

### Validaci√≥n Manual

Tambi√©n puedes proporcionar enlaces manualmente:

```python
ms_links = [
    "https://learn.microsoft.com/azure/storage/common/storage-account-create",
    "https://learn.microsoft.com/azure/storage/blobs/storage-blobs-introduction"
]
```

## üìà Integraci√≥n con Streamlit

Las m√©tricas se pueden mostrar en la interfaz web:

```python
from utils.comparison_with_retrieval_metrics import show_retrieval_metrics_comparison

# En tu p√°gina de Streamlit
show_retrieval_metrics_comparison(
    question=question,
    selected_question=selected_question,
    top_k=10,
    use_reranker=True
)
```

## üß™ Testing y Validaci√≥n

### Ejecutar Tests

```bash
# Tests unitarios
python test_retrieval_metrics.py

# Demo completa
python demo_retrieval_metrics.py
```

### Casos de Prueba

Los tests incluyen:
- ‚úÖ M√©tricas b√°sicas con datos sint√©ticos
- ‚úÖ Comparaci√≥n before/after reranking
- ‚úÖ Casos edge (sin documentos, sin ground truth, etc.)
- ‚úÖ Validaci√≥n de f√≥rmulas matem√°ticas

## üìä M√©tricas Agregadas

Para evaluaciones en lotes, el sistema calcula:

```python
from utils.retrieval_metrics import calculate_aggregated_metrics

# Calcular estad√≠sticas agregadas
aggregated = calculate_aggregated_metrics(all_metrics)

# Disponible:
# - mean: Promedio
# - median: Mediana
# - std: Desviaci√≥n est√°ndar
# - min/max: Valores extremos
```

## üé® Visualizaci√≥n

El sistema incluye gr√°ficos interactivos:

- **Heatmaps**: Mejoras por modelo y m√©trica
- **Barras comparativas**: Before vs After
- **L√≠neas de tendencia**: Evoluci√≥n por valor de k
- **Rankings**: Mejor modelo por m√©trica

## üìã Checklist de Implementaci√≥n

Para usar las m√©tricas en tu proyecto:

- [ ] Configura conexi√≥n a Weaviate
- [ ] Prepara datos de ground truth (respuestas aceptadas)
- [ ] Importa `answer_question_with_retrieval_metrics`
- [ ] Ejecuta con `calculate_metrics=True`
- [ ] Usa `format_metrics_for_display` para mostrar resultados
- [ ] Implementa en interfaz web con Streamlit
- [ ] Ejecuta tests para validar funcionamiento

## üîó Referencias

- Voorhees, E. M. (1999). The TREC-8 question answering track evaluation. TREC.
- Manning, C. D., et al. (2008). Introduction to Information Retrieval. Cambridge University Press.
- Karpukhin, V., et al. (2020). Dense Passage Retrieval for Open-Domain Question Answering. EMNLP.

## üÜò Troubleshooting

### Problemas Comunes

1. **Error: No ground truth links found**
   - Verifica que la respuesta aceptada contenga enlaces de Microsoft Learn
   - Usa el par√°metro `ms_links` para proporcionar enlaces manualmente

2. **M√©tricas todas en 0**
   - Revisa que los documentos recuperados contengan el campo `link`
   - Verifica que los enlaces coincidan exactamente con el ground truth

3. **Error de importaci√≥n**
   - Aseg√∫rate de que todos los m√≥dulos est√©n en el PYTHONPATH
   - Instala dependencias: `pandas`, `numpy`, `plotly`

4. **Rendimiento lento**
   - Reduce `top_k` para evaluaciones r√°pidas
   - Usa `generate_answer=False` para solo calcular m√©tricas de recuperaci√≥n
   - Implementa cach√© para resultados repetidos