# ANEXO F: Manual de Usuario - Aplicaci√≥n Streamlit

## F.1 Introducci√≥n

Este anexo proporciona las instrucciones detalladas para utilizar la aplicaci√≥n web desarrollada con Streamlit que permite interactuar con el sistema RAG de recuperaci√≥n sem√°ntica de documentaci√≥n t√©cnica de Microsoft Azure. La aplicaci√≥n ofrece cuatro funcionalidades principales organizadas en p√°ginas independientes.

## F.2 Requisitos del Sistema

### F.2.1 Requisitos de Hardware
- **Memoria RAM**: M√≠nimo 8 GB (16 GB recomendado)
- **Almacenamiento**: 20 GB libres para modelos y base de datos
- **Procesador**: CPU multin√∫cleo (GPU opcional para aceleraci√≥n)

### F.2.2 Requisitos de Software
- **Python**: 3.8 o superior
- **Sistema Operativo**: Windows 10/11, macOS, o Linux
- **Navegador Web**: Chrome, Firefox, Safari o Edge (versiones recientes)

### F.2.3 Dependencias Principales
- Streamlit 1.46.1
- ChromaDB 0.5.23
- Sentence-Transformers 2.3.0
- OpenAI API (para modelo Ada)
- Matplotlib/Plotly para visualizaciones

## F.3 Instalaci√≥n y Configuraci√≥n

### F.3.1 Clonar el Repositorio
```bash
git clone https://github.com/sirharold/SupportModel.git
cd SupportModel
```

### F.3.2 Instalar Dependencias
```bash
pip install -r requirements.txt
```

### F.3.3 Configurar Variables de Entorno
Crear archivo `.env` en la ra√≠z del proyecto:
```
OPENAI_API_KEY=your_openai_api_key
GOOGLE_API_KEY=your_google_api_key  # Si se usa Gemini
```

### F.3.4 Iniciar la Aplicaci√≥n
```bash
streamlit run src/apps/main_qa_app.py
```

La aplicaci√≥n se abrir√° autom√°ticamente en el navegador predeterminado en `http://localhost:8501`

## F.4 Navegaci√≥n Principal

### F.4.1 Sidebar de Navegaci√≥n

La aplicaci√≥n presenta un men√∫ lateral (sidebar) con dos secciones principales:

#### **üß≠ Navegaci√≥n**
Contiene las cuatro p√°ginas disponibles:
1. üîç B√∫squeda Individual
2. üìà An√°lisis de Datos
3. ‚öôÔ∏è Configuraci√≥n M√©tricas Acumulativas
4. üìä Resultados M√©tricas Acumulativas

#### **‚öôÔ∏è Configuraci√≥n**
Permite seleccionar:
- **Modelo de Embedding**: Ada, MPNet, MiniLM, E5-Large
- **Modelo Generativo**: TinyLlama, GPT-4, Gemini, Mistral

## F.5 P√°gina: üîç B√∫squeda Individual

### F.5.1 Prop√≥sito
Permite realizar consultas individuales sobre documentaci√≥n de Azure y obtener respuestas generadas por el sistema RAG.

### F.5.2 Elementos de la Interfaz

#### **Campo de Consulta**
- **Ubicaci√≥n**: Parte superior de la p√°gina principal
- **Placeholder**: "Ejemplo: How to configure Azure Virtual Network?"
- **Funci√≥n**: Ingrese su pregunta sobre Azure en lenguaje natural

#### **Par√°metros de B√∫squeda**
- **N√∫mero de documentos (k)**: Slider para seleccionar cu√°ntos documentos recuperar (1-20)
- **Enable CrossEncoder Reranking**: Checkbox para activar/desactivar reranking
- **Usar RAG (Retrieval Augmented Generation)**: Checkbox para generar respuestas completas

#### **Bot√≥n de B√∫squeda**
- **Texto**: "üîç Buscar"
- **Acci√≥n**: Inicia el proceso de recuperaci√≥n y generaci√≥n

### F.5.3 Proceso de Uso

1. **Ingresar Consulta**: Escriba una pregunta espec√≠fica sobre Azure
   - Ejemplo: "How to configure SSL certificates in Azure Application Gateway?"

2. **Configurar Par√°metros**:
   - Ajuste el n√∫mero de documentos seg√∫n necesidad (default: 10)
   - Active reranking para mejorar relevancia (recomendado)
   - Active RAG para obtener respuestas generadas

3. **Ejecutar B√∫squeda**: Haga clic en "üîç Buscar"

4. **Revisar Resultados**:
   - **Respuesta Generada**: Aparece primero si RAG est√° activo
   - **Documentos Recuperados**: Lista de documentos relevantes con:
     - T√≠tulo del documento
     - Score de relevancia
     - Contenido del fragmento
     - Enlace a la documentaci√≥n oficial

### F.5.4 Interpretaci√≥n de Resultados

#### **M√©tricas Mostradas**
- **Antes del Reranking**: Scores originales del modelo de embedding
- **Despu√©s del Reranking**: Scores ajustados por CrossEncoder
- **Diferencia**: Cambio en el ordenamiento y relevancia

#### **Visualizaci√≥n de Scores**
- Gr√°fico de barras comparativo antes/despu√©s del reranking
- Colores indican mejora (verde) o degradaci√≥n (rojo) en posici√≥n

### F.5.5 Casos de Uso T√≠picos

1. **Consulta de Procedimientos**: "How to create a virtual machine in Azure?"
2. **Resoluci√≥n de Problemas**: "Troubleshooting Azure Storage connection issues"
3. **Mejores Pr√°cticas**: "Best practices for Azure SQL Database security"
4. **Configuraci√≥n**: "Configure Azure Application Gateway with SSL"

## F.6 P√°gina: üìà An√°lisis de Datos (Cap√≠tulo 4)

### F.6.1 Prop√≥sito
Presenta un mosaico completo con todas las figuras y visualizaciones del **Cap√≠tulo 4: An√°lisis Exploratorio de Datos** de la tesis, mostrando las caracter√≠sticas del corpus Microsoft Azure Documentation.

### F.6.2 Contenido Principal

#### **üìä M√©tricas Generales**
Panel superior con 4 m√©tricas clave:
- **Total Documentos**: ~16,900 documentos procesados
- **Total Chunks**: ~67,600 fragmentos generados  
- **Total Preguntas**: ~18,000 de Microsoft Q&A
- **Promedio Chunks/Doc**: ~4.0 fragmentos por documento

#### **üé® Mosaico de Figuras (6 visualizaciones)**

**Fila 1: Distribuciones Principales**
- **Figura 4.1**: Histograma de distribuci√≥n de chunks por documento
- **Figura 4.2**: Gr√°fico de pie de √°reas tem√°ticas de Azure

**Fila 2: An√°lisis por Servicio y Preguntas**  
- **Figura 4.3**: Boxplot de chunks por servicio de Azure
- **Figura 4.4**: Histograma de tipos de preguntas Microsoft Q&A

**Fila 3: Proceso y An√°lisis Avanzado**
- **Figura 4.5**: Diagrama de flujo del proceso de Ground Truth
- **Figura 4.6**: Panel de an√°lisis de complejidad (4 subfiguras)

### F.6.3 Interpretaci√≥n de las Visualizaciones

#### **Figura 4.1: Distribuci√≥n de Chunks**
- Muestra la granularidad del corpus
- Mayor√≠a de documentos tienen 4-8 chunks
- L√≠neas de media y mediana para referencia
- Gradiente de colores por frecuencia

#### **Figura 4.2: √Åreas Tem√°ticas**
- **Compute & VMs**: 25% (√°rea dominante)
- **Storage & Databases**: 22% 
- **Networking**: 18%
- **Security & Identity**: 15%
- **DevOps & Deployment**: 12%
- **Monitoring & Analytics**: 8%

#### **Figura 4.3: Chunks por Servicio**
- Servicios complejos (Kubernetes, VMs) requieren m√°s chunks
- Boxplots muestran variabilidad por servicio
- Ordenados por mediana descendente
- Colores diferenciados por servicio

#### **Figura 4.4: Tipos de Preguntas**
- **Configuraci√≥n**: 3,200 preguntas (dominante)
- **Troubleshooting**: 2,800 preguntas
- **Mejores Pr√°cticas**: 2,400 preguntas
- Refleja necesidades pr√°cticas de usuarios

#### **Figura 4.5: Proceso Ground Truth**
- Flujo de 18,436 ‚Üí 2,067 pares validados
- Muestra filtros aplicados y porcentajes
- Diagrama con nodos y flechas direccionales
- Destaca la calidad sobre cantidad

#### **Figura 4.6: An√°lisis de Complejidad**
- **Subfigura 1**: Scatter plot longitud-chunks
- **Subfigura 2**: Histograma longitud de queries
- **Subfigura 3**: Matriz de correlaci√≥n
- **Subfigura 4**: Tendencias temporales

### F.6.4 Hallazgos Principales Mostrados

#### **üìà Caracter√≠sticas del Corpus**
- 187,031 chunks procesados exitosamente
- 62,417 documentos √∫nicos de Microsoft Learn
- Distribuci√≥n equilibrada entre servicios principales
- Granularidad √≥ptima para recuperaci√≥n sem√°ntica

#### **‚ùì Caracter√≠sticas de las Preguntas**  
- 18,436 preguntas originales de Microsoft Q&A
- 2,067 pares validados con ground truth
- 68.2% cobertura entre preguntas y documentos
- Enfoque pr√°ctico en configuraci√≥n y troubleshooting

### F.6.5 Navegaci√≥n y Uso

1. **Exploraci√≥n Visual**: Scroll vertical para ver todas las figuras
2. **Interpretaciones**: Cada figura incluye explicaci√≥n detallada
3. **M√©tricas Destacadas**: Panel superior con estad√≠sticas clave
4. **Conclusiones**: Resumen de hallazgos al final de la p√°gina
5. **Interactividad**: Figuras generadas din√°micamente con matplotlib

### F.6.6 Valor para la Investigaci√≥n

Esta p√°gina consolida todo el an√°lisis exploratorio en una vista comprehensiva que:
- **Documenta la calidad** del corpus utilizado
- **Justifica la metodolog√≠a** de segmentaci√≥n
- **Muestra la cobertura** entre preguntas y documentos
- **Valida la representatividad** del dataset de evaluaci√≥n

## F.7 P√°gina: ‚öôÔ∏è Configuraci√≥n M√©tricas Acumulativas

### F.7.1 Prop√≥sito
Permite configurar y generar archivos de configuraci√≥n para evaluaciones exhaustivas del sistema en Google Colab.

### F.7.2 Elementos de Configuraci√≥n

#### **üìù Configuraci√≥n General**
- **Nombre del Experimento**: Identificador √∫nico para la evaluaci√≥n
- **Descripci√≥n**: Detalles sobre el prop√≥sito de la evaluaci√≥n
- **N√∫mero de Preguntas**: Cantidad de preguntas a evaluar (10-1000)

#### **üéØ Selecci√≥n de Modelos**
- **Modelos de Embedding**: Checkboxes para seleccionar modelos a evaluar
  - ‚úÖ Ada (OpenAI)
  - ‚úÖ MPNet
  - ‚úÖ MiniLM
  - ‚úÖ E5-Large

#### **üìä M√©tricas a Calcular**
- **M√©tricas Tradicionales**: Precision@k, Recall@k, F1@k, NDCG@k
- **M√©tricas RAG**: Faithfulness, Answer Relevancy, Context Precision
- **M√©tricas Sem√°nticas**: BERTScore (Precision, Recall, F1)

#### **üîß Par√°metros de Evaluaci√≥n**
- **Top K**: N√∫mero de documentos a recuperar (default: 10)
- **Enable Reranking**: Activar CrossEncoder para todos los modelos
- **Batch Size**: Tama√±o de lote para procesamiento (default: 50)

### F.7.3 Proceso de Configuraci√≥n

1. **Definir Experimento**:
   ```
   Nombre: evaluacion_completa_agosto_2025
   Descripci√≥n: Evaluaci√≥n exhaustiva con 1000 preguntas por modelo
   ```

2. **Seleccionar Modelos**:
   - Marcar todos los modelos para comparaci√≥n completa
   - O seleccionar subconjunto para evaluaci√≥n espec√≠fica

3. **Configurar M√©tricas**:
   - Mantener todas las m√©tricas activas para an√°lisis completo
   - Desactivar m√©tricas costosas si hay limitaciones de tiempo

4. **Generar Configuraci√≥n**:
   - Click en "üíæ Generar Configuraci√≥n"
   - Se descarga archivo `evaluation_config_[timestamp].json`

### F.7.4 Archivo de Configuraci√≥n Generado

```json
{
  "experiment_name": "evaluacion_completa_agosto_2025",
  "description": "Evaluaci√≥n exhaustiva con 1000 preguntas por modelo",
  "timestamp": "2025-08-03T10:30:00",
  "models": ["ada", "mpnet", "minilm", "e5-large"],
  "num_questions": 1000,
  "metrics": {
    "traditional": ["precision", "recall", "f1", "ndcg", "mrr"],
    "rag": ["faithfulness", "answer_relevancy", "context_precision"],
    "semantic": ["bertscore_precision", "bertscore_recall", "bertscore_f1"]
  },
  "parameters": {
    "top_k": 10,
    "enable_reranking": true,
    "batch_size": 50,
    "seed": 42
  }
}
```

### F.7.5 Uso del Archivo en Google Colab

1. **Subir a Colab**: Cargar el archivo JSON generado
2. **Ejecutar Notebook**: `Cumulative_Ticket_Evaluation.ipynb`
3. **Monitorear Progreso**: La evaluaci√≥n mostrar√° progreso en tiempo real
4. **Descargar Resultados**: Al finalizar, descargar `cumulative_results_*.json`

## F.8 P√°gina: üìä Resultados M√©tricas Acumulativas

### F.8.1 Prop√≥sito
Visualiza y analiza los resultados de evaluaciones completas ejecutadas en Google Colab.

### F.8.2 Carga de Resultados

#### **üìÅ Cargar Archivo de Resultados**
1. Click en "Browse files" o arrastrar archivo
2. Seleccionar archivo `cumulative_results_*.json`
3. El sistema valida y carga autom√°ticamente

#### **Validaci√≥n del Archivo**
- Verifica estructura JSON correcta
- Confirma presencia de m√©tricas requeridas
- Detecta modelos evaluados
- Calcula estad√≠sticas de completitud

### F.8.3 Visualizaciones Disponibles

#### **üìä Comparaci√≥n de Modelos**
- **Gr√°fico de Barras**: Precision@5, Recall@5, F1@5 por modelo
- **Antes/Despu√©s Reranking**: Impacto del CrossEncoder
- **Tabla Resumen**: Todas las m√©tricas en formato tabular

#### **üìà An√°lisis de Rendimiento**
- **NDCG@k**: Calidad del ranking (1-15)
- **MRR**: Mean Reciprocal Rank
- **MAP**: Mean Average Precision

#### **üéØ M√©tricas RAG**
- **Faithfulness**: Fidelidad de las respuestas generadas
- **Answer Relevancy**: Relevancia de las respuestas
- **Context Precision**: Precisi√≥n del contexto recuperado

#### **üìâ An√°lisis Temporal**
- **Tiempo por Consulta**: Distribuci√≥n de latencias
- **Throughput**: Consultas procesadas por minuto
- **Bottlenecks**: Identificaci√≥n de cuellos de botella

### F.8.4 Interpretaci√≥n de Resultados

#### **Identificar Mejor Modelo**
1. Revisar Precision@5 (m√©trica principal)
2. Considerar balance Precision/Recall
3. Evaluar impacto del reranking
4. Verificar consistencia en m√©tricas RAG

#### **An√°lisis de Reranking**
- **Mejora Positiva**: Verde, el reranking ayuda
- **Impacto Negativo**: Rojo, el reranking degrada
- **Neutral**: Gris, cambio m√≠nimo

#### **Significancia Estad√≠stica**
- Valores p < 0.05 indican diferencias significativas
- Intervalos de confianza no solapados confirman diferencias
- Tests de Wilcoxon para comparaciones pareadas

### F.8.5 Exportaci√≥n de Resultados

#### **üìÑ Formatos Disponibles**
1. **CSV**: Tabla de m√©tricas para an√°lisis externo
2. **PNG**: Gr√°ficos de alta resoluci√≥n
3. **JSON**: Datos crudos para procesamiento adicional
4. **PDF**: Reporte completo con visualizaciones

#### **üé® Personalizaci√≥n**
- Seleccionar m√©tricas espec√≠ficas
- Filtrar modelos de inter√©s
- Ajustar escalas y colores
- Agregar anotaciones

### F.8.6 Casos de Uso Avanzados

1. **Comparaci√≥n Multi-Experimento**:
   - Cargar m√∫ltiples archivos de resultados
   - Comparar evoluci√≥n temporal
   - Identificar mejoras/degradaciones

2. **An√°lisis por Subgrupos**:
   - Filtrar por tipo de pregunta
   - Analizar por servicio de Azure
   - Segmentar por complejidad

3. **Optimizaci√≥n de Par√°metros**:
   - Variar top_k y analizar impacto
   - Ajustar umbrales de reranking
   - Encontrar configuraci√≥n √≥ptima

## F.9 Troubleshooting Com√∫n

### F.9.1 Errores de Inicio

**Error**: "ModuleNotFoundError"
- **Soluci√≥n**: Verificar instalaci√≥n de dependencias con `pip install -r requirements.txt`

**Error**: "ChromaDB connection failed"
- **Soluci√≥n**: Verificar que ChromaDB est√© instalado y la ruta sea correcta

### F.9.2 Problemas de Rendimiento

**S√≠ntoma**: B√∫squedas muy lentas
- **Causa**: Modelo de embedding no optimizado
- **Soluci√≥n**: Usar MiniLM para pruebas r√°pidas, Ada para producci√≥n

**S√≠ntoma**: Memoria insuficiente
- **Causa**: Modelos generativos grandes
- **Soluci√≥n**: Usar TinyLlama en lugar de Mistral

### F.9.3 Resultados Inesperados

**S√≠ntoma**: M√©tricas en cero
- **Causa**: Configuraci√≥n incorrecta del modelo
- **Soluci√≥n**: Verificar prefijos para E5-Large, normalizaci√≥n de embeddings

**S√≠ntoma**: Documentos irrelevantes
- **Causa**: Query muy general o ambigua
- **Soluci√≥n**: Ser m√°s espec√≠fico, incluir nombres de servicios Azure

## F.10 Mejores Pr√°cticas

### F.10.1 Para B√∫squeda Individual
1. Usar preguntas espec√≠ficas con contexto
2. Activar reranking para mejor precisi√≥n
3. Ajustar k seg√∫n necesidad (m√°s documentos = m√°s contexto)

### F.10.2 Para Evaluaciones
1. Usar m√≠nimo 100 preguntas para significancia estad√≠stica
2. Evaluar todos los modelos para comparaci√≥n completa
3. Guardar configuraciones para reproducibilidad

### F.10.3 Para An√°lisis
1. Comparar m√©tricas antes/despu√©s de cambios
2. Documentar configuraciones utilizadas
3. Exportar resultados para respaldo

## F.11 Conclusi√≥n

La aplicaci√≥n Streamlit proporciona una interfaz intuitiva y poderosa para interactuar con el sistema RAG de documentaci√≥n Azure. Las cuatro p√°ginas cubren el ciclo completo desde b√∫squedas individuales hasta evaluaciones exhaustivas, permitiendo tanto uso operacional como investigaci√≥n sistem√°tica del rendimiento del sistema.