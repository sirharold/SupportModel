# DIAGRAMA DEL PIPELINE DE EVALUACI√ìN RAG

## Introducci√≥n

Este documento presenta el diagrama completo del pipeline de evaluaci√≥n del sistema RAG, desde la configuraci√≥n inicial en Streamlit hasta la generaci√≥n y visualizaci√≥n de resultados. El pipeline proces√≥ 1,000 preguntas por modelo con 4 modelos de embedding diferentes durante 7.8 horas de evaluaci√≥n.

---

## DIAGRAMA 1: PIPELINE COMPLETO DE CONFIGURACI√ìN Y EVALUACI√ìN

```mermaid
flowchart TD
    %% FASE 1: CONFIGURACI√ìN EN STREAMLIT
    A[üë§ Usuario accede a Streamlit App] --> B[üìã Interfaz de Configuraci√≥n]
    B --> C{Seleccionar Modelos de Embedding}
    C --> C1[‚úì Ada - text-embedding-ada-002]
    C --> C2[‚úì MPNet - multi-qa-mpnet-base-dot-v1]
    C --> C3[‚úì MiniLM - all-MiniLM-L6-v2]
    C --> C4[‚úì E5-Large - intfloat/e5-large-v2]
    
    C1 --> D[‚öôÔ∏è Configurar Par√°metros]
    C2 --> D
    C3 --> D
    C4 --> D
    
    D --> D1[üìä Top-K: 15 documentos]
    D --> D2[üîÑ Reranking: CrossEncoder]
    D --> D3[ü§ñ RAG Metrics: True]
    D --> D4[üìù Questions: 1000]
    
    D1 --> E[üíæ Generar evaluation_config_TIMESTAMP.json]
    D2 --> E
    D3 --> E
    D4 --> E
    
    E --> F[‚òÅÔ∏è Subir config a Google Drive]
    F --> G[üîó Usuario obtiene enlace a Google Colab]
    
    %% FASE 2: CARGA EN GOOGLE COLAB
    G --> H[üöÄ Google Colab - Cumulative_Ticket_Evaluation.ipynb]
    H --> I[üîß Setup: Mount Drive + Install Packages]
    I --> J[üîë Cargar API Keys - OpenAI + HuggingFace]
    J --> K[üìÇ Cargar Embeddings Pre-computados]
    
    K --> K1[üìÑ docs_ada_with_embeddings_*.parquet<br/>187,031 docs √ó 1536D]
    K --> K2[üìÑ docs_mpnet_with_embeddings_*.parquet<br/>187,031 docs √ó 768D]
    K --> K3[üìÑ docs_minilm_with_embeddings_*.parquet<br/>187,031 docs √ó 384D]
    K --> K4[üìÑ docs_e5large_with_embeddings_*.parquet<br/>187,031 docs √ó 1024D]
    
    K1 --> L[‚öôÔ∏è Inicializar Pipeline de Evaluaci√≥n]
    K2 --> L
    K3 --> L
    K4 --> L
    
    %% FASE 3: CONFIGURACI√ìN DE MODELOS
    L --> M[ü§ñ Cargar Modelos de Embeddings]
    M --> M1[üî• Ada: OpenAI API calls]
    M --> M2[üß† MPNet: sentence-transformers]
    M --> M3[‚ö° MiniLM: sentence-transformers]
    M --> M4[üìà E5-Large: sentence-transformers]
    
    M1 --> N[üéØ Cargar CrossEncoder ms-marco-MiniLM-L-6-v2]
    M2 --> N
    M3 --> N
    M4 --> N
    
    N --> O[üìã Cargar configuration file]
    O --> P[üîç Extraer 1000 preguntas + ground truth]
    
    %% Styling para claridad
    classDef configPhase fill:#e1f5fe,stroke:#0277bd,stroke-width:2px
    classDef colabPhase fill:#f3e5f5,stroke:#7b1fa2,stroke-width:2px
    classDef modelPhase fill:#e8f5e8,stroke:#388e3c,stroke-width:2px
    
    class A,B,C,C1,C2,C3,C4,D,D1,D2,D3,D4,E,F,G configPhase
    class H,I,J,K,K1,K2,K3,K4,L colabPhase
    class M,M1,M2,M3,M4,N,O,P modelPhase
```

---

## DIAGRAMA 2: LOOP DE EVALUACI√ìN DETALLADO (N=1000 √ó M=4)

```mermaid
flowchart TD
    %% INICIALIZACI√ìN
    START[üöÄ Inicio de Evaluaci√≥n<br/>4 modelos √ó 1000 preguntas] --> LOOP1[üîÑ FOR model in [ada, mpnet, minilm, e5-large]]
    
    %% LOOP PRINCIPAL POR MODELO
    LOOP1 --> MODEL[üìä Modelo Actual: {model}]
    MODEL --> INIT[‚öôÔ∏è Inicializar Acumuladores]
    INIT --> INIT1[üìà all_before_metrics = []]
    INIT --> INIT2[üìà all_after_metrics = []]
    INIT --> INIT3[ü§ñ individual_rag_metrics = []]
    INIT --> INIT4[üìä before_scores = []]
    INIT --> INIT5[üìä after_scores = []]
    
    INIT1 --> LOOP2[üîÑ FOR i in range(1000) preguntas]
    INIT2 --> LOOP2
    INIT3 --> LOOP2
    INIT4 --> LOOP2
    INIT5 --> LOOP2
    
    %% LOOP POR PREGUNTA
    LOOP2 --> Q[‚ùì Pregunta[i]]
    Q --> Q1[üìù question = pregunta_actual]
    Q --> Q2[üîó ground_truth_links = enlaces_validados]
    Q --> Q3[üí¨ ground_truth_answer = respuesta_aceptada]
    
    Q1 --> EMB[üß† Generar Query Embedding]
    Q2 --> EMB
    Q3 --> EMB
    
    %% GENERACI√ìN DE EMBEDDINGS
    EMB --> EMB_DECISION{¬øModelo = Ada?}
    EMB_DECISION -->|S√ç| EMB_ADA[üî• OpenAI API Call<br/>text-embedding-ada-002<br/>‚Üí vector[1536]]
    EMB_DECISION -->|NO| EMB_LOCAL[üß† sentence-transformers.encode<br/>‚Üí vector[dim_model]]
    
    EMB_ADA --> SEARCH[üîç B√∫squeda Vectorial]
    EMB_LOCAL --> SEARCH
    
    %% B√öSQUEDA VECTORIAL
    SEARCH --> SEARCH1[üìä Cosine Similarity con 187,031 docs]
    SEARCH1 --> SEARCH2[üéØ Top-15 documentos m√°s similares]
    SEARCH2 --> SEARCH3[üìã Ranking inicial por similarity]
    
    SEARCH3 --> METRICS_BEFORE[üìà Calcular M√©tricas ANTES]
    
    %% M√âTRICAS ANTES DEL RERANKING
    METRICS_BEFORE --> MB1[‚ö° Precision@1,2,3...15]
    METRICS_BEFORE --> MB2[üéØ Recall@1,2,3...15]
    METRICS_BEFORE --> MB3[üèÜ F1@1,2,3...15]
    METRICS_BEFORE --> MB4[üìä NDCG@1,2,3...15]
    METRICS_BEFORE --> MB5[üó∫Ô∏è MAP@1,2,3...15]
    METRICS_BEFORE --> MB6[üî¢ MRR@1,2,3...15]
    
    MB1 --> RERANK[üîÑ CrossEncoder Reranking]
    MB2 --> RERANK
    MB3 --> RERANK
    MB4 --> RERANK
    MB5 --> RERANK
    MB6 --> RERANK
    
    %% RERANKING PROCESS
    RERANK --> RE1[üìù Preparar pares [query, doc_content]]
    RE1 --> RE2[üß† CrossEncoder.predict<br/>ms-marco-MiniLM-L-6-v2]
    RE2 --> RE3[üìä Min-Max Normalization ‚Üí [0,1]]
    RE3 --> RE4[üîÑ Re-ordenar por CrossEncoder score]
    RE4 --> RE5[üìã Nuevo ranking Top-15]
    
    RE5 --> METRICS_AFTER[üìà Calcular M√©tricas DESPU√âS]
    
    %% M√âTRICAS DESPU√âS DEL RERANKING
    METRICS_AFTER --> MA1[‚ö° Precision@1,2,3...15]
    METRICS_AFTER --> MA2[üéØ Recall@1,2,3...15]
    METRICS_AFTER --> MA3[üèÜ F1@1,2,3...15]
    METRICS_AFTER --> MA4[üìä NDCG@1,2,3...15]
    METRICS_AFTER --> MA5[üó∫Ô∏è MAP@1,2,3...15]
    METRICS_AFTER --> MA6[üî¢ MRR@1,2,3...15]
    
    MA1 --> RAG[ü§ñ RAG Answer Generation]
    MA2 --> RAG
    MA3 --> RAG
    MA4 --> RAG
    MA5 --> RAG
    MA6 --> RAG
    
    %% RAG METRICS
    RAG --> RAG1[üí¨ GPT-3.5-turbo Generate Answer]
    RAG1 --> RAG2[üìä Calcular RAGAS Metrics]
    RAG2 --> RAG3[‚úÖ Faithfulness Score]
    RAG2 --> RAG4[üéØ Answer Relevancy Score]
    RAG2 --> RAG5[‚úîÔ∏è Answer Correctness Score]
    RAG2 --> RAG6[üìã Context Precision Score]
    RAG2 --> RAG7[üîç Context Recall Score]
    RAG2 --> RAG8[üß† Semantic Similarity Score]
    
    RAG3 --> BERT[üéØ BERTScore Calculation]
    RAG4 --> BERT
    RAG5 --> BERT
    RAG6 --> BERT
    RAG7 --> BERT
    RAG8 --> BERT
    
    BERT --> BERT1[üìä BERT Precision]
    BERT --> BERT2[üìä BERT Recall]
    BERT --> BERT3[üìä BERT F1]
    
    %% ACUMULACI√ìN
    BERT1 --> ACCUMULATE[üìà Acumular Resultados]
    BERT2 --> ACCUMULATE
    BERT3 --> ACCUMULATE
    
    ACCUMULATE --> ACC1[‚ûï all_before_metrics.append]
    ACCUMULATE --> ACC2[‚ûï all_after_metrics.append]
    ACCUMULATE --> ACC3[‚ûï individual_rag_metrics.append]
    ACCUMULATE --> ACC4[‚ûï before_scores.append]
    ACCUMULATE --> ACC5[‚ûï after_scores.append]
    
    %% CONTROL DE LOOP
    ACC1 --> QUESTION_CHECK{i < 999?}
    ACC2 --> QUESTION_CHECK
    ACC3 --> QUESTION_CHECK
    ACC4 --> QUESTION_CHECK
    ACC5 --> QUESTION_CHECK
    
    QUESTION_CHECK -->|S√ç| LOOP2
    QUESTION_CHECK -->|NO| AVG[üìä Calcular Promedios del Modelo]
    
    %% C√ÅLCULO DE PROMEDIOS
    AVG --> AVG1[üìà avg_before_metrics = np.mean]
    AVG --> AVG2[üìà avg_after_metrics = np.mean]
    AVG --> AVG3[ü§ñ rag_averages = np.mean]
    
    AVG1 --> MODEL_DONE[‚úÖ Modelo Completado]
    AVG2 --> MODEL_DONE
    AVG3 --> MODEL_DONE
    
    MODEL_DONE --> MODEL_CHECK{¬øM√°s modelos?}
    MODEL_CHECK -->|S√ç| LOOP1
    MODEL_CHECK -->|NO| SAVE[üíæ Guardar Resultados]
    
    %% GUARDADO DE RESULTADOS
    SAVE --> SAVE1[üìÅ cumulative_results_TIMESTAMP.json]
    SAVE1 --> SAVE2[‚òÅÔ∏è Subir a Google Drive]
    SAVE2 --> SAVE3[üîî Notificar Usuario]
    
    SAVE3 --> END[üéâ Evaluaci√≥n Completada<br/>7.8 horas total]
    
    %% Contadores de Progreso
    LOOP2 -.-> PROGRESS1[üìä Ada: 1000/1000 preguntas<br/>‚è±Ô∏è 2:25:23 horas]
    LOOP2 -.-> PROGRESS2[üìä E5-Large: 1000/1000 preguntas<br/>‚è±Ô∏è 1:56:59 horas]
    LOOP2 -.-> PROGRESS3[üìä MPNet: 1000/1000 preguntas<br/>‚è±Ô∏è 1:47:46 horas]
    LOOP2 -.-> PROGRESS4[üìä MiniLM: 1000/1000 preguntas<br/>‚è±Ô∏è 1:40:06 horas]
    
    %% Styling para diferentes fases
    classDef loopStyle fill:#fff3e0,stroke:#ff6f00,stroke-width:2px
    classDef embeddingStyle fill:#e8f5e8,stroke:#388e3c,stroke-width:2px
    classDef searchStyle fill:#e3f2fd,stroke:#1976d2,stroke-width:2px
    classDef metricsStyle fill:#fce4ec,stroke:#c2185b,stroke-width:2px
    classDef ragStyle fill:#f3e5f5,stroke:#7b1fa2,stroke-width:2px
    classDef saveStyle fill:#fff8e1,stroke:#f57c00,stroke-width:2px
    
    class START,LOOP1,LOOP2,MODEL,QUESTION_CHECK,MODEL_CHECK loopStyle
    class EMB,EMB_DECISION,EMB_ADA,EMB_LOCAL embeddingStyle
    class SEARCH,SEARCH1,SEARCH2,SEARCH3 searchStyle
    class METRICS_BEFORE,MB1,MB2,MB3,MB4,MB5,MB6,METRICS_AFTER,MA1,MA2,MA3,MA4,MA5,MA6 metricsStyle
    class RAG,RAG1,RAG2,RAG3,RAG4,RAG5,RAG6,RAG7,RAG8,BERT,BERT1,BERT2,BERT3 ragStyle
    class SAVE,SAVE1,SAVE2,SAVE3,END saveStyle
```

---

## DIAGRAMA 3: FLUJO DE VISUALIZACI√ìN EN STREAMLIT

```mermaid
flowchart TD
    %% RETORNO A STREAMLIT
    A[üìÅ cumulative_results_TIMESTAMP.json<br/>Generado en Google Drive] --> B[üë§ Usuario regresa a Streamlit]
    B --> C[üìä Navegar a "Cumulative Metrics Results"]
    
    %% CARGA DE RESULTADOS
    C --> D[üìÇ Seleccionar archivo de resultados]
    D --> E[üîç Auto-detectar archivos recientes]
    E --> F[‚úÖ Cargar cumulative_results_20250802_222752.json]
    
    %% PROCESAMIENTO DE DATOS
    F --> G[‚öôÔ∏è Procesar datos JSON]
    G --> G1[üìä Extraer avg_before_metrics]
    G --> G2[üìä Extraer avg_after_metrics]
    G --> G3[ü§ñ Extraer rag_metrics]
    G --> G4[üìà Extraer individual_rag_metrics]
    
    %% VISUALIZACI√ìN PRINCIPAL
    G1 --> H[üé® Generar Visualizaciones]
    G2 --> H
    G3 --> H
    G4 --> H
    
    H --> H1[üìä Tabla Comparativa Principal]
    H --> H2[üìà Gr√°ficos Before/After Reranking]
    H --> H3[ü§ñ Dashboard de RAG Metrics]
    H --> H4[üìä An√°lisis Estad√≠stico]
    
    %% TABLA COMPARATIVA
    H1 --> I1[üìã Model | Questions | Dimensions]
    H1 --> I2[‚ö° Precision@5 (Before ‚Üí After)]
    H1 --> I3[üéØ Recall@5 (Before ‚Üí After)]
    H1 --> I4[üèÜ F1@5 (Before ‚Üí After)]
    H1 --> I5[üìä NDCG@5 (Before ‚Üí After)]
    H1 --> I6[üî¢ MRR (Before ‚Üí After)]
    H1 --> I7[üìà % Change por m√©trica]
    
    %% GR√ÅFICOS INTERACTIVOS
    H2 --> J1[üìä Plotly Bar Charts]
    H2 --> J2[üìà Line Charts - Mejoras por K]
    H2 --> J3[üéØ Scatter Plot - Score vs Performance]
    H2 --> J4[üìä Heatmap - Model Comparison]
    
    %% RAG METRICS DASHBOARD
    H3 --> K1[‚úÖ Faithfulness: 0.961-0.967]
    H3 --> K2[üéØ Answer Relevancy: Calculado por GPT]
    H3 --> K3[‚úîÔ∏è Answer Correctness: vs Ground Truth]
    H3 --> K4[üìã Context Precision: Relevancia de contexto]
    H3 --> K5[üîç Context Recall: Cobertura de informaci√≥n]
    H3 --> K6[üß† Semantic Similarity: BERTScore]
    H3 --> K7[üìä BERT Precision/Recall/F1]
    
    %% AN√ÅLISIS ESTAD√çSTICO
    H4 --> L1[üìä Significancia Estad√≠stica]
    H4 --> L2[üìà Distribuci√≥n de Scores]
    H4 --> L3[üéØ Ranking de Modelos]
    H4 --> L4[‚ö° Impacto del CrossEncoder]
    H4 --> L5[üìä Performance vs Dimensionalidad]
    
    %% INSIGHTS AUTOMATIZADOS
    K1 --> M[ü§ñ Generar Insights con LLM]
    K2 --> M
    K3 --> M
    K4 --> M
    K5 --> M
    K6 --> M
    K7 --> M
    
    M --> M1[üí° "Ada mantiene liderazgo pero reranking impacta negativamente"]
    M --> M2[üí° "E5-Large ahora funcional - problema resuelto"]
    M --> M3[üí° "MiniLM se beneficia m√°s del CrossEncoder (+12.3%)"]
    M --> M4[üí° "Jerarqu√≠a clara: Ada > MPNet > E5-Large > MiniLM"]
    
    %% EXPORTACI√ìN
    L1 --> N[üì§ Opciones de Exportaci√≥n]
    L2 --> N
    L3 --> N
    L4 --> N
    L5 --> N
    
    N --> N1[üìä Export DataFrame to CSV]
    N --> N2[üìà Export Charts to PNG/PDF]
    N --> N3[üìã Generate Report to Word]
    N --> N4[üìÅ Save Session State]
    
    %% DOCUMENTACI√ìN
    M1 --> O[üìö Actualizar Documentaci√≥n]
    M2 --> O
    M3 --> O
    M4 --> O
    
    O --> O1[üìù Cap√≠tulo 7 - Resultados]
    O --> O2[üìä Anexos - M√©tricas Detalladas]
    O --> O3[üéØ Conclusiones - Performance]
    O --> O4[üìà Recomendaciones - Uso de Modelos]
    
    %% FEEDBACK LOOP
    O1 --> P[üîÑ Feedback para Pr√≥ximas Evaluaciones]
    O2 --> P
    O3 --> P
    O4 --> P
    
    P --> P1[‚öôÔ∏è Ajustar par√°metros de evaluaci√≥n]
    P --> P2[üéØ Optimizar pipeline de reranking]
    P --> P3[üìä Mejorar m√©tricas de evaluaci√≥n]
    P --> P4[ü§ñ Refinar generaci√≥n de respuestas]
    
    %% Styling
    classDef dataStyle fill:#e8f5e8,stroke:#388e3c,stroke-width:2px
    classDef vizStyle fill:#e3f2fd,stroke:#1976d2,stroke-width:2px
    classDef ragStyle fill:#f3e5f5,stroke:#7b1fa2,stroke-width:2px
    classDef analyticsStyle fill:#fff3e0,stroke:#ff6f00,stroke-width:2px
    classDef exportStyle fill:#fce4ec,stroke:#c2185b,stroke-width:2px
    
    class A,B,C,D,E,F,G,G1,G2,G3,G4 dataStyle
    class H,H1,H2,H3,H4,I1,I2,I3,I4,I5,I6,I7,J1,J2,J3,J4 vizStyle
    class K1,K2,K3,K4,K5,K6,K7,M,M1,M2,M3,M4 ragStyle
    class L1,L2,L3,L4,L5,O,O1,O2,O3,O4,P,P1,P2,P3,P4 analyticsStyle
    class N,N1,N2,N3,N4 exportStyle
```

---

## M√âTRICAS CLAVE DEL PIPELINE

### Volumen de Datos Procesados
- **4 modelos de embedding** evaluados en paralelo
- **1,000 preguntas** por modelo = **4,000 evaluaciones totales**
- **15 documentos** recuperados por pregunta = **60,000 documentos analizados**
- **187,031 documentos** en cada colecci√≥n vectorial
- **748,124 vectores totales** en ChromaDB

### Tiempo de Ejecuci√≥n por Modelo
1. **Ada**: 2:25:23 horas (8.72 seg/pregunta)
2. **E5-Large**: 1:56:59 horas (7.02 seg/pregunta)
3. **MPNet**: 1:47:46 horas (6.47 seg/pregunta)
4. **MiniLM**: 1:40:06 horas (6.01 seg/pregunta)
5. **Total**: 7.8 horas de evaluaci√≥n continua

### APIs y Servicios Utilizados
- **OpenAI API**: 1,000 calls para embeddings Ada + 4,000 calls para RAG
- **CrossEncoder**: 60,000 predicciones de reranking
- **Google Drive**: Sincronizaci√≥n autom√°tica de archivos
- **ChromaDB**: 4,000 b√∫squedas vectoriales coseno

### M√©tricas Generadas por Evaluaci√≥n
- **46 m√©tricas tradicionales** por pregunta (Precision, Recall, F1, NDCG, MAP, MRR @1-15)
- **9 m√©tricas RAG** por pregunta (6 RAGAS + 3 BERTScore)
- **Total**: 55 m√©tricas √ó 4,000 preguntas = **220,000 valores calculados**

---

## CONCLUSIONES DEL PIPELINE

### Eficiencia del Sistema
1. **Paralelizaci√≥n exitosa**: 4 modelos evaluados secuencialmente con recursos optimizados
2. **Gesti√≥n de memoria**: Embeddings pre-computados evitan rec√°lculo costoso
3. **API rate limiting**: Control autom√°tico de llamadas a OpenAI
4. **Checkpoint resilience**: Capacidad de recuperaci√≥n ante fallos

### Calidad de Datos
1. **Datos reales**: Sin simulaci√≥n ni valores aleatorios
2. **Ground truth validado**: 2,067 pares pregunta-documento verificados
3. **Normalizaci√≥n URL**: Matching preciso entre predicciones y ground truth
4. **CrossEncoder calibrado**: Normalizaci√≥n Min-Max para scores interpretables

### Reproducibilidad
1. **Configuraci√≥n versionada**: Archivos JSON con timestamp completo
2. **Semillas determin√≠sticas**: Resultados reproducibles
3. **Logging exhaustivo**: Trazabilidad completa del proceso
4. **Metadatos ricos**: Informaci√≥n completa de verificaci√≥n