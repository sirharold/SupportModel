#!/usr/bin/env python3
"""
Test script para verificar que se generen m√©tricas para k=1,3,5,10.
"""

import sys
import os
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

def test_k_values_generation():
    """Test que verifica la generaci√≥n de m√©tricas para todos los valores de k."""
    print("üß™ TESTING GENERACI√ìN DE M√âTRICAS PARA K=1,3,5,10")
    print("=" * 60)
    
    from src.evaluation.metrics.retrieval import calculate_before_after_reranking_metrics
    
    # Datos de prueba
    question = "¬øC√≥mo crear un Azure Storage Account?"
    
    docs_before = [
        {'title': 'Azure Storage Overview', 'link': 'https://learn.microsoft.com/azure/storage/common/storage-account-overview?view=azure-cli', 'score': 0.8},
        {'title': 'Create Storage Account', 'link': 'https://learn.microsoft.com/azure/storage/common/storage-account-create#portal', 'score': 0.7},
        {'title': 'Storage Security', 'link': 'https://learn.microsoft.com/azure/storage/common/storage-security-guide', 'score': 0.6},
        {'title': 'Blob Storage', 'link': 'https://learn.microsoft.com/azure/storage/blobs/storage-blobs-introduction', 'score': 0.5},
        {'title': 'Other Doc', 'link': 'https://example.com/other', 'score': 0.4}
    ]
    
    docs_after = [
        {'title': 'Create Storage Account', 'link': 'https://learn.microsoft.com/azure/storage/common/storage-account-create#portal', 'score': 0.95},
        {'title': 'Azure Storage Overview', 'link': 'https://learn.microsoft.com/azure/storage/common/storage-account-overview?view=azure-cli', 'score': 0.9},
        {'title': 'Blob Storage', 'link': 'https://learn.microsoft.com/azure/storage/blobs/storage-blobs-introduction', 'score': 0.85},
        {'title': 'Storage Security', 'link': 'https://learn.microsoft.com/azure/storage/common/storage-security-guide', 'score': 0.8},
        {'title': 'Other Doc', 'link': 'https://example.com/other', 'score': 0.4}
    ]
    
    ground_truth_answer = """
    Para crear una Azure Storage Account:
    1. Gu√≠a completa: https://learn.microsoft.com/azure/storage/common/storage-account-create
    2. Informaci√≥n general: https://learn.microsoft.com/azure/storage/common/storage-account-overview
    3. Configuraci√≥n de Blob Storage: https://learn.microsoft.com/azure/storage/blobs/storage-blobs-introduction
    """
    
    try:
        # Calcular m√©tricas con k_values expl√≠cito
        result = calculate_before_after_reranking_metrics(
            question=question,
            docs_before_reranking=docs_before,
            docs_after_reranking=docs_after,
            ground_truth_answer=ground_truth_answer,
            ms_links=None,  # Se extraer√°n autom√°ticamente
            k_values=[1, 3, 5, 10]
        )
        
        print("üìä M√âTRICAS CALCULADAS:")
        print("=" * 40)
        
        before_metrics = result['before_reranking']
        after_metrics = result['after_reranking']
        
        print("üìã M√©tricas BEFORE reranking:")
        for key, value in before_metrics.items():
            print(f"  {key}: {value:.4f}")
        
        print("\nüìã M√©tricas AFTER reranking:")
        for key, value in after_metrics.items():
            print(f"  {key}: {value:.4f}")
        
        # Verificar que existen m√©tricas para cada k
        expected_k_values = [1, 3, 5, 10]
        missing_k_values = []
        
        for k in expected_k_values:
            k_metrics = [metric for metric in before_metrics.keys() if f'@{k}' in metric]
            if not k_metrics:
                missing_k_values.append(k)
            else:
                print(f"\n‚úÖ M√©tricas para k={k}: {len(k_metrics)} encontradas")
                print(f"   Ejemplos: {k_metrics[:3]}")
        
        if missing_k_values:
            print(f"\n‚ùå VALORES DE K FALTANTES: {missing_k_values}")
            return False
        else:
            print(f"\n‚úÖ TODOS LOS VALORES DE K PRESENTES: {expected_k_values}")
        
        # Verificar tipos espec√≠ficos de m√©tricas
        metric_types = ['Recall', 'Precision', 'Accuracy']
        for k in expected_k_values:
            for metric_type in metric_types:
                metric_name = f'{metric_type}@{k}'
                if metric_name in before_metrics and metric_name in after_metrics:
                    print(f"‚úÖ {metric_name}: Before={before_metrics[metric_name]:.4f}, After={after_metrics[metric_name]:.4f}")
                else:
                    print(f"‚ùå {metric_name}: FALTANTE")
                    return False
        
        # Verificar estructura del resultado para p√°gina de comparaci√≥n
        comparison_data = {
            'Modelo': 'test-model',
            'Ground Truth': result.get('ground_truth_links_count', 0),
            'MRR_Before': before_metrics['MRR'],
            'MRR_After': after_metrics['MRR'],
            'MRR_Œî': after_metrics['MRR'] - before_metrics['MRR'],
            'MRR_%': ((after_metrics['MRR'] - before_metrics['MRR']) / before_metrics['MRR'] * 100) if before_metrics['MRR'] > 0 else 0
        }
        
        # Agregar m√©tricas para cada k
        for k in expected_k_values:
            for metric_type in metric_types:
                before_val = before_metrics.get(f'{metric_type}@{k}', 0)
                after_val = after_metrics.get(f'{metric_type}@{k}', 0)
                improvement = after_val - before_val
                pct_improvement = (improvement / before_val * 100) if before_val > 0 else 0
                
                comparison_data[f'{metric_type}@{k}_Before'] = before_val
                comparison_data[f'{metric_type}@{k}_After'] = after_val
                comparison_data[f'{metric_type}@{k}_Œî'] = improvement
                comparison_data[f'{metric_type}@{k}_%'] = pct_improvement
        
        print(f"\nüìä ESTRUCTURA PARA COMPARACI√ìN:")
        print(f"Total de columnas generadas: {len(comparison_data)}")
        
        # Verificar columnas espec√≠ficas que el usuario report√≥ como faltantes
        missing_columns = []
        for k in [3, 10]:  # Los valores que el usuario dice que faltan
            for metric_type in metric_types:
                col_name = f'{metric_type}@{k}_Before'
                if col_name not in comparison_data:
                    missing_columns.append(col_name)
        
        if missing_columns:
            print(f"‚ùå COLUMNAS FALTANTES PARA K=3,10: {missing_columns}")
            return False
        else:
            print("‚úÖ TODAS LAS COLUMNAS PARA K=3,10 EST√ÅN PRESENTES")
        
        # Mostrar algunas columnas espec√≠ficas para k=3 y k=10
        print(f"\nüìã M√âTRICAS ESPEC√çFICAS PARA K=3 Y K=10:")
        for k in [3, 10]:
            print(f"  k={k}:")
            for metric_type in metric_types:
                before_key = f'{metric_type}@{k}_Before'
                after_key = f'{metric_type}@{k}_After'
                print(f"    {metric_type}@{k}: {comparison_data[before_key]:.4f} ‚Üí {comparison_data[after_key]:.4f}")
        
        return True
        
    except Exception as e:
        print(f"‚ùå ERROR: {e}")
        import traceback
        traceback.print_exc()
        return False

def test_comparison_page_integration():
    """Test que simula la l√≥gica de la p√°gina de comparaci√≥n."""
    print("\nüß™ TESTING INTEGRACI√ìN CON P√ÅGINA DE COMPARACI√ìN")
    print("=" * 60)
    
    # Simular datos como los que llegan a la p√°gina de comparaci√≥n
    retrieval_comparison_data = [
        {
            'Modelo': 'multi-qa-mpnet-base-dot-v1',
            'Ground Truth': 3,
            'MRR_Before': 0.3333,
            'MRR_After': 1.0000,
            'MRR_Œî': 0.6667,
            'MRR_%': 200.0,
        }
    ]
    
    # Simular la l√≥gica de generaci√≥n de columnas de la p√°gina
    for k in [1, 3, 5, 10]:
        retrieval_comparison_data[0].update({
            f'Recall@{k}_Before': 0.3333 if k == 1 else 0.6667,
            f'Recall@{k}_After': 0.3333 if k == 1 else 1.0000,
            f'Recall@{k}_Œî': 0.0000 if k == 1 else 0.3333,
            f'Recall@{k}_%': 0.0 if k == 1 else 50.0,
            f'Precision@{k}_Before': 1.0000 if k == 1 else 0.4000,
            f'Precision@{k}_After': 1.0000 if k == 1 else 0.6000,
            f'Precision@{k}_Œî': 0.0000 if k == 1 else 0.2000,
            f'Precision@{k}_%': 0.0 if k == 1 else 50.0,
            f'Accuracy@{k}_Before': 0.5000,
            f'Accuracy@{k}_After': 0.6000,
            f'Accuracy@{k}_Œî': 0.1000,
            f'Accuracy@{k}_%': 20.0,
        })
    
    # Verificar que las columnas esperadas existen
    expected_columns_k3 = [
        'Recall@3_Before', 'Recall@3_After', 'Recall@3_Œî', 'Recall@3_%',
        'Precision@3_Before', 'Precision@3_After', 'Precision@3_Œî', 'Precision@3_%',
        'Accuracy@3_Before', 'Accuracy@3_After', 'Accuracy@3_Œî', 'Accuracy@3_%'
    ]
    
    expected_columns_k10 = [
        'Recall@10_Before', 'Recall@10_After', 'Recall@10_Œî', 'Recall@10_%',
        'Precision@10_Before', 'Precision@10_After', 'Precision@10_Œî', 'Precision@10_%',
        'Accuracy@10_Before', 'Accuracy@10_After', 'Accuracy@10_Œî', 'Accuracy@10_%'
    ]
    
    missing_columns = []
    
    for col in expected_columns_k3 + expected_columns_k10:
        if col not in retrieval_comparison_data[0]:
            missing_columns.append(col)
    
    if missing_columns:
        print(f"‚ùå COLUMNAS FALTANTES: {missing_columns}")
        return False
    
    print("‚úÖ TODAS LAS COLUMNAS PARA K=3,10 EST√ÅN PRESENTES EN SIMULACI√ìN")
    
    # Mostrar las columnas que el usuario report√≥ como faltantes
    print(f"\nüìã COLUMNAS REPORTADAS COMO FALTANTES POR EL USUARIO:")
    for k in [3, 10]:
        print(f"  k={k}:")
        for metric_type in ['Recall', 'Precision', 'Accuracy']:
            before_col = f'{metric_type}@{k}_Before'
            after_col = f'{metric_type}@{k}_After'
            if before_col in retrieval_comparison_data[0]:
                print(f"    ‚úÖ {before_col}: {retrieval_comparison_data[0][before_col]}")
                print(f"    ‚úÖ {after_col}: {retrieval_comparison_data[0][after_col]}")
            else:
                print(f"    ‚ùå {before_col}: FALTANTE")
    
    return True

def main():
    """Ejecuta todos los tests."""
    print("üöÄ INICIANDO INVESTIGACI√ìN DE M√âTRICAS FALTANTES PARA K=3,10")
    print("=" * 80)
    
    tests = [
        ("Generaci√≥n de M√©tricas K=1,3,5,10", test_k_values_generation),
        ("Integraci√≥n con P√°gina de Comparaci√≥n", test_comparison_page_integration)
    ]
    
    passed = 0
    failed = 0
    
    for test_name, test_func in tests:
        try:
            if test_func():
                passed += 1
                print(f"‚úÖ {test_name} TEST PASSED")
            else:
                failed += 1
                print(f"‚ùå {test_name} TEST FAILED")
        except Exception as e:
            failed += 1
            print(f"‚ùå {test_name} TEST ERROR: {e}")
    
    print("\n" + "=" * 80)
    print(f"üìä RESUMEN DE TESTS: {passed} PASSED, {failed} FAILED")
    
    if failed == 0:
        print("üéâ ¬°TODOS LOS TESTS PASARON!")
        print("\nüí° POSIBLES CAUSAS DEL PROBLEMA REPORTADO:")
        print("1. ‚úÖ M√©tricas se generan correctamente para k=1,3,5,10")
        print("2. ‚ùì El problema puede estar en la VISUALIZACI√ìN en Streamlit")
        print("3. ‚ùì Puede haber un filtro o condici√≥n que oculta k=3,10")
        print("4. ‚ùì El problema puede ser en el proceso de datos espec√≠fico del usuario")
    else:
        print("‚ö†Ô∏è  Algunos tests fallaron - hay un problema en la generaci√≥n de m√©tricas")
    
    return failed == 0

if __name__ == "__main__":
    success = main()
    sys.exit(0 if success else 1)